{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0824f5",
   "metadata": {},
   "source": [
    "# ü§ô Huggingface Course Gemma3 (4B) Vision Grpo on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-Gemma3_(4B)-Vision-GRPO.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Huggingface Course Gemma3 (4B) Vision Grpo</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-Gemma3_(4B)-Vision-GRPO.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\"><a href=\"https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt\"><img src=\"https://github.com/unslothai/notebooks/raw/main/assets/hf%20course.png\" width=\"165\"></a>\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "In this [Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt) and Unsloth notebook, you will learn to transform Gemma3 (4B) Vision GRPO into a Reasoning model using GRPO.\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGZXlLdltffg"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560,
     "referenced_widgets": [
      "4ec1bba5873e4d6cb8558218c9c9db2b",
      "d5540027119d4a33b63b3140dca5a8b0",
      "fa292014fb83480fb8d8a7e1391f6fe0",
      "40669cb0a32c495490c641abca1939c4",
      "8d957c43b82649d0a2e1ceb950e2fab9",
      "42ba58d02c3b4b32b080063d573e1110",
      "f67bff5550454748bbcfbaa9b1d2b923",
      "ae978022d80141029cfda4d97ec89ade",
      "2094ff1c81564554bfcca4a7eae926c7",
      "b21b807d8ed0426eac51aa1ff9281485",
      "f8919d628fea47eaad421470f5df3599",
      "bf224bc57f394f91a8ab4af3e51923fb",
      "bc8b798125594cf3a419ccb5f3de9504",
      "3ea46c2bee994d0fb0cea27db6fdde13",
      "2f8fb351f6ac4de19742aa36d395f380",
      "5316a49d70864d4090471d28675d2fc5",
      "68d3110606db4c39b541f2a8478884f8",
      "56b043afcfd34d04aac16c13c5bf57e7",
      "c24ee5a998db4d01bb8d1f4210d8fd9c",
      "3e8aabd67d524518900c16b061df8f2b",
      "68f0211210c042e99669ee6223df611b",
      "4180b8ffd9054ba0b3de1fd6647b08f9",
      "93e9d76a22d34b2588aec73a8a7a56e8",
      "72536f5bfc394b9ea5331c9321c335a5",
      "7025d58a365748cf88af444044a0c76d",
      "101ed32d7c0b441fa6085a991afdb2f6",
      "6fc89d06cb084ad3b1c1404433f8ee55",
      "b7ac94f68cd849f8a73bb190b07ce81b",
      "58dc959a31664f7c83e4f41dc462be65",
      "0434a825ae234a51a4aceb519cbcf758",
      "29cae6c525bf470994ebc045b3246445",
      "7cf8141d845845af9233dd2e6b948652",
      "d41c374a6f614aab92ebb14903e290e0",
      "6eb7a75e1ca441f3a283494f5db68243",
      "7d3e12dde5ff4bf49eec355b7d1493c1",
      "09e859e3cf7b4d15ba1991274900a642",
      "954d92f9f7ea476887da2001cec7bc70",
      "a1e694fc3c5f42ef93186ec475fcb54c",
      "c1d3c0070f9640c99f9a07d05fbfe487",
      "f879abd876ba43589071ac10e7f95578",
      "ec1c88c6bd894ef88370c1911f876113",
      "939bc6c8ecb34e0dbea8eaa8472c89d6",
      "ceb7a0ca2a17439ca230617833fa8718",
      "a180fd291b2f404ea42443a895ea4f64",
      "855f7b2a953f4c1987ed05194e1c5669",
      "4698e3ff39fe4326afc838014686e40c",
      "e849f201c2b0484ab0c47313ebcda789",
      "594387b783ec4dc8a9d06f72a981a52f",
      "3266bcd2d81c4469b82b2f2c77738dc1",
      "d9aaea68b0af463b98307408e7ab0ed9",
      "2b2c7a841538440995f6fb38d2048576",
      "923a65ff277843b984e56e57540370cb",
      "693bb1cffaca4028b9fe79c61f32d087",
      "77a6235e945940ed9f65cca0838654d6",
      "715b064f3e1f48168b73ecc877d59b4f",
      "a7789a82df8a4c059311addc28c6b298",
      "ca1d143ba07f4926869e09318714e9ce",
      "5caa3764878c4c148cd28da3c16839e2",
      "c249fbc288214f8b928d34eeb232b9db",
      "42822ce910e94ad9b225114c9aca7f25",
      "7d3377fb21c44c4fb57d44dbbaed948f",
      "570e924bf3954fadb611c655652e664b",
      "bd68261ba5a946128123d58c04eafcc2",
      "36fc2ee47a8d48b0ad06d1f6a8feb545",
      "ab6d8d1dc825499c8a4147593b745387",
      "30c9cd22f79a4450b3ab646016229a22",
      "2861cfe3052b4b80afe8c6e69c787c90",
      "e28c29a8c1d240c9a2a5b7f687df193b",
      "a4e55f2ad20d42c18c1e5dc7f9157064",
      "10453389d16e45149422b28c78dfdc0a",
      "85c9f8c9b77145e4a6cf83cce438d94d",
      "1de0591f5b4145ebaa82d5b63786f849",
      "0827e0a4b9bc43b2866e2c002bee1ffa",
      "a99d5dba93d64995b42c693f1fde6ef9",
      "dfad819eb97d497cb1b32396b52299ee",
      "257436c5a9854722ab1d341361e6252a",
      "b4d26b77bc96409bbc74c25919624da9",
      "fce7d1e6a25e4f26988f58392b8a88b8",
      "989fb0607bbc46c987c2249d07a5a04e",
      "9d5e3578d5cb47809c5efb679cf8b0ff",
      "e3c9bd4d67774d0a8deb88135b3c5f56",
      "ae6daaee120547babfee22e0ea9235d1",
      "18a3964cbcea4e099ae9ea7d9083cf4e",
      "964ebdc88e914cae8d14043237695e6c",
      "1e29b33877e044eba09211b47985a0c4",
      "4302dc8050ee4f2ab413d67528e0ef65",
      "6367839c23d34d54968e3a821eff79ad",
      "9bf4efb353ea4e6c9a668f0c2ad72960",
      "220d2b69ab814382921efca85f5d318e",
      "fed99840e9d4488a82ccd33be9ce9b0c",
      "0ee367b1471943dbb5f10350b9b7236e",
      "bf4e19812ace4693835f75cdd1b1777f",
      "f905b16abf8e443f97398f6a6fcf84c3",
      "417c55f97ff44b85bd09657a51869ed9",
      "553373a07d624395a500b553b839f551",
      "fbe4e57747a54cf0a9f26c5b28ba1cfe",
      "65e3e59bb48c47eb9c9bf36b4fe220d0",
      "b3a3cf2ebe2c42ce8c946b5f4f306857",
      "c426e1bfd5cf45e9a021222e39c25fa7",
      "21f503f75fc94cb78fab0e83bd9e754c",
      "64fc08787f9d421685b20d6fe55d44d6",
      "1a22fda7a7304719923ba5039fef8a83",
      "854a4395af7f4e2ba0448e2db3dd410a",
      "0c086bcea1d14a299c1bf702ee6745d6",
      "82c9694d1ccd4e769656fae72dc287e2",
      "e59518ad74a748228b18a16b84f4f527",
      "2bbbc449e0934e5bb516a65f714a4f6c",
      "5b6baabe692449cf93f199e36ab4df3f",
      "38251023a1ab46a4a2452af4c474452e",
      "0fb725e4a46e486abecc966d40bcaa18",
      "1fe94421ccea4da987d347d802550509",
      "9aca7c9a843f4933a22d073948054442",
      "0253768a51414a25a8cdbea9ecb5d02f",
      "db5236a6166a49d2b75f7551822ce501",
      "2024941feba54a4785a1d42cc9e433ce",
      "13d1da473b9a403a9cd159ec4fb202f8",
      "8c215070e88b43e6adf5c06239d6ad7a",
      "c90bd75a91484e5f85d3e77f9ce5e106",
      "15fc19beb7a64d3f98ae548b2be19d52",
      "9e1c31a97af04ce3990af8b734e47008",
      "07eb9ccae0f04c22a0aa1ade329e81f1"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "ea9935d4-2ae5-4bf7-947c-1bfc14851035"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3-4b-it\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient fine-tuning, allowing us to train only 1% of all model parameters efficiently.\n",
    "\n",
    "**[NEW]** We also support fine-tuning only the vision component, only the language component, or both. Additionally, you can choose to fine-tune the attention modules, the MLP layers, or both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "d1a358df-4fd1-41ab-dc38-7961b6e3091f"
   },
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "AI4Math/MathVista is a dataset that involves using images to solve logic and math problems, for this notebook, it will only be math problems with numeric answers for simpilicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "084bf05df14a4acebac1e89e54371fb8",
      "7e5464a4805b432bb6f8d74c299bd04b",
      "d7c3812ab1f64bf2890511b772ac797a",
      "9c31c5639c964f77849e730c06f74813",
      "77e2055956a54a5d95ea5dd45f210d6c",
      "98eea9d4e081484682705ec4e1b049dc",
      "0b1b31a601fb4279acf86fa87ed02b1d",
      "01dc895a06ec42d998053a4f683c7d2c",
      "3bfff6f28bc641bfbbd3ed42936adda6",
      "9c77bddc0b154bab9da75fab54ad987d",
      "eb7daf34ddbf4542935dbb1a25c49d80",
      "f2c105a061734373acb112e8b9ecb3f3",
      "11685223566044d0873c90aaa1507a62",
      "ad2c0388dcd8401b9d38cce871054cca",
      "50b97a124adc42ceb2c67673be362bfd",
      "250789514b85473bb4bdf91643a27612",
      "c8e09b0f1bcd498fb1e089e70906c8fc",
      "d1eb78621ce74016a17ec6110da16f47",
      "6169cc4ca62d47b78cfbec777c18a738",
      "f7117b93285a4d7c9bc9a342fceceef0",
      "0acd4cb8322f4b869d4e69545862434e",
      "a2cbe6eba7ba440d8930af5cad6f9a9a",
      "171e89138ad045a1805d4c9be9782e0b",
      "00e7bc2fd1974d79a322a6936f61d4cc",
      "b8a2d3cc4e924979832891e02ecaa3d3",
      "7d00052e5b554179b59658723c9be20d",
      "61867864031c4780bad871f62cfa850d",
      "abac4a0b22c544bc96421d55e1c0c0e4",
      "7027de5b736c4147a62420b148a67245",
      "af4d0bd535594b48913fd7a3963638c3",
      "18062a3d934945d6b0505277d3498640",
      "80eadbf62548405a9a78a0950151d7dd",
      "aa7ca7b6d183401eba18376d35fd11b4",
      "c392764ba7f6489ca746aa8735694726",
      "7b08b48076574c9396432a625c40bd54",
      "1489964509b64a049505f20852c07046",
      "68a4ec14cc144801b005b7ce0e623df9",
      "112dc1e4e76f448e9b31d4c479be7fc2",
      "f7a7db8ce318448883847e7529dd5a52",
      "23fa964905be43c1a10eb9cc9b004c42",
      "9b0ba70b515e47b3bbe3d0184c5e892a",
      "eb36054494a146328a39e2ad765432f8",
      "a868782f0df54cba98bdd24253d9cfec",
      "42c31fbd2fa8403a89c32972ce1562a6",
      "903b5a753aa34a408a71107212c4b2d7",
      "6bda7df87ff2497e9ade8d646895071e",
      "377ea3caacef4cfa87cb9f6e12b6859f",
      "e74801c43509488aa505aaf501f95ba3",
      "780d0d7d84ba44db8c4e44de7547d289",
      "39efa09efb6744a5bea52a279ff0eaad",
      "72f87cdf1c974782b8d52ccdc7ec7a09",
      "ff57cebd741b489e90cc8054905f6cf3",
      "3f88e0b954b349f398935db33df33e76",
      "d84318dded104e859e742e9eedbc9508",
      "4276ba30838e49ad9909db31c9084243",
      "1db0dd7b8d084d0a928e53bcdd1bf49f",
      "6f9e07d1e0104e839bc76d55982ec9e9",
      "86faaa71a206466e8f539bc8e2490e37",
      "a59daf2369804fcc9b98e5f7796368ee",
      "053527aac20d4b2b88c1a6a84bcec865",
      "a3f651821f2c4cee92a3f07205daaf32",
      "c738fa16be984c9ab05a92945c17d8bb",
      "7132c5ccf9f045179d0601e37f3e9bdc",
      "881f6744067a4ec3bfb8ea9b71e4363a",
      "643d9d8935954394a62a46134fbdcc68",
      "73c45cfb0eb14894a9df3be669a59fc9"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "1719c2f1-d763-41c7-ce27-78c8ef84519d"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import torch\n",
    "\n",
    "dataset = load_dataset(\"AI4Math/MathVista\",split=\"testmini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL5AQL4PCHGq"
   },
   "source": [
    "Let us see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "id": "oTioAh62CHGq",
    "outputId": "56476e32-071d-49ec-dbac-e21497f018dd"
   },
   "outputs": [],
   "source": [
    "dataset[\"decoded_image\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkUjslUkCHGq",
    "outputId": "a5e46e05-db51-426f-a9c2-699b71eb15aa"
   },
   "outputs": [],
   "source": [
    "dataset[\"question\"][5], dataset[\"answer\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fd9RmocCHGq"
   },
   "source": [
    "The image of our data is a part of the math problem.\n",
    "\n",
    "To make the rewarding easy later on, let us filter only the numeric answer so that we can create reward function that gives score if reward is float or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "72a7a44d2dec44e99f5b359f4caca867",
      "8119ae87617f4df59ff4ecce383e35b8",
      "3ad86ddddf4243ecb2a11c07de739587",
      "4e0f0dba45784c12b3f03d74bd1625aa",
      "b4cce73134494236812fc50cf3785455",
      "b23eb71b48d94658ac63696ad2c9e813",
      "83ba23d055e748c88f198fb39327cdbc",
      "3d50ada0052e470a801b81b46c4ba160",
      "cdf7c8c53a8f45638df3f3fe8bd12d4f",
      "c60470244f4c467e8e14a3b81fb37777",
      "332f9e59536049979deb98151e237b97"
     ]
    },
    "id": "Ka5ZqAuFCHGq",
    "outputId": "67c8c3a3-066b-453d-de00-9c241b6651d0"
   },
   "outputs": [],
   "source": [
    "def is_numeric_answer(example):\n",
    "  try:\n",
    "    float(example[\"answer\"])\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "dataset = dataset.filter(is_numeric_answer) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y5duJcZCHGq"
   },
   "source": [
    "We also resize the images to be 512 by 512 pixels to make the images managable in context length. We also convert them to RGB so they are compatible with TRL's trainer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e182ea774da240c99fb87758d35e3bce",
      "d63bb4ddd62e4a11882763998592a09a",
      "255113a33719461d94f8afdc91d8827f",
      "46f899cc554d4373a117f61598a91241",
      "66048b2979234c24a49f1d90a28fec09",
      "6c34f266c64d466d8d0a5083ebf3ae2a",
      "e49db50f8a83401193a8562dfb371a45",
      "b1aa5be860e94ef0890d08335d9f3303",
      "0dec022f987942409a6344c8f3e15a27",
      "0cdfc50dc61d4cf888f9a82cc3ed60f0",
      "1511a91b6abb453d8329d3c7b3f317af",
      "de052bec259f44f29baf3ca1b190edc8",
      "bd3d3f83f07b43e0a37e04db56dc7f03",
      "8a16a4cb00de4098af726845aa28f153",
      "b741c4f968aa4c308a55b928faa7cd44",
      "5f18af6019994e94b2203e99defa7aa6",
      "3fe5469fbac9418ea44381ee4022d768",
      "21eefa51dfe44b678bd83403cc6eff77",
      "6d5f7ea47d92448db85272a3c641b47a",
      "f7d199cb27b7465f90d9805209355e12",
      "eaa27ef45b364362b5cb7b6dbbe73307",
      "2182738bcecb492cbda487631f049b75"
     ]
    },
    "id": "X53sB1EhCHGq",
    "outputId": "a8977e78-1f78-49d8-adad-8fc1f761bfce"
   },
   "outputs": [],
   "source": [
    "# Filter have big images\n",
    "def resize_images(example):\n",
    "    image = example[\"decoded_image\"]\n",
    "    image = image.resize((512,512))\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(resize_images)\n",
    "\n",
    "def convert_to_rgb(example):\n",
    "    image = example[\"decoded_image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(convert_to_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEumjKEOCHGq"
   },
   "source": [
    "This is the conversational template that is needed to collate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "eb283a24a0e74468b4481486fca2a90e",
      "d2e66c321c244ffd89fd6b223ab2fed3",
      "6593a80f6a654f5f98b437a514df3de0",
      "9ea447b404564f5e87b8327fd9fd770c",
      "b8b9dc2288a24a3988ff5bf196fad76e",
      "4975de7767804e4f8e949c92bc564b48",
      "6b05bb786dae42e0b3bdca43061f360d",
      "dab8de9d65234f8dad0b5b8f94848022",
      "b4c5322887474466a37bca27e9aa114b",
      "736b9a8ebd214b1fa4f5257cc6062f38",
      "280a2a30968447adaa1241fb773e94dd"
     ]
    },
    "id": "E1GrxnxSCHGq",
    "outputId": "11d65959-eeb2-483c-9efd-0c9416c4ee2d"
   },
   "outputs": [],
   "source": [
    "# Define the delimiter variables for clarity and easy modification\n",
    "REASONING_START = \"<REASONING>\"\n",
    "REASONING_END = \"</REASONING>\"\n",
    "SOLUTION_START = \"<SOLUTION>\"\n",
    "SOLUTION_END = \"</SOLUTION>\"\n",
    "\n",
    "def make_conversation(example):\n",
    "    # Define placeholder constants if they are not defined globally\n",
    "\n",
    "    # The user's text prompt\n",
    "    text_content = (\n",
    "        f\"{example['question']}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
    "        f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
    "    )\n",
    "\n",
    "    # Construct the prompt in the desired multi-modal format\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},  # Placeholder for the image\n",
    "                {\"type\": \"text\", \"text\": text_content},  # The text part of the prompt\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # The actual image data is kept separate for the processor\n",
    "    return {\"prompt\": prompt, \"image\": example[\"decoded_image\"], \"answer\": example[\"answer\"]}\n",
    "\n",
    "train_dataset = dataset.map(make_conversation)\n",
    "\n",
    "#We reformatting dataset like this because decoded_images are the actual images\n",
    "#The \"image\": example[\"decoded_image\"] does not properly format the dataset correctly\n",
    "\n",
    "# 1. Remove the original 'image' column\n",
    "train_dataset = train_dataset.remove_columns(\"image\")\n",
    "\n",
    "# 2. Rename 'decoded_image' to 'image'\n",
    "train_dataset = train_dataset.rename_column(\"decoded_image\", \"image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqLoZNxWCHGr"
   },
   "source": [
    "Applying Chat Template across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "33228c61f8e24974bc44cbc90dac6d87",
      "c9b0ac0ca2ba4e7dad62054e633bc734",
      "418f89679b9c460b86063cd4b38b1f29",
      "b2c54acca3d5431ea870e11412acaab5",
      "d53beb936fb44dd8a7f38eca7014b8a1",
      "af11c04146b34cf580ecc7dd18e00977",
      "30d9c4d039ae46e4a80547e82cb18b11",
      "a1cadfdc7e134f2da1ff41fe6bac352c",
      "4ff0e98652cc41be87f934c53168a2e4",
      "475de0169e734038ab19ec84936a016c",
      "77771a38888c483bb9d2d00d9a56a16c"
     ]
    },
    "id": "mYS9rjqvCHGr",
    "outputId": "440aa63b-091f-4ee5-9969-25c0a0db9a40"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda example: {\n",
    "        \"prompt\": tokenizer.apply_chat_template(\n",
    "            example[\"prompt\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvON5vdhCHGr"
   },
   "source": [
    "We use a basic formatting functions to see if reasoning starts and ends as well as if the answers were written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISIv0E0sCHGr"
   },
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "def formatting_reward_func(completions,**kwargs):\n",
    "    import re\n",
    "    thinking_pattern = f'{REASONING_START}(.*?){REASONING_END}'\n",
    "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
    "\n",
    "    scores=[]\n",
    "    for completion in completions :\n",
    "      score=0\n",
    "      thinking_matches = re.findall(thinking_pattern, completion, re.DOTALL)\n",
    "      answer_matches = re.findall(answer_pattern, completion, re.DOTALL)\n",
    "      if len(thinking_matches) == 1 :\n",
    "        score +=1.0\n",
    "      if len(answer_matches) == 1 :\n",
    "        score +=1.0\n",
    "      scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    import re\n",
    "\n",
    "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
    "\n",
    "    responses = [re.findall(answer_pattern, completion, re.DOTALL) for completion in completions]\n",
    "    q = prompts[0]\n",
    "\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:{completions[0]}\")\n",
    "    return [2.0 if len(r)==1 and a == r[0].replace('\\n','') else 0.0 for r, a in zip(responses, answer)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNI3AX33CHGr"
   },
   "source": [
    "Here is the first example prompt in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "ASl_f3bqCHGr",
    "outputId": "1561d7e4-2983-4d7c-f706-feb0b8a43c90"
   },
   "outputs": [],
   "source": [
    "train_dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSMu5MbUCHGr"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4m61cpgpCHGr",
    "outputId": "7f83e515-a06d-4d58-9b53-956601afd7b4"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 1024,\n",
    "    max_completion_length = 1024,\n",
    "    #num_train_epochs = 2, # Set to 1 for a full training run\n",
    "    importance_sampling_level = \"sequence\",\n",
    "    mask_truncated_completions=False,\n",
    "    loss_type='dr_grpo',\n",
    "    max_steps = 60,\n",
    "    save_steps = 60,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir=\"/workspace/outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dB_J_7vJCHGr"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "44VvQbLRCHGr",
    "outputId": "1a8ac05f-f196-4b2d-8591-e31bfd5501a2"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # Pass the processor to handle multimodal inputs\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[formatting_reward_func, correctness_reward_func],\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can modify the instruction and input‚Äîjust leave the output blank.\n",
    "\n",
    "We'll use the best hyperparameters for inference on Gemma: `top_p=0.95`, `top_k=64`, and `temperature=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "435e14f5-a138-44e8-8d80-bc7172e3981f"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "image = dataset[100][\"decoded_image\"]\n",
    "instruction = (\n",
    "    f\"{dataset[100][\"question\"]}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
    "    f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, use Hugging Face‚Äôs `push_to_hub` for online saving, or `save_pretrained` for local storage.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "09ab7b36-3cca-455a-db67-c5ecb3796833"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "1522786e-8de9-413e-8c75-8b21af39cd8f"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastVisionModel\n",
    "\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=True,  # Set to False for 16bit LoRA\n",
    "    )\n",
    "    FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "sample = dataset[1]\n",
    "image = sample[\"decoded_image\"].convert(\"RGB\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    f\"{sample[\"question\"]}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
    "                    f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Select ONLY 1 to save! (Both not needed!)\n",
    "\n",
    "# Save locally to 16bit\n",
    "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
    "\n",
    "# To export and save to your Hugging Face account\n",
    "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
