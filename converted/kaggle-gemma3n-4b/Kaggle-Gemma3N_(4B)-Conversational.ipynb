{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c57345",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Gemma3N (4B) Conversational on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3N_(4B)-Conversational.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Gemma3N (4B) Conversational</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3N_(4B)-Conversational.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pip3-autoremove'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'unsloth'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers==4.56.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps trl==0.22.2'])\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBN09c1tUlSV"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps --upgrade timm # Only for Gemma 3N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675,
     "referenced_widgets": [
      "a18e4f1b115043238b23dfa89d4aadfc",
      "b89a448d6f6a4740a6669e97362ff010",
      "d523dd9488c74a9ea7b54b6ded8d5995",
      "450e64cd49264777a7aa9b21340cd7f4",
      "f73eeed5ba2b426fb221945733e48a5a",
      "59218d73bed84d77b48ea4560f759634",
      "c55fec1fce124357bd96dfc09e0f77a4",
      "32a3f300caba43e8bf90e29a8729c0df",
      "cebf26dc6eba4886a20888574ad69fdc",
      "a46ffff5a95c489ab77f538cc0a776d1",
      "e15f8583441842d8b9bdc4de9b590950",
      "c35e11c09d394901ad68692c6ef7c7bb",
      "dd8e84990e7f48989c7b544791d81f30",
      "89cead435c6b4a3b892c375efb939f60",
      "3ad1ddfb8c6f43e2909ab08b72d10ec2",
      "35073dbb5bb3401098440c62593a1121",
      "8ae4688e9e0d400b9e7a1ebaf3bc651d",
      "025111e631d441a698ee21eb9ff6b8cc",
      "69a13669ce0248fe9d982ce2aa16b433",
      "0b48ef5162b74fadb6379579925016b4",
      "c23a8bea7c93423b872f0a5a17c157b3",
      "5474bc98d61f4c6895943cb44f423ebf",
      "63c7b7c15daa49648f201a553ba50a79",
      "4e33739ea8cc48db9807e929398f1aea",
      "1f51323353374ac0a38ed868a69a802b",
      "7cdbd097b9a64ebc8a881091bcc97dd4",
      "b7df867afe78440093e695c739cae8a6",
      "0206f33a5dcb4007971ff0b91d90c3e6",
      "ad4068f23d5e466fa3020cc8b7ff7bd6",
      "34107eb5765d4b2982588d55995dd6b6",
      "1753743eff4946d0860e25d6e7e72232",
      "a6c2c09075054183b6337e10b8f649c9",
      "f5742d6a67544e59a9f4abefb736491e",
      "8d1b37bac59f4534b2719efb9a1c9831",
      "009a69071608418c9690fc7bea80ea5d",
      "9578b43d681d40088ff2a3f39156b4a0",
      "9797e597021e41e6a9b7e0bc5e688d08",
      "3fac3e5ab51d4dbf83be599d258fb04f",
      "d66ecbc2c4664427bb85cb4a3ae629a4",
      "b6586c2ddfcc47ecb9175058ec70f220",
      "f1d0bf55f70049b3a183b7a5282ee7b3",
      "61c7054acbc64bb0b280e2ab689d1528",
      "03bd2d4b756442cebe7f92c264881529",
      "6bf78342bb6b48f18c47fc143a44410a",
      "7f5dbf97536b45dda3109887a137a4a4",
      "6528e73f5dc34298a308fc4b4b8e4fde",
      "653c213b43744436904f70ac959fae57",
      "e8b16df11064454dbbe2f40f88c559dc",
      "527cea501709468095bbe1b254e7c3c0",
      "fb57e4d8d515498c963504d21acaaba8",
      "f1c5f64d3b98416b9503b204d1010364",
      "1f70e0dd74ce4f65ac95a4429b41557c",
      "e3fd3cf3250440249c44df6d88f8487e",
      "2e2f97f0db284c6ca68949f0b627b795",
      "d13e712c163445da9cf352331c75efd5",
      "27939884e1e344ed9287569785a5089b",
      "a00eb4bc406b4fa29a366b47dd7c5f86",
      "3667372eceb04aa685ffdd81800c2410",
      "00cb8fa0460a4136a0a8bf5e2153a76f",
      "6661d4f703d449a3929721b50a081da7",
      "0dd0e55d29294132adc738521f88d37c",
      "658c5c919a594d05bb7bed9beaad4819",
      "8b9562654fd84648aef429fd7e958fdf",
      "d8e370c8648942c0b5d8a4d2e3eb5b27",
      "d7d67d83b522461290d1881e8c5b60ea",
      "ab83d29573b84bc08dc0af8d1aec2e89",
      "7b9af692b0c3495e8a14f277aadd7dba",
      "6f87ee3ef23f450a84f5b209d7b51875",
      "e2eeb6b0ce594981ba142fc0b1639c3e",
      "b23cfce1d7e8491aafcefc9183dcdf82",
      "6f6d79e974514fc88c4b3e7a222257be",
      "c986bc76882a4bb49e24078428d3dcd0",
      "9709b2d8704d4caaac2d0b823ffe261f",
      "f8a95390e1f24d919ebaf1ed58feed3b",
      "946b257eaa504727a418d6a7ed2fa425",
      "06ad544485aa4baeb122c8349d852794",
      "5215dbf687cf4785a61c3e16c3a337fb",
      "8d3ce008ce5e4183852edf7e3187f05b",
      "b03b2c03908d40e38c43445b8c446f5e",
      "ca75c12f06d24f21bcf6604e6f0397f4",
      "47ca41fff81c43ae88009ff5d09db64e",
      "1ba27593feb449018b37cd4924d479a1",
      "66300c9d5358483182a26d079fed510a",
      "c7a3a70197fc4cac95a654076eda6738",
      "dd1da0606a6346b4936d9ea75d8a6f19",
      "ba8b8b2d78d849b28af170dcee0db478",
      "9292da388d2546c9b3fa6ac0e505df20",
      "202f306725a647b380c383aa0b5ece6e",
      "b800f7e606ae4e4a909210f30e04ea49",
      "ce85137293d04558b0f2b5697f0a123b",
      "463b74158c514bd599c4d34b1d72e996",
      "a5d9877c6973411dbea99261043a381d",
      "d1b39de83d97474e9857eb9753f6af4a",
      "4701566b458b428593b7a6e4ed21c7f2",
      "d8eb169c55104e35ab744c37073d508f",
      "3da0efa9aa144025ac7016c7f65dbc15",
      "e0c818575a7442fa9f46b264e4eaf664",
      "1fa98ce95a2841abad06f6e9c3a19164",
      "89f9f2f2e6024f5bb401ff56dc5cbd48",
      "371d7b83e61b4dbe87169c04056ec7f9",
      "8f87e23fff5b4e0398b8806900c23d89",
      "2149fe964e17402eaa61a2160897a6cd",
      "1ae66757c8b34d46828cbcd304b33c03",
      "eed5c6058d934c1aa1b1e892ab25811c",
      "18003f900be14303bca6a501e03fce41",
      "bd5f119da8f742dba20a4371ac3f35ca",
      "7c2894c2e7b244bfa445bff739327568",
      "960d57cbbe704b50a88cee957b874826",
      "c0b79ef54987489d91ef01cdc19b3175",
      "5e0ab0e8fc144d27bc5834dd16b20ef6",
      "a08785e4b8944943aa94235b484053d9",
      "bbc7b60f54df43bea8ff7f4b8ae8e0d7",
      "5ac7f075c5ca4178bc2f9341663a7f63",
      "ffa1527a05564922bd53a7310661b151",
      "40721bbaf61b4c8e8114142be433108b",
      "e6d5bf2dcfe5486e889a5dc3ce32bb8c",
      "89858db5d694445ba3cbe37a573f9253",
      "f5f1f4a30893411ba7fa823c541e7338",
      "78d4fec912844afe8dc9dad100883e09",
      "a4f6fb7e33c1420198017d98076ae099",
      "48fa153e76e84a2891a406d381b25ad3",
      "e016f5805449446985198ad9408518e6",
      "2013e517ba6b4c99ab4f214a1fbfa356",
      "8f0ee4dcfed84d16bdb6360e1e6f316e",
      "b30ef78f00f14f9f85135ebf4b3bd022",
      "7faddc5beb3b49af8fd324f80f2505be",
      "88ab0ec7df484277b739480e9ed6ae43",
      "11f3d888a42949dd884e897b424db91e",
      "220adaa2a4ed48bb882140ef53c7f93f",
      "b91a65879a5045d1bc6bfae5846e91ec",
      "930cdd33b446423595752487536c8827",
      "8be585eeaace4a56a875cbe1e6d32498",
      "0b712b69eaf440b58bdcb2d026c38320",
      "631a3920722f4c9aa8660a96753b69a8",
      "37ee95f247ed4d31ac663838179136a2",
      "4e99df46e6ef4c9e9b0b83f3e81c8ecb",
      "b3bd7af0d8df40b39bc735e69480a1c8",
      "7b531f3c8daa45cbbe2e60c7dc4471e0",
      "1e1e0903f1df4972824e67373f176c05",
      "c160c53620b64c04bc4f281d3442a28f",
      "cfc5208be06f42dcbd0409fae36e1aa9",
      "a831cceb89eb476288082efa7b426a9e",
      "1ecd97cc62fc404e81fbb29b3865f382"
     ]
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "2796d292-fe2d-483d-dce9-caff7af34e83"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixr4dyTHVIcI"
   },
   "source": [
    "# Gemma 3N can process Text, Vision and Audio!\n",
    "\n",
    "Let's first experience how Gemma 3N can handle multimodal inputs. We use Gemma 3N's recommended settings of `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UsfUPU-oVQYu"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    _ = model.generate(\n",
    "        **tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\"),\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2-ddk0CWeTA"
   },
   "source": [
    "# Gemma 3N can see images!\n",
    "\n",
    "<img src=\"https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg\" alt=\"Alt text\" height=\"256\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jGeSb9bWe0k",
    "outputId": "18d0e1e2-5acb-467b-820b-9df46438c28f"
   },
   "outputs": [],
   "source": [
    "sloth_link = \"https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"image\", \"image\" : sloth_link },\n",
    "        { \"type\": \"text\",  \"text\" : \"Which films does this animal feature in?\" }\n",
    "    ]\n",
    "}]\n",
    "# You might have to wait 1 minute for Unsloth's auto compiler\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh0BzbZPWtRD"
   },
   "source": [
    "Let's make a poem about sloths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3ExuK8cWuT3",
    "outputId": "c8899856-325d-4b40-bbf9-f08de5feedc1"
   },
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{ \"type\" : \"text\",\n",
    "                  \"text\" : \"Write a poem about sloths.\" }]\n",
    "}]\n",
    "do_gemma_3n_inference(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZrmFRZpZtGf"
   },
   "source": [
    "# Gemma 3N can also hear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "68crYajNZtw1",
    "outputId": "997423cb-e6a4-4443-ce87-89e7953c40be"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "Audio(\"https://www.nasa.gov/wp-content/uploads/2015/01/591240main_JFKmoonspeech.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3vrdoa0Z01X"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['wget -qqq https://www.nasa.gov/wp-content/uploads/2015/01/591240main_JFKmoonspeech.mp3 -O audio.mp3'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJr_D4O9Z2Zh",
    "outputId": "b08643d9-6c1b-4dda-af99-8589940e7dcf"
   },
   "outputs": [],
   "source": [
    "audio_file = \"audio.mp3\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"audio\", \"audio\" : audio_file },\n",
    "        { \"type\": \"text\",  \"text\" : \"What is this audio about?\" }\n",
    "    ]\n",
    "}]\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L15JuAmmaOkB"
   },
   "source": [
    "# Let's combine all 3 modalities together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is37bsDZaRwV",
    "outputId": "a198b9b6-8670-4383-bd4f-eccdd2e5e652"
   },
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"audio\", \"audio\" : audio_file },\n",
    "        { \"type\": \"image\", \"image\" : sloth_link },\n",
    "        { \"type\": \"text\",  \"text\" : \"What is this audio and image about? \"\\\n",
    "                                    \"How are they related?\" }\n",
    "    ]\n",
    "}]\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw5XPyYFajyM"
   },
   "source": [
    "# Let's finetune Gemma 3N!\n",
    "\n",
    "You can finetune the vision and text parts for now through selection - the audio part can also be finetuned - we're working to make it selectable as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "41e67287-1439-484f-c866-a6dab36a89d0"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Hello!<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Hey there!<end_of_turn>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQkXuGYxbJ-e"
   },
   "source": [
    "We get the first 3000 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169,
     "referenced_widgets": [
      "1a5f63e9aadd42a98dd323d8afcd6f81",
      "c52e7f6043bb4e09998da74beb9ff375",
      "52d55892494449fca142c9326f91d869",
      "82b84bff5f5247d6828565a7d66e5652",
      "ac7476ff81cb46cc90f34fb91b948fdc",
      "b13867d1757e4701979e369779a36b1d",
      "248c1d4fd7954504a162cd65e2654659",
      "db3c5e83433a4695a2eda04522adb860",
      "a74d374942f941bdb95bbf8183bfe561",
      "15ffea52c16541a7b5608a5e45065645",
      "e5d96f8995ff45c4b8c1dfaa413b1cb0",
      "05188c52d25f4122a15616def13bc6b2",
      "9eb43df67c384979a683247a58491c10",
      "fa7c4ea60c974f50acedabc2d39c7ec8",
      "789ef01b508747449146c9cc9e9a1ec3",
      "e11b9d49eb3b48ccbc1cd7c2a5a58e5e",
      "d164d30c9c944e5da83aa012cb8492b5",
      "2505d695a3284911be8d2f87a2210189",
      "67b7dbd6f45d429290d2491804aa152c",
      "46c6bb01e8834b6c8043a47b39dcc996",
      "bb9ec874ecd941a39977288772b49e4d",
      "7b25a228342647579be74885507b7b79",
      "563dd15c351a4d9ebcd253a0f6f4f711",
      "3fdd2677b12f49eea4423e41ed313677",
      "5472d226dd5446939549c4479da544b1",
      "065c923a447b43feb98c160b2a9931ba",
      "cbead554d2e94f1fb9ad816456bfbd91",
      "7f6fe07a34ff498b83c858a598663c50",
      "7961267807694e3384974b8734ac8cfd",
      "a692b687ab054611ac084b68c07436d8",
      "572915e568774e25bf6d23fcc9ff5ab5",
      "a26e8d8d74434037a2855b4205c5e16f",
      "5b5b02f392ae487b8e228d136bf1bbf5"
     ]
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "24ba3102-09e4-48d5-b0b4-3035c8bfe0c5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train[:3000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "ab27c043128341b68316a68a12e9e076",
      "2e2ad2cc081740008faa6702f5f59b44",
      "a759e80646a04569b623055621ec7c2e",
      "392cf965aeb64b60add7e397200c4866",
      "4193d905e8294ed086f5b1902a55f443",
      "f740d4720e47404fab6a302c70647d96",
      "deb0fa3589ea456dae2034da0f3c8c07",
      "be0519a0fd5d4da5a38891860d91a827",
      "955a0a85cf694f03bfd9875c7f1be2ab",
      "d148202b13274d99a055ddfb94680b64",
      "c978550e41f1436bac7e5123855b1495"
     ]
    },
    "id": "reoBXmAn7HlN",
    "outputId": "3ffdb364-f90d-4ab1-bb6a-4a9eec9e0002"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "Let's see how row 100 looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "76a882d1-3978-4163-8de2-91ed5cdd12a0"
   },
   "outputs": [],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xs0LXio7rfd"
   },
   "source": [
    "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9f0054a51d9340e2907e53a80785b296",
      "ebb491b75b1c415c80da59551defbd80",
      "b43e8c761c4347e1a01c7aac5a937b9f",
      "5080a79d9096444fa7ec24615b0d007d",
      "ebb46c3a3eec4f14922bb2915e613bea",
      "eacfcd95ef1c4b49a6ff8897b815efdd",
      "35289ddb889645dfbb6c1688d8622d38",
      "d880e1aec916469c94b4ffd907953f1b",
      "bf561e6f36734965b3b3ca7b1049f6b6",
      "23eddb8e8a1144258c7e6e409576e299",
      "769edf9db000411a86fc4988b9940106"
     ]
    },
    "id": "1ahE8Ys37JDJ",
    "outputId": "094ebc7f-397a-4140-9a1d-82ee433ddc35"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "Let's see how the chat template did! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "gGFzmplrEy9I",
    "outputId": "bb37d7ea-e713-439e-f099-4d7deb9b3102"
   },
   "outputs": [],
   "source": [
    "dataset[100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "48fdf447a1824adb8ee0b3f22f95b610",
      "501c9d8673a9449dacf217b24912dfcf",
      "64ed5d5d40924b52a1b93e52624306c5",
      "88f995d2771c4ae2b6a8a26669634d1d",
      "3b8cf2a785e14020b020284880f8b726",
      "f4ac75045a5247fa913c47271437a910",
      "c2cd9da6de8046bda143e1bf07c64913",
      "d54c4382a5c44cc5bfae0f303dea15d3",
      "391c6c8e88b74a97969e523513d16e96",
      "ceb779b86cee4a168af951d0e5d9bbb3",
      "8085aa45c2f64a6395c3ddfbc450accf"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "c3130963-d202-470e-832a-e1b604f0536a"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "42634f2e7a7e4d5ab0992bec45e6607d",
      "17c8a4feff59499f86d82a09ab54c017",
      "5c570bdcfcdf4e57a49ae752ceff50f2",
      "1d1fb691357544a3a29032b14bbdd6b9",
      "a4d8211a756646119b748641ea5d4b83",
      "d25c40d318e24f7c832f4ebb9a884b9e",
      "286de8ee29bd4c77a32446c2194777ef",
      "88575c0ee2fa46349ac642642b37800f",
      "b8c2462293dd480c85503bc08b194e09",
      "2062ecc4fca44ad58b8963e84f7f09e4",
      "3e5bc0d900e6454ba7d1d6a30839fa01"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "f25a861e-5ec5-4c2a-8e41-319522a73c0f"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "LtsMVtlkUhja",
    "outputId": "bcdfba15-5bbf-43d3-c8f0-cf2355569af2"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "a4235fa9-2fbf-450b-c93d-ec6cf374504c"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "99592e08-f375-42b0-ecb0-0acf240a2720"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "# Let's train the model!\n",
    "\n",
    "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "8c368cf7-5f59-4de1-8a42-b5eb30c14ee4"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "14236050-bc1c-4ca8-a9ab-1f66635839dc"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "758f6ed8-6c93-4bd5-aa3b-16b6546c6e1e"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n",
    "    }]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "1dcf3cec-7485-497b-b19b-fa126bcbf223"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"Why is the sky blue?\",}]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "f8bc2909-78be-4999-d1e8-b9fc72d2c377"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3n\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3n\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3n\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3n\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "c428817d-ffe8-4470-9ed6-4fd5118d4bca"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastModel\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"gemma-3n\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3N?\",}]\n",
    "}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3N-finetune`. Set `if False` to `if True` to let it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3N-finetune\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6O48DbNIAr0"
   },
   "source": [
    "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV-CiKPrIFG0"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_ACCOUNT/gemma-3N-finetune\", tokenizer,\n",
    "        token = \"hf_...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3N-finetune\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgcJIhJ0I_es"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"gemma-3N-finetune\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        repo_id = \"HF_ACCOUNT/gemma-3N-finetune-gguf\",\n",
    "        token = \"hf_...\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnz9QOYTMvbH"
   },
   "source": [
    "Now, use the `gemma-3N-finetune.gguf` file or `gemma-3N-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
