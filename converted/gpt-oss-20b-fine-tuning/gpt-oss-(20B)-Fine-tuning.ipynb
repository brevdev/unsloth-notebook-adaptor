{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97a086b",
   "metadata": {},
   "source": [
    "# ü§ô gpt-oss-20b on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">gpt-oss-20b</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">A100-40GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">24 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">reasoning, fine-tuning, large-model</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvO4R73aUesf"
   },
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1SrRxzzUesi"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waDcYbdVUesj"
   },
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McoI2OmhUesj"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXL5C2w_Uesk"
   },
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJq3z_gYnVgd"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through a finetuning example. To use our `MXFP4` inference example, use this [notebook](https://github.com/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567,
     "referenced_widgets": [
      "741f0639f9b940b18982c7e11bf2776b",
      "1984fcdb990545409b1eb5efa83fb74c",
      "c3e145a4a02146c6bbdbf65342dddfb7",
      "45c1fc1922704d31acd78586f2f24e3c",
      "e24ce0dca3244d22b1a2d7763cb6c082",
      "7443c0fd5e6d4d6685f2065fb2e2ba70",
      "50c54564844148d9b3e7e6581b378581",
      "2a84ece06b7947e1a98a449cf85a936a",
      "0b31b86c64ee4829a00186c84dd3f02d",
      "8a264877643e414cafbf13a2a858e0b4",
      "8fc2f79476a042139f9d102fda3659c1",
      "465f616098e84535a8bd81e4b7bc48fb",
      "c3a1abaf8e6a4aadb08c47ab1ecf31cc",
      "7ee8e9e299094075a0bef74f34d98148",
      "a3a8773128d648898cfea66e243f8cc7",
      "95a60ff99a894b79a29f41f21fe5fc0c",
      "fa2247a1b9764b6eae21a8d9a2f8b209",
      "3fa70e26d4104b1085d00470aa1cfd0b",
      "d1c44e87bfec4f449006331a90e18d8b",
      "0792d4b357fa42b18440e7a5da733303",
      "c382b9bd34bb454b8258f7449fd474b5",
      "8c101696848249068d030a96e0c6c60e",
      "27713c97dfed4e949eb4a867f4b8b49d",
      "34b691734dbc4ec8b6310e311dd8fa51",
      "8b45a43e31de493a929ba37dad775b5a",
      "ccbc409c003941afa7195bf3dca118ad",
      "85ee6a7e8d284a57b025384532618248",
      "1f6439128060443191f6b30202db795c",
      "7fea129bb7b647b59091ea7a9a87d3dc",
      "64a6d168d1df4d779a071a76b06741bb",
      "656dd2412f0c46dd97df8e1868546bfa",
      "f1ab8b3f3e284ee289dad96e4210c941",
      "4e2799787cde488784e0fa01966498bb",
      "d84e6a0d2f4041489f9712625fe39f62",
      "4fd0890942c04a6e847f4f970519776c",
      "ade025bedf0840bdaac3919c4410e0a0",
      "04cf24bb9fc348bab0af44e19e0a1581",
      "1905cb49c1a2432995ede8cd4378bc50",
      "8d1e3b6922704df88b10ae3142de4b47",
      "63b2591a017d4ff198929a0871a51093",
      "a496796cfac1448895eaf759c28371bb",
      "9bbaf65dd04f4f7e98da1f25cc5eafc5",
      "d85c53a503804efbbee2394be0712729",
      "b40cd1b255a54d1bbd48f2da08ea819a",
      "685c24e719b049c0906d4394761a5cef",
      "ba1dcab9ae5846aa8e55698df9bac399",
      "ac72477a2c2b4c238c9d5d73d9b9ebe6",
      "e239e2d42b5e44fb8ba357b6d0729999",
      "860a0e5e090746bb933a299f11151bf1",
      "abcb42dbc96647a6bd495d9ba77391a2",
      "6df6d559f05e4f6bb0d9ce499e0b1a98",
      "910c66799eb04ebb942f5c3f852a04a4",
      "30198a658913418180698a3f4fb17447",
      "aab741d6bc724adabf00d78d8553acad",
      "eb9fc88e72b74b31aedc2227edfd5a5d",
      "82b632689b9a4326943bffee42a9896e",
      "4e7e7bdc675c4aa693ff132d200f4129",
      "1683f5ebdbf34ab18423dec107582a69",
      "e008d801be094721a4061f3f29712a1d",
      "8abfbead57c9454e839491871a757ee5",
      "84f57051afa64509aaf5b66a0b688504",
      "d9fc14064e4a467fa3ee9ca997ffc49f",
      "49ac15e4b07f461c9d184c20bb684ae7",
      "a45a54bf8ddf4dc8bd0f218bd9b0332f",
      "362a7ebb91b6460fb24b1f7f14512f62",
      "bb03f539d0874ebf96071fae3ef0903f",
      "816a80760fd54840b1f7eaf04040636a",
      "f5e8d86766b041a0b4fec413492301c6",
      "4a66b779c34943d19d9ffa2228ef7aa4",
      "d8962a743f4a461aa334edf64958c0e5",
      "e7b901dd9c514c9bb51e6b8a68cd9c22",
      "08cfeb587faa414381fd3c5a922da5ec",
      "8aaa5a4605714a588b735c58b062df9a",
      "3bbf61f114aa4905a054fe4b17a391ef",
      "4bf17000a346405292d1d0066c4a1e8c",
      "470b5d6d93e446d0b7e2301b2b77836f",
      "0fc5d95258bf472b8edf7e97b0da25a5",
      "da9d41a751b3443d8c2ad316b690e2c5",
      "6bcdf81a29f749aaa094b468e07b1053",
      "fbdf347653fd428c87468860b326b4a7",
      "08b2a042227e4d28880f62241f352c46",
      "53ce128bbac24673a7385c1e780df17d",
      "a5ee688f438b4eb999538c7cbcfeb086",
      "b8faf4a8f1814ec1989f0fd89ace476e",
      "021a1f03b895489f8dc237004f748d1e",
      "08c1764425ae43249a86b2e4ff57bd3a",
      "c8a43278c2184ff4988cd0dac4bf3409",
      "0542d27515794cc081d2a5b1153dc16b",
      "45b3c06156df45339dfa7273390cbd75",
      "7e6eff01b238491e91266ed1846808f3",
      "c9b9b17ce642452fa3ddb8fd9d524212",
      "6992ff862b70481cb6830519462595f9",
      "7328a60db40f4ebd81c4f2864a280e32",
      "bb2534bf08ac44238fd0985016cc2c2e",
      "9bfe8b11c8ce4c21a29975d2e0e5516a",
      "2a6364af8bad430cbeabe700b7b75e32",
      "c21ac23070f443fe8a85fb346a4f9ab2",
      "113957394b44469f8b2e3b2c05275a37",
      "4a27c3c5e5d24cbca7bbca7535bc7d51",
      "8f571f1ac98c412b94a7f8a33e20a329",
      "888a63e07c9c4f8f8ae1d3cf9c502efc",
      "4e33ed051bb84af5aa6256f041d23968",
      "7302fd70bd994814b227cbdfad1c524e",
      "2f2ad958f36643d4a19b274c13dfbe0d",
      "513df6aaa32e4cf4a09dbee53f114948",
      "dbddc05c3497405d88d6a30ec1fb912f",
      "06293d8f8b6249a2b3f16bcea4237844",
      "32b06e0fbcb645e8a3f507bf7295f7f8",
      "31274bda9fca483483f9c038e5e2a8ca",
      "30754c4e8c324a11b915c8fcb89921b4",
      "d5dbd14dee3a46c9a5a59deca80f6717",
      "aee9504237a244a2a3d886e8e154f4c3",
      "418b91821a454b9cbbe7f024e39615ee",
      "74a8b3c929a34f57bfe1d3d8536de79f",
      "118b98d118f548f8ab2ac4cd6f69d478",
      "071bebbf6c14430a89d44ed92fbf1508",
      "af199ad58bec46c6b669f9107cbd46e4",
      "d0cd25f14ca445f9abb1eefa2dd918fd",
      "bb178448d49c4a5e903df3ba95ad428b",
      "c465a2652eea40c283fabbf64f7cf1cf",
      "8f30960fd4ec4fa8aa0f15bebc0b6329"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "7d1f0b4b-bfca-4fef-da0a-89a08f904320"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    dtype = dtype, # None for auto detection\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVqtZVxxnVgf"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_LK81NRnVgg",
    "outputId": "ac7e8417-e14a-4040-ad00-420fcf5a8e54"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-sFShVvnVgg"
   },
   "source": [
    "### Reasoning Effort\n",
    "The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n",
    "\n",
    "----\n",
    "\n",
    "The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n",
    "\n",
    "* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
    "* **Medium**: A balance between performance and speed.\n",
    "* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxCi64FnnVgh",
    "outputId": "4a3869f1-08b7-4534-bff1-e346d402b547"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"low\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlAzq_RinVgh"
   },
   "source": [
    "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaPPyXN1nVgh",
    "outputId": "f279ff3e-2a0f-45de-d364-16f92dba6552"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"medium\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0iuyJt7nVgh"
   },
   "source": [
    "Lastly we will test it using `reasoning_effort` to `high`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrjUXjN8nVgh",
    "outputId": "d45f2451-71cc-4fda-8ba8-959f27657e66"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"high\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6BnnYcbnVgh"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91gfk9L3nVgh"
   },
   "source": [
    "The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184,
     "referenced_widgets": [
      "2c2883d0c1704432a8c1e6e44b7ae875",
      "efd9d697ff334785b5d4fdb5b935508e",
      "2366c85251274f48bb2e43ce2378671c",
      "cc57646b3a7b4c7d8d8f9b830f9fd531",
      "5d01c9216aef49409c7c96ac2d7a17d6",
      "a8b9c29d58874d488eeaeb0aa3b1e45d",
      "fd36f2df14a1475b828707141df5e544",
      "f6a95996ac9446799eb87bdba3510256",
      "47d26005dc624e2c87281e766bacf892",
      "71e78d08354b40989612af283f58a1d2",
      "e4e39b77ae8c4eaa8cf77f3645c63d34",
      "a335575c3f244a14a8b25c922c1e58a3",
      "2fa6c740586d475ca9497df70226c474",
      "3acc4dc1663e460aac1287fafd91fbdf",
      "d01ec0e3bb014eac82dce4d757413826",
      "91876ada8b444a4ab4a04466ff32bd94",
      "a6d7b6d3b6af4db3bdad9498b4a30d14",
      "a77a3cdc6e4942aa892fb71c96177a35",
      "0ceaea82391840f5b3fe1f1e3ce0488a",
      "91b6caee85b44d118294afe5a0ecdfe5",
      "864badb76fb54147b571f5e7ba057e18",
      "db76c780b3ae438f9866f6abbd350667",
      "7124b9e1a6f84a32b24dcfb70fed70cd",
      "12e5c793b9c04234bd8a20f151c9d177",
      "3c9df0776ccf47808eb9fbc5fec6e4eb",
      "72784b1ff3c24a8d9907495d89b8de9e",
      "98e08544f19041bbb6e9a599864750d6",
      "93f61b7c6fea4f69bd62016d3d7c0948",
      "bafebb51424044d4b9d06f6ac16c952a",
      "1bab42e8f2df469fb12eb5cdff5ef66b",
      "9925c091e4a5400ebb1eee5d7ad44525",
      "b847333d8f384193b634b8dbe63b4e72",
      "4935d891baa147a0b671ef3deb74634d"
     ]
    },
    "id": "62QfuPXBnVgi",
    "outputId": "c0ae9bb6-217d-4300-88df-577c514e9072"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoVaL_i-nVgj"
   },
   "source": [
    "To format our dataset, we will apply our version of the GPT OSS prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "1523624326f247d5bd8bc723bcd2d3d9",
      "456a78b49ec64eae825e55bb1aad4bc2",
      "353c36ef010a4b8693f11c8139db9d57",
      "0e8a6e3abb96460494a8c00d729d1437",
      "a17b289b75324e3195b240041c683eb5",
      "9bf4d18bf2c04af1bd28b99ecd1a7823",
      "d4ae6a9eac014c7ebbf121e6fecf0561",
      "11f800ac163e494b968265757318f9cc",
      "196f737c5ff34ab38b9889c43932712f",
      "299c683225944f17bf22603e95261a16",
      "985bb4bcf09f49b1b605b4dcc51f2e98"
     ]
    },
    "id": "FW-l11GBnVgj",
    "outputId": "04d0d8cf-0e97-4d51-d225-49ff218820f4"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1EmcWNinVgj"
   },
   "source": [
    "Let's take a look at the dataset, and check what the 1st example shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNgkE5QxnVgj",
    "outputId": "fc6fcb41-4a70-420c-c491-15e8e7f8a5d2"
   },
   "outputs": [],
   "source": [
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ3i-AMFnVgj"
   },
   "source": [
    "What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtdsxyl6nVgk"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "8db3d9fa93694512acd724880cfa8dfb",
      "03a54920a9a7401ba87447275faf0961",
      "1eb1fd494c3f418cbd09c5d59f0b7c82",
      "80ca90657599453990ace2416a4183f2",
      "c4e45a7897c049d2b7f67178e93b1833",
      "12730b1382fa4e83abed6f32f35bb5d4",
      "944639d6efce481381ced9a85cea3dfd",
      "d8baf776a9f6406a87b5539413b9e36d",
      "6cc633ebe508463b83038b51fe8f42af",
      "fb31a66bafcf459e859a215637b670cd",
      "bfe000b6b14045d2af6e3aec2961b008"
     ]
    },
    "id": "O-XZLeLYnVgk",
    "outputId": "9d1c5775-4749-4263-b1bd-ebaa8baf4ea7"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wux_ogwXUes1"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes and lower loss as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "979bcf180c7e47aea72283ee230ede65",
      "4e28cb5d9fcf41b28fd5e05fac7ea4e6",
      "6892c80679714bc99c42f6ba121875cc",
      "68a6ae81acba4ac0bce64f36076fbcb9",
      "38e07fbbd7254828bde0862a07ee045e",
      "a2f45045e68542f2a4f5b948701ae831",
      "babb568764cd4f64b0e06e28116aaa05",
      "8c887da857cb4446bb7abdfb76930b56",
      "4c679dfdcd2646e3913be3f0e29970e3",
      "576257b0c537476faae2808cf84f18f4",
      "70682ebaeba9444fac87b4a7afcaef7f"
     ]
    },
    "id": "tuPxsV86Ues1",
    "outputId": "727b7621-17ba-4777-b6e6-5b3d9a3814bb"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "gpt_oss_kwargs = dict(instruction_part = \"<|start|>user<|message|>\", response_part=\"<|start|>assistant<|channel|>final<|message|>\")\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    **gpt_oss_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRBSbQbaUes2"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "FDWuDdlTUes2",
    "outputId": "41162013-1e2d-44dc-af3c-f6f854b0da16"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rI4bYApUes2"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "0s8tgYAeUes2",
    "outputId": "59e4d790-c1b8-42be-c5e2-0a2dd1fd85e8"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOXwyYHLUes2",
    "outputId": "0c8730e5-4289-4a82-b775-5a363625a5b9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6mt6yqSUes3"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aFaejiSonVgk",
    "outputId": "07b77bf7-d4ae-408b-fa68-5ed7543623c7"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G3eBV3EnVgk",
    "outputId": "fe855757-9537-4c29-9365-db76209b0573"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuK0hVOsnVgk"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdVCmTuBnVgl",
    "outputId": "d04b95e9-6a9d-4590-8ae3-a614f6ee62ef"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"medium\",\n",
    ").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e1j8KRb4AwO"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds7ByU7e4KF7"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_model\")\n",
    "# model.push_to_hub(\"hf_username/finetuned_model\", token = \"hf_...\") # Save to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELyXzRpl4hr0"
   },
   "source": [
    "To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCMDSxvD4SKu",
    "outputId": "6d85001d-7f53-40df-edf2-4f626e232390"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"finetuned_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 1024,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"high\",\n",
    ").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBC14dW3Ues7"
   },
   "source": [
    "### Saving to float16 for VLLM or mxfp4\n",
    "\n",
    "We also support saving to `float16` or `mxfp4` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vtLk1WvUes7"
   },
   "outputs": [],
   "source": [
    "# Merge and push to hub in mxfp4 4bit format\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
    "if False: model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
    "\n",
    "# Merge and push to hub in 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMNviX7XnVgl"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
