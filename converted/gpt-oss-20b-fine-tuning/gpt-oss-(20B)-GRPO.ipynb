{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7102503f",
   "metadata": {},
   "source": [
    "# ü§ô gpt-oss-20b on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">gpt-oss-20b</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">A100-40GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">24 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">reasoning, fine-tuning, large-model</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkH_y8UC9lvv"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzPgFeIkZn9q"
   },
   "source": [
    "# Goal: Make faster kernels with Reinforcement Learning\n",
    "\n",
    "Our goal is to make a faster matrix multiplication kernel by doing RL on GTP-OSS 20B with Unsloth.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Matrix_multiplication_qtl1.svg/500px-Matrix_multiplication_qtl1.svg.png\" height=200 />\n",
    "\n",
    "You will learn how to:\n",
    "1. Counteract **reward hacking** like cheating, caching, laziness.\n",
    "2. Timing and correctness of kernels and time limits.\n",
    "3. Making good **reward functions**\n",
    "4. How to seriously do RL to make optimized CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575,
     "referenced_widgets": [
      "ce40e56d7a524059a366371d52a5d7f5",
      "99e4796e59e0472eac83077cff768bf0",
      "bb788875516e4aba9ec87406be52eeff",
      "13d9d625e8734afdb9b1d896b2b0992e",
      "154717a8ed9745048a8906af3b221d41",
      "6dda7866d8c14e88be4fbd797417f556",
      "c535608487724a408d60b5d1c54b18b8",
      "856ded5e0d6e491eb38d316f2f43fbd7",
      "d09aa36b14ff466d96b1236be27919de",
      "a0e82f0030454bafb6bfb6bb1c9b3f7b",
      "ea437a5e5ee741ac8ac528362dc45bb6",
      "f0284fe69116476685139bc8ad7d80d2",
      "7dceab848c9043bfbad62c0ac6d9f13c",
      "e25e6d26258a4d5e864a45b7a3107c4f",
      "9190b3de6fcd4e0ca50c20c055eef834",
      "a7b21195b2824db89f19f8695fcb56cd",
      "127fc2d8cfc54b978b2a49f8dc4c1d32",
      "49e3ddd3f92146baa5a8cb017a036252",
      "c7c83df93e1444baae6168970d498ba9",
      "8d2807bfe85447bdad67987460e99168",
      "dea001037d0b479585264f675754415a",
      "e913fb6b7b86498d847c64196478a628",
      "5cae2f7900ca4509a68d345e26c738e5",
      "bbc57ccb80df4cb68c271c3ffcb85bce",
      "9809a8d0fa1d4bf8b5a96996ad44e9c5",
      "a2ee831695754546845b552ab4c880ad",
      "59259ca023ec4f739f723dccb312bca3",
      "df7811d1db294affbb1f7400b6d67393",
      "bfde109172ab414092bd3c0618e6ac77",
      "39fc8be5ba8141d09eed02162d2512bf",
      "380159861a234f5aa9a0737bd7f03bcb",
      "5323ebbabec14d7dad77fc47538eba93",
      "6f3bfc7e482147999b1fcf399e669452",
      "882a51f080ed407ebe01a05b970e48a5",
      "aa9017bafbe84475a843d4116e2d8cc1",
      "9b62f5baa62f4bccb8c2b7da62c280c7",
      "229f715276c941479a98f632b6a4028e",
      "99b4fa0ecb204d41a4d9237b604fb8e5",
      "59fa8c137a1649559e4d3c7486cbdf30",
      "c39b4f53a4fb409fb414b26a5f776eb8",
      "95461ebf2f9d44c9b745da5c35d3ecea",
      "a7be22b2a2cb4d7ebfa5016cd081af86",
      "b0963a510564455ba9182e877470d219",
      "023cb6271415454489dc5999bbba8a1b",
      "6b2ef4d303ee437cb19a2ad0800e3d9e",
      "81aa522133724d51adc0e1f19cc0bf8f",
      "8c2b98be4b2a466d834d2c8b918946f7",
      "e4275bea01d24aad92bf3b52c0f6032f",
      "3314a4aba3e44ab59c7265d2fbb8f101",
      "c7551368a9e74325986547cf00feb51f",
      "6f5b6b3036c34a248657e83d2db979ff",
      "d4a2d9865d6641b4891de7d61d465f12",
      "e299af20255f4b96bb66c649e12dafc8",
      "1b9a8cb9d8644923b2ee8b5b99de26da",
      "7321c679967f4e20b1feb305cb9d5e46",
      "adfde043c5694bc381fef6d0253ee113",
      "89ae701ad8ae4531bccbfaf2df88ee0f",
      "667c64b9f6294f5a85e6653c220d3c19",
      "3011422a32664f0b99d80456f17848b9",
      "7e7be5dbde274fb1bd2b673fa17e3c6c",
      "fe4ac2a3569c44c5813b447609931a2e",
      "79c650567e584ddc9834ef06ef8e7309",
      "2325e2f0d5d54bc685581d47c4045af2",
      "42c96fe6e04743b994950ce8f774b645",
      "46c0c957337744f19ee57dffa144bb88",
      "479521faaed1410d9b18b6e136fdccae",
      "6126ce31d40c4afbbc4f30ff618c5105",
      "6162fa10939c49feb3cbb5a25bd31f70",
      "508965f1eba9451f8d07cc5ce4cd1e09",
      "1f9374e2838647438922c6f3d16c2b6b",
      "2619d2a3049a49c9ab567d5ace2e6da5",
      "d5841e9d82a44eabb3a10c6990770b26",
      "c9e25b142c284eb585d7fef89e9cfb20",
      "9a76637b3afd4c56be2d99c200c5de8e",
      "e397e262339c437593d7159870e4ddd9",
      "011e3d0bb18541a5a2b0c071afb6eb32",
      "0f1c01032192466fa1aca285847f6391",
      "ff63b135c33544e6b8f327817b04e183",
      "39dd71d4a7d741dba0d95c6964207bf8",
      "a26fca95ca7f484fa3fbd89bf3736fa1",
      "451add29f7d84acfad95b1fa96a5cdd2",
      "a461767972644fdda27f372c6c0d983a",
      "2cff36ba4f974bd495bcc5808bc15f7b",
      "84dd3e02fd8249189c93a017f7e1cc93",
      "ab126a046cdd44e0a047abcb1528113d",
      "06fc35ae1cd245c2b711ae3d05004a35",
      "bcedab6c5dd0423b91eea9734e5b6904",
      "528d3a190af640d9ab7a7d7fe086f31c",
      "719839a5728a46248edcda02252e9561",
      "ddfbeb16103f44a08e3d7bb6bf4da86a",
      "1cb6e34da8ad46b3a0057ab01de94fc1",
      "ff698edf61144077bf61fe03369cdf5b",
      "07edfc41f3f44c9d851e12d984f19703",
      "8f439204fa79466aa2d20d0d2a28d74d",
      "805f7c86609347ed8fe945a38111edf3",
      "e6cb50ddb26844a084ae5a7d4a492eca",
      "515df8cf4bcf4fb0aa14705a195aced2",
      "db16c5bfe0be4728a4cacda8573101c4",
      "a4d88610a9464bbf9be3922d760b0670",
      "0a49a476e7c844bd8048e4e9c056fa7b",
      "5b2ec551f1ca490886287c0230ae7e8f",
      "9ddafad46ede4baebbc1f06928bbf01e",
      "d893a89d9c764fbf98459a301ffaa06a",
      "0fc9cecf55314af69389827852f390f3",
      "7c2a5bfc484043fe8c83d766c84b1827",
      "d481cf7937ea4399adf8153a672db38d",
      "c66e476d9fb046499f5dc878844b9aa8",
      "72469601dc8f485dbe1065124f86ad0b",
      "404ff34787884e879ab8e8c1cb99250f",
      "9fe343b36ef1493b86fc6a788bd916ed",
      "db814fd85f5342539537014ff47a4679",
      "85dac41978864ba78784413758a2e393",
      "0c65a1aa0c7243d196e96bd6ef6ad556",
      "e19c28cd15104cff8bff0bc3a1338f3f",
      "c22e6f19fcd949738d3da38670209bf1",
      "194f8f1987c84e9992e6ae7ab8352874",
      "3bd095f10eb24396aeec9d84629190d3",
      "f2072d86f3824135a8c8d4fd0fe4c7f3",
      "744f0a91396f4df6a3e5e4a32a785801",
      "a715051a1252454880f83726c380857f",
      "925b4741bcf24e52a91070b1c2a133d5",
      "291b76fb815b419fb8d12001ffb53dcc",
      "a9e779db1e7940e89b8599d4d19bf672",
      "3f92e70ba76d4cc59309fe9dd9adbd31",
      "4a6ea77ab6e547d0be2db28c3fc6a0de",
      "9efc5450ab4241f7a4668af9d6af33e8",
      "c103676f50c549d981e719d8baece21a",
      "57112275643b4fce8f681ac9d21bc2b1",
      "8b68fbede6a04c95a8996bf9e7945a68",
      "1947120737b344e996a45f935a18c2f0",
      "3e89a1c5e2ed49b280947c51e1713483",
      "7b808206dd32488c8d62888934c3ee12"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "3813293f-2f73-452c-8f1f-dc8170eb0e53"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 768 # Can increase for longer RL output\n",
    "lora_rank = 4 # Larger rank = smarter, but slower\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    offload_embedding = True, # Reduces VRAM by 1GB,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfeUs-lQJDSq"
   },
   "source": [
    "We now add some small amount of LoRA weights to GPT-OSS so we only need to train those, instead of training on the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rGa-o3HJCo1",
    "outputId": "3e5a1359-a1aa-4704-e164-c5640d232698"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0QnO9_YJBOI"
   },
   "source": [
    "# Optimized matrix multiplication\n",
    "\n",
    "Numpy has optimized matrix multiplication kernels for CPUs via BLAS optimized operations. For GPUs, one can use CUDA accelerated cuBLAS kernels which PyTorch calls under the hood.\n",
    "\n",
    "To generate some random matrices to do matrix multiplication, we can do the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9CI4jtgL5mw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_random_matrices(seed = 3407, n = 256):\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    n, k, m = random_state.randint(1, n+1, size = 3)\n",
    "    A = np.random.uniform(-10, 10, size = (n, k))\n",
    "    B = np.random.uniform(-10, 10, size = (k, m))\n",
    "    return A, A.tolist(), B, B.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BcaLniVKLpa"
   },
   "source": [
    "We shall generate a small matrix, and see the matrix multiplied output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M8kGaFRJ2ic",
    "outputId": "3c5c0ee1-ae76-41d2-d087-8441b6c9177e"
   },
   "outputs": [],
   "source": [
    "A, A_list, B, B_list = generate_random_matrices(seed = 42, n = 5)\n",
    "print(A)\n",
    "print(B)\n",
    "print(np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "envzrXmjKRff"
   },
   "source": [
    "We can call a LLM to generate a simple matrix multiply kernel in Python only, and we can calculate the differences between the actual result and the kernel's result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-gSgthFI_wq"
   },
   "outputs": [],
   "source": [
    "def calculate_difference(pred, real):\n",
    "    if pred is None: return 5, 5\n",
    "    assert real is not None\n",
    "    import numpy as np\n",
    "    try:\n",
    "        difference = pred - real\n",
    "    except:\n",
    "        return 5, 5\n",
    "    amax_error = float(np.amax(difference))\n",
    "    mse_error  = float(np.mean(np.square(difference)))\n",
    "    return amax_error, mse_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9gmkbTnKbcF"
   },
   "outputs": [],
   "source": [
    "# Kernel generated by GPT-5\n",
    "def matmul(A, B):\n",
    "    z, s = zip, sum\n",
    "    Bt = list(z(*B))\n",
    "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-WfRwQeKtEZ"
   },
   "source": [
    "We see the error below is very small, so that's good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QvIidsPKg2C",
    "outputId": "6aee038b-0c1d-4825-e72e-4cbb28c45e18"
   },
   "outputs": [],
   "source": [
    "prediction = matmul(A_list, B_list)\n",
    "calculate_difference(prediction, np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR6czU96cpxf"
   },
   "source": [
    "# Countering Reward Hacking\n",
    "\n",
    "The ultimate goal of RL is to maximize some reward (say speed, revenue, some metric).\n",
    "\n",
    "But RL can **cheat** When the RL algorithm learns a trick or exploits something to increase the reward, without actually doing the task at end, this is called \"Reward Hacking\".\n",
    "\n",
    "Some good examples are in https://en.wikipedia.org/wiki/Reward_hacking\n",
    "\n",
    "For matrix multiplication kernels, we might see the following issues:\n",
    "\n",
    "* Laziness: RL learns to use Numpy, Torch, other libraries, which calls optimized CUDA kernels.\n",
    "* Caching: RL learns to cache the result of the output\n",
    "* Cheating: RL learns to find the actual output by inspecting Python global variables\n",
    "* RL learns to edit the timing function to make it output 0 time as passed.\n",
    "\n",
    "And possibly more. We shall try to address each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRhLV_bZMYxy"
   },
   "source": [
    "# Countering Reward Hacking 1: Stop laziness\n",
    "We can stop the RL algorithm from calling optimized code by inspecting if the generated code imports other non standard Python libraries. We used GPT-5 to help generate this check `check_only_stdlib_imports`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MXPZ1MsUMqNZ"
   },
   "outputs": [],
   "source": [
    "#@title (Collapsible code)\n",
    "import ast\n",
    "import sys\n",
    "import sysconfig\n",
    "from pathlib import Path\n",
    "\n",
    "def _stdlib_names():\n",
    "    \"\"\"\n",
    "    Build a set of canonical stdlib top-level module/package names.\n",
    "    Uses sys.stdlib_module_names when available (3.10+), with a\n",
    "    filesystem fallback for older versions/edge cases.\n",
    "    \"\"\"\n",
    "    names = {m.lower() for m in getattr(sys, \"stdlib_module_names\", set())}\n",
    "    names |= {m.lower() for m in sys.builtin_module_names}\n",
    "    names.add(\"__future__\")  # special-case\n",
    "\n",
    "    # Fallback/augmentation: scan the stdlib directory\n",
    "    try:\n",
    "        stdlib_dir = Path(sysconfig.get_path(\"stdlib\"))\n",
    "        if stdlib_dir.exists():\n",
    "            for p in stdlib_dir.iterdir():\n",
    "                if p.name == \"site-packages\":\n",
    "                    continue\n",
    "                if p.suffix == \".py\":\n",
    "                    names.add(p.stem.lower())\n",
    "                elif p.is_dir() and (p / \"__init__.py\").exists():\n",
    "                    names.add(p.name.lower())\n",
    "    except Exception:\n",
    "        # conservative fallback; the names set above will still work well\n",
    "        pass\n",
    "\n",
    "    return names\n",
    "\n",
    "_STDLIB_SET = _stdlib_names()\n",
    "\n",
    "def check_only_stdlib_imports(code: str):\n",
    "    \"\"\"\n",
    "    Return (ok: bool, details: dict)\n",
    "\n",
    "    ok == True  -> all absolute imports are from the stdlib.\n",
    "    ok == False -> details['non_stdlib'] lists offending top-level modules.\n",
    "\n",
    "    details includes:\n",
    "      - stdlib: sorted list of stdlib imports found\n",
    "      - non_stdlib: sorted list of non-stdlib imports found\n",
    "      - relative_imports: count of relative imports (always allowed here)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ast.parse(code)\n",
    "    except SyntaxError as e:\n",
    "        return False, {\n",
    "            \"error\": f\"SyntaxError: {e}\",\n",
    "            \"stdlib\": [],\n",
    "            \"non_stdlib\": [],\n",
    "            \"relative_imports\": 0,\n",
    "        }\n",
    "\n",
    "    abs_imports = set()\n",
    "    relative_count = 0\n",
    "\n",
    "    class Visitor(ast.NodeVisitor):\n",
    "        def visit_Import(self, node: ast.Import):\n",
    "            for alias in node.names:\n",
    "                abs_imports.add(alias.name.split(\".\")[0])\n",
    "        def visit_ImportFrom(self, node: ast.ImportFrom):\n",
    "            nonlocal relative_count\n",
    "            if (node.level or 0) > 0:\n",
    "                # relative import\n",
    "                relative_count += 1\n",
    "            else:\n",
    "                if node.module:\n",
    "                    abs_imports.add(node.module.split(\".\")[0])\n",
    "\n",
    "    Visitor().visit(tree)\n",
    "\n",
    "    stdlib_found = sorted(m for m in abs_imports if m.lower() in _STDLIB_SET)\n",
    "    non_stdlib = sorted(m for m in abs_imports if m.lower() not in _STDLIB_SET)\n",
    "\n",
    "    return len(non_stdlib) == 0, {\n",
    "        \"stdlib\": stdlib_found,\n",
    "        \"non_stdlib\": non_stdlib,\n",
    "        \"relative_imports\": relative_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngUAw1lMM9JQ"
   },
   "source": [
    "For example, let's call `check_only_stdlib_imports` on a random piece of matrix multiplication code generated by GPT-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zz80kvg6M4BG",
    "outputId": "273b3ddd-540b-4f21-b671-69af0e8cf5f1"
   },
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "def matmul(A, B):\n",
    "    import numpy as np\n",
    "    from torch import matmul\n",
    "    z, s = zip, sum\n",
    "    Bt = list(z(*B))\n",
    "    return [[s(a*b for a, b in z(row, col)) for col in Bt] for row in A]\n",
    "\"\"\"\n",
    "ok, info = check_only_stdlib_imports(sample)\n",
    "print(\"Only stdlib imports?\", ok)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6lgkGkEN7B0"
   },
   "source": [
    "# Countering Reward Hacking 2: Stop cheating\n",
    "We can stop the RL algorithm from using global or cached variables by restricting it's `locals` and `globals`.\n",
    "\n",
    "We are also going to use `exec` to create the function, so we have to save the output to an empty dict.\n",
    "\n",
    "We also disallow global variable access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrIeYu-lOLSv",
    "outputId": "421497b7-c256-4d09-eb16-984c0e1095d9"
   },
   "outputs": [],
   "source": [
    "output_function = {}\n",
    "exec(sample, {}, output_function)\n",
    "output_function[\"matmul\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDSrjOTLVyQm"
   },
   "source": [
    "We also disallow global variable access via `types.FunctionType(f.__code__, {})`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcmYAmohVqw2",
    "outputId": "47b134cc-2733-4cc9-81ab-a6592be9bb0a"
   },
   "outputs": [],
   "source": [
    "import types\n",
    "output_function[\"matmul\"] = types.FunctionType(output_function[\"matmul\"].__code__, {})\n",
    "\n",
    "def import_numpy():\n",
    "    np.matmul\n",
    "    print(\"Success\")\n",
    "\n",
    "import_numpy()\n",
    "import_numpy = types.FunctionType(import_numpy.__code__, {})\n",
    "try:\n",
    "    import_numpy()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tJKwLUgZsRq"
   },
   "outputs": [],
   "source": [
    "def create_locked_down_function(function):\n",
    "    output_function = {}\n",
    "    exec(function, {}, output_function)\n",
    "    new_matmul = output_function[\"matmul\"]\n",
    "    new_matmul = types.FunctionType(new_matmul.__code__, {})\n",
    "    return new_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl-IxTZ6Nvm9"
   },
   "source": [
    "# Countering Reward Hacking 3: Stop caching\n",
    "We can stop the RL algorithm from using cached data by wiping the cache with a large fake matrix. We also have to benchmark carefully with multiple loops and turns.\n",
    "\n",
    "We also add a **timer** to not make the algorithm go in an endless loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZwGdNyMNlEV"
   },
   "outputs": [],
   "source": [
    "import os, gc, time, statistics\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "class TimeoutError(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def _handler(signum, frame):\n",
    "        raise TimeoutError(f\"Timed out after {seconds}s\")\n",
    "    old = signal.signal(signal.SIGALRM, _handler)\n",
    "    signal.setitimer(signal.ITIMER_REAL, seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0.0)\n",
    "        signal.signal(signal.SIGALRM, old)\n",
    "\n",
    "class Benchmarker:\n",
    "    def __init__(self, trials = 3, loops = 1, timeout = 30):\n",
    "        self.buffer = np.zeros(2 * 1024 * 1024 * 1024, dtype = np.uint8)\n",
    "        self.trials = trials\n",
    "        self.loops = loops\n",
    "        assert timeout > 0 # Cannot be 0 since it won't work!\n",
    "        self.timeout = timeout\n",
    "    def thrash(self):\n",
    "        # Edit the buffer to wipe cache lines\n",
    "        self.buffer ^= 1\n",
    "        return int(self.buffer[::4096].sum())\n",
    "\n",
    "    def benchmark(self, function, arguments):\n",
    "        assert len(arguments) == self.loops\n",
    "        samples = []\n",
    "        exceptions = []\n",
    "        timed_out = 0\n",
    "        for _ in range(self.trials):\n",
    "            gc.collect(); gc.disable(); self.thrash()\n",
    "            t_start = time.perf_counter_ns()\n",
    "            for i in range(self.loops):\n",
    "                try:\n",
    "                    with time_limit(self.timeout):\n",
    "                        function(*arguments[i])\n",
    "                except TimeoutError as e:\n",
    "                    timed_out += 1\n",
    "                except Exception as e:\n",
    "                    exceptions.append(str(e))\n",
    "            t_end = time.perf_counter_ns()\n",
    "            gc.enable()\n",
    "            samples.append((t_end - t_start) // max(1, self.loops))\n",
    "        return {\n",
    "            \"median_ns\": int(statistics.median(samples)),\n",
    "            \"mean_ns\": int(statistics.fmean(samples)),\n",
    "            \"stdev_ns\": int(statistics.pstdev(samples) if len(samples) > 1 else 0),\n",
    "            \"exceptions\" : exceptions,\n",
    "            \"timeouts\" : timed_out,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV5M0DCyOvon"
   },
   "source": [
    "For example we use our matmul kernel we had, and benchmark it with a 10 second delay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8df8tZcEOuYJ",
    "outputId": "b4b2cdf0-f35b-485a-e479-c2f082d37939"
   },
   "outputs": [],
   "source": [
    "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
    "Benchmarker(trials = 1, timeout = 10).benchmark(output_function[\"matmul\"], [(A_list, B_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CzwCyXIPK04"
   },
   "source": [
    "# Data & RL task setup\n",
    "\n",
    "We now have to create a prompt to the model for which it will do some task. For our matrix multiply example, we use the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-2RRE4HMrQO",
    "outputId": "fc5a4a66-154a-419e-e200-63987fff4204"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Create a new fast matrix multiplication function using only native Python code.\n",
    "You are given a list of list of numbers.\n",
    "Output your new function in backticks using the format below:\n",
    "```python\n",
    "def matmul(A, B):\n",
    "    return ...\n",
    "```\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIdudFUodN4i"
   },
   "source": [
    "First, let's prompt GPT-OSS without RL and see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HJxrS76h3Ds",
    "outputId": "f3616393-c222-47f3-ffc5-9a87bc106e61"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    "    reasoning_effort = \"low\",\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 512,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iknaWZNudTNq"
   },
   "source": [
    "# Reward functions\n",
    "\n",
    "We now design the `extract_function` function which simply extracts the function wrapped in 3 backticks.\n",
    "\n",
    "And 4 reward functions:\n",
    "\n",
    "1. `function_works` which rewards the model if the strategy is a valid Python function.\n",
    "2. `no_cheating` which checks if the function imported other modules, and if it did, we penalize it.\n",
    "3. `correctness_check` which checks if the kernel was correct or wrong - it shouldn't generate gibberish!\n",
    "4. `speed_check` checks the performance relative to Numpy matmul directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JJGXKdJ-Zl_",
    "outputId": "3d16b32b-990b-4472-c3a3-267c7d6cf47c"
   },
   "outputs": [],
   "source": [
    "def extract_function(text):\n",
    "    if text.count(\"```\") >= 2:\n",
    "        first = text.find(\"```\") + 3\n",
    "        second = text.find(\"```\", first)\n",
    "        fx = text[first : second].strip()\n",
    "        fx = fx.removeprefix(\"python\\n\")\n",
    "        fx = fx[fx.find(\"def\"):]\n",
    "        if fx.startswith(\"def matmul(A, B):\"): return fx\n",
    "    return None\n",
    "print(extract_function(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLXEcf_HSJlI"
   },
   "source": [
    "Below is our `function_works` reward function which uses Python's `exec` but guarded by not allowing leakage of local and global variables. We can also use `check_only_stdlib_imports` first to check if there are errors before even executing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3-B0IIsS56S",
    "outputId": "67486b3d-aeb5-41d8-f4b5-b6ba9cdc97ad"
   },
   "outputs": [],
   "source": [
    "ok, info = check_only_stdlib_imports(\"def a\")\n",
    "ok, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFNXORy-lpO"
   },
   "outputs": [],
   "source": [
    "def function_works(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        print(function)\n",
    "        if function is not None:\n",
    "            ok, info = check_only_stdlib_imports(function)\n",
    "        if function is None or \"error\" in info:\n",
    "            score = -2.0\n",
    "        else:\n",
    "            try:\n",
    "                new_matmul = create_locked_down_function(function)\n",
    "                score = 1.0\n",
    "            except:\n",
    "                score = -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "`no_cheating` checks if the function cheated since it might have imported Numpy or Torch optimized code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfHzCVx-nGK"
   },
   "outputs": [],
   "source": [
    "def no_cheating(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if function is not None:\n",
    "            ok, info = check_only_stdlib_imports(function)\n",
    "        else:\n",
    "            ok = False\n",
    "        scores.append(1.0 if ok else -20.0) # Penalize heavily!\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slnqWG3FTror"
   },
   "source": [
    "Next `correctness_check` checks if the kernel was correct. We want to penalize if the absolute error is larger than 1, and if the mean squared error is somewhat bigger then machine epsilon.\n",
    "\n",
    "We have to execute the code now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFBp-MkyYeoE",
    "outputId": "5d065a27-553a-4599-bfaa-375a7570357c"
   },
   "outputs": [],
   "source": [
    "np.finfo(np.float64).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNi129lYTpZ2"
   },
   "outputs": [],
   "source": [
    "def correctness_check(completions, **kwargs):\n",
    "    scores = []\n",
    "    # Generate some random matrices of size less than 128\n",
    "    A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 128)\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if function is not None:\n",
    "            ok, info = check_only_stdlib_imports(function)\n",
    "        if function is None or \"error\" in info:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            new_matmul = create_locked_down_function(function)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            pred = new_matmul(A_list.copy(), B_list.copy())\n",
    "        except:\n",
    "            # Failed!\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        true = np.matmul(A, B)\n",
    "        amax_error, mse_error = calculate_difference(pred, true)\n",
    "\n",
    "        # Check correctness and score!\n",
    "        machine_epsilon = 100*np.finfo(np.float64).eps\n",
    "        if   amax_error >= 3:   score = -3.0\n",
    "        elif amax_error >= 2:   score = -2.5\n",
    "        elif amax_error >= 1:   score = -2.0\n",
    "        elif amax_error >= 0.5: score = -1.0\n",
    "        elif amax_error >= 100*machine_epsilon: score = 0.0\n",
    "        elif amax_error >= machine_epsilon: score = 1.0\n",
    "        else: score = 3.0\n",
    "\n",
    "        if   mse_error >= 3:   score += -3.0\n",
    "        elif mse_error >= 2:   score += -2.5\n",
    "        elif mse_error >= 1:   score += -2.0\n",
    "        elif mse_error >= 0.5: score += -1.0\n",
    "        elif mse_error >= 100*machine_epsilon: score += 0.0\n",
    "        elif mse_error >= machine_epsilon: score += 1.0\n",
    "        else: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpTrofI9ZIn8"
   },
   "source": [
    "Finally our benchmarking function for `speed_check`! We shall limit the timer to 10 seconds and do 3 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5xkIAzuZMnO",
    "outputId": "e86c1d87-6eb4-4858-9fe1-1123efb6f5df"
   },
   "outputs": [],
   "source": [
    "A, A_list, B, B_list = generate_random_matrices(seed = 0, n = 256)\n",
    "benchmarker = Benchmarker(trials = 3, timeout = 10)\n",
    "numpy_results = benchmarker.benchmark(np.matmul, [(A, B)])\n",
    "numpy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNDc6skFZZW6",
    "outputId": "0a012106-7517-46f9-c995-fe52841cbdd5"
   },
   "outputs": [],
   "source": [
    "new_matmul = create_locked_down_function(extract_function(prompt))\n",
    "new_results = benchmarker.benchmark(new_matmul, [(A_list, B_list)])\n",
    "new_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noUAaX24aqNS"
   },
   "source": [
    "We can take the difference and do a negative sign for slower ones. If the ratio is less than 1 (ie faster, we shall invert it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IT9nXcjaI-X",
    "outputId": "41ba4700-da7e-45f4-e7dd-195a069bda74"
   },
   "outputs": [],
   "source": [
    "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
    "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
    "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntYNFV0ra-MX",
    "outputId": "b6b445eb-60be-49c7-edb5-c604fbf03f2b"
   },
   "outputs": [],
   "source": [
    "new_results[\"median_ns\"] = 3\n",
    "numpy_results[\"median_ns\"] = 1000\n",
    "negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
    "positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
    "reward = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHmXmAxtbVpP"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "import gc\n",
    "def speed_check(completions, **kwargs):\n",
    "    scores = []\n",
    "    # Generate some random matrices of size less than 256\n",
    "    A, A_list, B, B_list = generate_random_matrices(seed = np.random.randint(10000), n = 256)\n",
    "    numpy_results = benchmarker.benchmark(np.matmul, [(A, B)])\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if function is not None:\n",
    "            ok, info = check_only_stdlib_imports(function)\n",
    "        if function is None or \"error\" in info:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            new_matmul = create_locked_down_function(function)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        new_results = benchmarker.benchmark(new_matmul, [(A_list.copy(), B_list.copy())])\n",
    "\n",
    "        # Get score and clip to -10, 10\n",
    "        negative = -(new_results[\"median_ns\"] / numpy_results[\"median_ns\"]) / 100\n",
    "        positive = +(numpy_results[\"median_ns\"] / new_results[\"median_ns\"]) / 100\n",
    "        score = negative if new_results[\"median_ns\"] >= numpy_results[\"median_ns\"] else positive\n",
    "        if score >= 10:  score = 10\n",
    "        if score <= -10: score = -10\n",
    "        scores.append(score)\n",
    "    # Free memory to counteract OOMs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCpSxtvSeAG_"
   },
   "source": [
    "We create the dataset which includes a replica of our prompt. Remember to add reasoning effort of low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ldf6SjLHVPRv",
    "outputId": "aa498168-bd8c-4352-ddf1-5b36ecc32b2a"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list([{\"prompt\" : [{\"role\": \"user\", \"content\": prompt.strip()}], \"answer\" : 0, \"reasoning_effort\": \"low\"}]*1000)\n",
    "maximum_length = len(tokenizer(prompt.strip())[\"input_ids\"])\n",
    "print(maximum_length)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations! We also support GSDP, GAPO, Dr GRPO and more! Go to our docs https://docs.unsloth.ai/ for more info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "6c93c354-d103-4e10-e741-35d052b741ab"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir=\"/workspace/outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "c1f2d72a-0633-4bb8-a462-49ac2de2f25a"
   },
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        function_works,\n",
    "        no_cheating,\n",
    "        correctness_check,\n",
    "        speed_check,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQhtuwP4cf34"
   },
   "source": [
    "And let's train the model!\n",
    "\n",
    "**NOTE** A T4 free GPU might take 5 minutes for one generation sadly since it's an old GPU - A100 or H100 will be much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGRxPdSCcfC3",
    "outputId": "7535481b-b1ac-491d-e0e8-b91b2d93f81d"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "# Inference\n",
    "Now let's try the model we just trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BZZHOKiF9Ct"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    "    reasoning_effort = \"low\",\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 or MXFP4 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `mxfp4` for MXFP4 (OpenAI's GPT-OSS native precision). We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge and push to hub in mxfp4 4bit format\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
    "if False: model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
    "\n",
    "# Merge and push to hub in 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V15Yhj1V9lwG"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
