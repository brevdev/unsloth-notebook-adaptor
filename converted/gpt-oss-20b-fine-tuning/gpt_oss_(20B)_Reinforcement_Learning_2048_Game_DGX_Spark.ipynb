{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5969b6",
   "metadata": {},
   "source": [
    "# ü§ô gpt-oss-20b on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">gpt-oss-20b</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">A100-40GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">24 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">reasoning, fine-tuning, large-model</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-20b.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzPgFeIkZn9q"
   },
   "source": [
    "# Goal: Make GPT-OSS play games with Reinforcement Learning\n",
    "\n",
    "Our goal is to make GPT-OSS play the 2048 game with reinforcement learning, or a variant of it called [GRPO](https://arxiv.org/abs/2501.12948).\n",
    "\n",
    "We want the model to devise a strategy to play 2048, and we will run this strategy until we win or lose. We then reward the model if it created a good strategy (winning the game), and we'll penalize it (negative reward) if the strategy was a bad one.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f9/2048_win.png/500px-2048_win.png\" height=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31KIMLJLnHET"
   },
   "source": [
    "# Installation\n",
    "We'll be using [Unsloth](https://github.com/unslothai/unsloth) to do RL on GPT-OSS 20B. Unsloth saves 70% VRAM usage and makes reinforcement learning 2 to 6x faster, which allows us to fit GPT-OSS RL in a free Google Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGoDZwcunHEU"
   },
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcLYwLyQLADE"
   },
   "source": [
    "We'll load GPT-OSS 20B and set some parameters:\n",
    "* `max_seq_length = 768` The maximum context length of the model. Increasing it will use more memory, and 768 was the maximum we found to fit on a free 15GB Tesla T4 machine\n",
    "* `lora_rank = 4` The larger this number, the smarter the RL process, but the slower and more memory usage\n",
    "* `load_in_4bit = True` Uses quantization to reduce memory usage by 75% without reducing accuracy that much. `load_in_16bit` will be faster but will need a 80GB GPU (H100, B200)\n",
    "* `offload_embedding = True` New Unsloth optimization which moves the embedding to CPU RAM, reducing VRAM by 1GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575,
     "referenced_widgets": [
      "abe2b0a2913d4633943f44333ae799f8",
      "2c40c6b846924200b29616a590af1672",
      "749e8407a901483c8b513a2fb71596c8",
      "7baca79d720c40b5a923b9717e28c982",
      "68ea891644ca4753a8e1bf278ff47e84",
      "06ab9eaa6f0f48c4b68cff1ca4b9f2fa",
      "d98c2b1e979b4929891a8ee0c11f55df",
      "ef01b874478b4bb497d31d2f8dd6145a",
      "d50ea8cded9848ffa18be1ae6a2559df",
      "ffabf89ecd9d48a5a3fc2a1c855ce080",
      "614c5332c7d045109102a329e7f69dfd",
      "caf742160db041a1b6c2cfdf78f2dc9a",
      "34a9e38b0b454a69a067d1ddadec7626",
      "263b7dc0b3fd465fac89b9266b19d526",
      "5b7af68130f04a63ad3efa3d9f602ebe",
      "2a6aa92676c74509b58373ca604c5b3b",
      "9c4d6839934b4b13952a850d2084d498",
      "c6a1decbc0e7421db622033214913cb9",
      "147743757c804b85af2ef194f5f84e6a",
      "2820e352ab004e818949acc31eb3888d",
      "80fa3aef5e2040d9904c6b87b7214ca0",
      "0f99489932aa409b94ba34764aff19b0",
      "6ab4e5676ad84807a126fffa99f7a0d4",
      "e61ef80398444c13bf7cd20ef21a5057",
      "5ebe7b4e4ed24c53b783ee46377c682d",
      "e0fdef0087bc4a91a11932a2d933c001",
      "596c2a62a635469eb74233ce00586a6f",
      "da4324e287e64e5ba98fc110693066df",
      "8c7c6bb04a3f4a1494b34529f95a195c",
      "51aaa109480d4ae6bd419aea689d22ee",
      "acf4e50a248342f68d26daef21baa419",
      "7d3379cbd27a4218a9d84c5a12f3bb88",
      "7841bc90b6a74120ab3e603c76332a01",
      "3f9b801b52da4eb79f730d87bea5c338",
      "b66c6ded549d4db8a2e5ea8e5016615c",
      "43da5073c3ad4e98a3ade17a0bb3b93d",
      "40365e2c9fef49148e4c93592d458afc",
      "7e9d5212fc7844f286e14b70cbf0bc7a",
      "77d34c0f1de548b4872208a063bb5017",
      "bf96e8666c224c26b0a01451d08e907a",
      "4513a73fa95b41b5b6edadc9143ba9c1",
      "792d75a7d18945e7972826ac5b2ac386",
      "2a6f43b64d164636a2d9708f0190f21b",
      "65c62d2198e64ee4a9e6547c2733135a",
      "219ca32ab51e4b4385b2c1026a78503a",
      "6c2ccfe3363b40b58fc26ea164d4ead4",
      "07f0420c4dfa477caccd7ae96551c2e4",
      "1c96edb2f7c948b9968b1239982af942",
      "d93be4994f104b6e99d89a9e73cd6abd",
      "4da21f53bf7f4e2d8132eb43e6ecc739",
      "735f70fac43449e3974de1b783d56d33",
      "ad75f887a140416abfca615b2fc3c385",
      "dee02a37a6f44f168546ee0077dc20d1",
      "ee23056662ad4b719b65005d776e0e72",
      "87765ca0996b403dbe29deef48d548bf",
      "8db5e86577744ff1a39c8e198eee5dd3",
      "4b9b3fe8dc764eedb9e18f166fe2f548",
      "cca95e973bc445d3811335debf7c446e",
      "e507a46b4c754d9a8aede2aac0d203bc",
      "751a46fbb8e24efabfb381a85c90fbe8",
      "87a808c4d4f54f719adcd29de7206e1b",
      "5f0b2a0e1953406b88af2c884904e2da",
      "2fa84865e9f14c1491402ef81517b4bd",
      "245590db7d374515a428ff4abbd25588",
      "e2973e6c02834a7c9f2f6ce5755f35f0",
      "48741bbdeccb459aa4eea9c61339764b",
      "1183d3f2ad3c4fb0af1d925b5f9e3efe",
      "9cc51d8029eb4217bc37daa918649692",
      "41f13d2f023e405180689e03bc2c32a1",
      "247484c0bf5945bcb4627b48928366c8",
      "14c0f20a9ab341ee966fe77815099ff0",
      "a219f3b89a34443abe612846676f9356",
      "152d7bf2a74f400db3d3ecaa719ef8d1",
      "36676899a61f4be4b631f6271f6ecec9",
      "77ecad9f150c430fa85f5833d97c42df",
      "cef064f1c55f41bf957fc4623260fdb4",
      "37cbe8800af04a42a0355922969b6393",
      "f8dacdab001d4db0b6b3776ac7d3634a",
      "5a59fb5f7acf4213847c985e66c9ee3c",
      "ae6d42fb84fc4984af1d4430acdcd3c9",
      "02d120e49f2c4f95a6090b1d8d521767",
      "8f1e6c36b84c4115a671dcb9ade41c8b",
      "81a728910a2341a785a6f252bbb371f7",
      "69a8d50f11244ba688c183d14d2395ec",
      "350f29f737534bfba4258bc31ec274a2",
      "9beac0680e3049dfafcb6ec185fd2265",
      "dbf5ed93dac646ed979fa7a8c569dfe3",
      "4db5ee5b7b674abba75fbce264e6dfa3",
      "0c0c96eeac664f339aa4511bf47087e2",
      "18451e19df5449b1853b5e13dacd19c5",
      "d864d29d02c54ecfaedd7b866a6df8c2",
      "7875163297284832a35aca84cbb105ce",
      "d42d8228ea1247a1a81bb99b18c4640c",
      "bcda4c9a48e943a6a0ef812fcd64a6db",
      "61e491b843c347b6b2a9948de7caf01d",
      "dee07d33b8de4c3b847fcff670e68102",
      "b07acf871a0a46f1889bfb439d13752b",
      "ba94310dc12a4a258205b14901ad3f94",
      "a93210a691414502ba3c2dff03ffb4ce",
      "fd2fe9ef6da64f72ab29d481d1739f5e",
      "dbfeea8ee2374b8c8fa70431c35f281f",
      "84d27c45065e426badbfcfcdc8ff16b6",
      "fa9ea0d3234e41689c827485d0360885",
      "4cb119127b404f46a53012c62d004e28",
      "d9020a2a2c8440db81d2cfdf0289b667",
      "04d39c4dda9f4a1bb01b8d6320032372",
      "4d67b10ec7794170addb4e968e20f170",
      "55ac5c2a82ee48fe988e1e4f26c168b0",
      "9a079a30b4ae4bbc80122faf83e0ad59",
      "acda8e7582934fecbbf854e66e23f698",
      "4fbc4cfe529d471ba85f3ae8e53b28d6",
      "a0d0fedc5bec4f5b943fddf9a954fbdf",
      "cab602573c6940919f93e59fe6f4838d",
      "51b8f4ce40f94ac39cf44d98f1522ec7",
      "32d6af64f2464cfb965671f2692b4e15",
      "e1e77d98b01f4376a6c075975c27571e",
      "6a47e60b10a6481b94aee021c8dbc7ba",
      "5657a84bf4b74710b2de1a54f9236e39",
      "7bd5d1beeb0e49e293d9f6b91bb6d7fb",
      "60ceb890b5644493a8886d91b9dac461",
      "40138ff29073407abb95f793509fc320",
      "0ac4d8e674804ad6bdc5f2d62f2e0d33",
      "7bfcd9acf29646db8b6123708d1ffe27",
      "5e88d6515f16475fb72d7c153422b591",
      "5e5b77dd649547f896ab306fccc94a4e",
      "a843fa23e6c94fb486bff8764574fdc5",
      "fd0ac7ed3d3146ec85913f4e05c4a2f6",
      "77204d81ff8f4ee585361a503fa647dc",
      "923653dfe90e475a9efa44baf98ba9a0",
      "62600092f8cc43f493b86b0169f67be1",
      "59e46bbe96df4b88ad31c09096ce0e0a",
      "8f5c7b88a2cc4b5abb0814c814833349"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "2f85e1d0-8810-4b41-b683-0c33578d991c"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 768 # Can increase for longer RL output\n",
    "lora_rank = 4        # Larger rank = smarter, but slower\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\", # unsloth/gpt-oss-20b-BF16 for H100s\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,      # False for LoRA 16bit. Choose False on H100s\n",
    "    offload_embedding = True, # Reduces VRAM by 1GB,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfeUs-lQJDSq"
   },
   "source": [
    "To do efficient RL, we will use [LoRA](https://arxiv.org/abs/2106.09685), which allows us to only add 1 to 5% of extra weights to the model for finetuning purposes. This allows us to save memory usage by over 60%, and yet it retains good accuracy. Read Unsloth's [GPT-OSS RL Guide](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rGa-o3HJCo1",
    "outputId": "6dc27dbf-0c60-4996-8e97-932aab7c14fb"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0QnO9_YJBOI"
   },
   "source": [
    "# 2048 game\n",
    "\n",
    "We used GPT-5 to create a variant of the 2048 game. It should output the current game board state, and allow us to advance the game board state with 1 action (up, down, left, right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "D9CI4jtgL5mw"
   },
   "outputs": [],
   "source": [
    "#@title (Collapsible) 2048 Game Implementation\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional\n",
    "import random\n",
    "import copy\n",
    "\n",
    "def _compress_and_merge_row_left(row: List[int]) -> Tuple[List[int], int, bool]:\n",
    "    n = len(row)\n",
    "    tiles = [x for x in row if x != 0]\n",
    "    gained = 0\n",
    "    i = 0\n",
    "    merged = []\n",
    "    while i < len(tiles):\n",
    "        if i + 1 < len(tiles) and tiles[i] == tiles[i + 1]:\n",
    "            v = tiles[i] * 2\n",
    "            gained += v\n",
    "            merged.append(v)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(tiles[i])\n",
    "            i += 1\n",
    "    merged += [0] * (n - len(merged))\n",
    "    changed = merged != row\n",
    "    return merged, gained, changed\n",
    "\n",
    "def _move_left(board: List[List[int]]) -> Tuple[List[List[int]], int, bool]:\n",
    "    changed_any = False\n",
    "    total_gain = 0\n",
    "    new_board = []\n",
    "    for row in board:\n",
    "        new_row, gained, changed = _compress_and_merge_row_left(row)\n",
    "        new_board.append(new_row)\n",
    "        total_gain += gained\n",
    "        changed_any = changed_any or changed\n",
    "    return new_board, total_gain, changed_any\n",
    "\n",
    "def _move_right(board: List[List[int]]) -> Tuple[List[List[int]], int, bool]:\n",
    "    changed_any = False\n",
    "    total_gain = 0\n",
    "    new_board = []\n",
    "    for row in board:\n",
    "        rev = list(reversed(row))\n",
    "        new_rev, gained, changed = _compress_and_merge_row_left(rev)\n",
    "        new_row = list(reversed(new_rev))\n",
    "        new_board.append(new_row)\n",
    "        total_gain += gained\n",
    "        changed_any = changed_any or changed\n",
    "    return new_board, total_gain, changed_any\n",
    "\n",
    "def _transpose(board: List[List[int]]) -> List[List[int]]:\n",
    "    return [list(row) for row in zip(*board)]\n",
    "\n",
    "def _move_up(board: List[List[int]]) -> Tuple[List[List[int]], int, bool]:\n",
    "    t = _transpose(board)\n",
    "    moved, gain, changed = _move_left(t)\n",
    "    return _transpose(moved), gain, changed\n",
    "\n",
    "def _move_down(board: List[List[int]]) -> Tuple[List[List[int]], int, bool]:\n",
    "    t = _transpose(board)\n",
    "    moved, gain, changed = _move_right(t)\n",
    "    return _transpose(moved), gain, changed\n",
    "\n",
    "def _empty_cells(board: List[List[int]]) -> List[Tuple[int, int]]:\n",
    "    size = len(board)\n",
    "    return [(r, c) for r in range(size) for c in range(size) if board[r][c] == 0]\n",
    "\n",
    "def _can_move(board: List[List[int]]) -> bool:\n",
    "    if _empty_cells(board):\n",
    "        return True\n",
    "    size = len(board)\n",
    "    for r in range(size):\n",
    "        for c in range(size - 1):\n",
    "            if board[r][c] == board[r][c + 1]:\n",
    "                return True\n",
    "    for r in range(size - 1):\n",
    "        for c in range(size):\n",
    "            if board[r][c] == board[r + 1][c]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "@dataclass\n",
    "class GameBoard:\n",
    "    size: int\n",
    "    seed: Optional[int] = None\n",
    "    target: int = 2048\n",
    "    probability_fours: float = 0.10 # originally spawns (4) 10% of the time!\n",
    "    _rng: random.Random = field(init=False, repr=False)\n",
    "    _board: List[List[int]] = field(init=False, repr=False)\n",
    "    _score: int = field(default=0, init=False, repr=False)\n",
    "    _state: str = field(default=\"ongoing\", init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.size < 2:\n",
    "            raise ValueError(\"Board size must be at least 2.\")\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self._add_random_tile()\n",
    "        self._add_random_tile()\n",
    "        self._update_state_after_change()\n",
    "\n",
    "    class _BoardView:\n",
    "        def __init__(self, game: \"GameBoard\"):\n",
    "            self._game = game\n",
    "        def __iter__(self):\n",
    "            return iter(self._game._board)\n",
    "        def __len__(self):\n",
    "            return len(self._game._board)\n",
    "        def __getitem__(self, idx):\n",
    "            return self._game._board[idx]\n",
    "        def __repr__(self) -> str:\n",
    "            return repr(self._game._board)\n",
    "        __str__ = __repr__\n",
    "        def do_action(self, key: str) -> None:\n",
    "            self._game.do_action(key)\n",
    "        def state(self) -> str:\n",
    "            return self._game.state()\n",
    "        def pretty(self, colors: bool = True, border: bool = True, dot_for_zero: bool = True) -> str:\n",
    "            return self._game._render_pretty(colors=colors, border=border, dot_for_zero=dot_for_zero)\n",
    "\n",
    "    def board(self) -> \"_BoardView\":\n",
    "        return GameBoard._BoardView(self)\n",
    "    def state(self) -> str:\n",
    "        return self._state\n",
    "    def score(self) -> int:\n",
    "        return self._score\n",
    "    def do_action(self, key: str) -> None:\n",
    "        if self._state != \"ongoing\":\n",
    "            return\n",
    "        if not isinstance(key, str) or len(key) == 0:\n",
    "            self._state = \"failed\"\n",
    "            return\n",
    "        k = key.strip().lower()\n",
    "        if k == \"q\":\n",
    "            self._state = \"failed\"\n",
    "            return\n",
    "        move_map = {\"a\": _move_left, \"d\": _move_right, \"w\": _move_up, \"s\": _move_down}\n",
    "        if k not in move_map:\n",
    "            self._state = \"failed\"\n",
    "            return\n",
    "        mover = move_map[k]\n",
    "        new_board, gain, changed = mover(self._board)\n",
    "        if changed:\n",
    "            self._board = new_board\n",
    "            self._score += gain\n",
    "            self._add_random_tile()\n",
    "        self._update_state_after_change()\n",
    "    def _add_random_tile(self) -> bool:\n",
    "        empties = _empty_cells(self._board)\n",
    "        if not empties:\n",
    "            return False\n",
    "        r, c = self._rng.choice(empties)\n",
    "        self._board[r][c] = 4 if self._rng.random() < self.probability_fours else 2\n",
    "        return True\n",
    "    def _update_state_after_change(self) -> None:\n",
    "        if any(self.target in row for row in self._board):\n",
    "            self._state = \"success\"\n",
    "            return\n",
    "        if not _can_move(self._board):\n",
    "            self._state = \"failed\"\n",
    "            return\n",
    "        self._state = \"ongoing\"\n",
    "    def _render_pretty(self, colors: bool = True, border: bool = True, dot_for_zero: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Pretty-print the board with colors that scale from 0 up to self.target.\n",
    "        Uses ANSI 256-color codes (works in most terminals). Set colors=False to disable.\n",
    "        \"\"\"\n",
    "        import math\n",
    "\n",
    "        b = self._board\n",
    "        mx = max((max(row) for row in b), default=0)\n",
    "        cell_w = max(3, len(str(mx)))\n",
    "\n",
    "        RESET = \"\\x1b[0m\"\n",
    "\n",
    "        # A smooth-ish gradient from cool ‚Üí warm\n",
    "        # (blue/cyan/green ‚Üí yellow/orange/red). Tweak or expand as you like.\n",
    "        GRAD = [33, 39, 45, 51, 50, 49, 48, 47, 46, 82, 118, 154, 190, 226, 220, 214, 208, 202, 196]\n",
    "        ZERO_FG = 239  # dim gray\n",
    "\n",
    "        def color_code(v: int) -> str:\n",
    "            if not colors:\n",
    "                return \"\"\n",
    "            if v == 0:\n",
    "                return f\"\\x1b[38;5;{ZERO_FG}m\"\n",
    "            # Normalize by exponent relative to target: r in [0,1]\n",
    "            t = max(2, self.target)  # safety; avoid log2(1)\n",
    "            # Guard: if v is not a power of two or is <1, handle gracefully\n",
    "            try:\n",
    "                r = max(0.0, min(1.0, math.log2(v) / math.log2(t)))\n",
    "            except ValueError:\n",
    "                r = 0.0\n",
    "            idx = int(round(r * (len(GRAD) - 1)))\n",
    "            return f\"\\x1b[38;5;{GRAD[idx]}m\"\n",
    "\n",
    "        def fmt(v: int) -> str:\n",
    "            s = \".\" if (v == 0 and dot_for_zero) else str(v)\n",
    "            s = s.rjust(cell_w)\n",
    "            return color_code(v) + s + (RESET if colors else \"\")\n",
    "\n",
    "        def hline(left: str, mid: str, right: str) -> str:\n",
    "            return left + mid.join(\"‚îÄ\" * cell_w for _ in range(self.size)) + right\n",
    "\n",
    "        rows = []\n",
    "        if border:\n",
    "            rows.append(hline(\"‚îå\", \"‚î¨\", \"‚îê\"))\n",
    "        for r in range(self.size):\n",
    "            content = \"‚îÇ\".join(fmt(v) for v in b[r])\n",
    "            rows.append((\"‚îÇ\" + content + \"‚îÇ\") if border else content)\n",
    "            if border:\n",
    "                rows.append(hline(\"‚îî\" if r == self.size - 1 else \"‚îú\",\n",
    "                                \"‚î¥\" if r == self.size - 1 else \"‚îº\",\n",
    "                                \"‚îò\" if r == self.size - 1 else \"‚î§\"))\n",
    "        return \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BcaLniVKLpa"
   },
   "source": [
    "For example let's create a board of size 5 X 5 and set the target to 8 instead of 2048.\n",
    "\n",
    "**[NOTE]** 2048 originally spawns a (4) 10% of the time! We can disable this for harder games. See [Wikipedia page](https://en.wikipedia.org/wiki/2048_(video_game)) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-M8kGaFRJ2ic",
    "outputId": "fad6c36b-cb16-490f-ad4f-6bf998dd24ab"
   },
   "outputs": [],
   "source": [
    "game = GameBoard(size = 5, seed = 42, target = 8, probability_fours = 0.10)\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zclUeNxosv4k",
    "outputId": "ad099448-d1f2-4471-cbc1-f463293e06ba"
   },
   "outputs": [],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "envzrXmjKRff"
   },
   "source": [
    "We'll use WASD for the action space:\n",
    "\n",
    "```\n",
    "   W\n",
    "A  S  D\n",
    "```\n",
    "Also `game.state()` will say `success` if we succeeded in getting the target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-gSgthFI_wq",
    "outputId": "68af4e66-80c8-4fa0-c7f3-e9ba22923494"
   },
   "outputs": [],
   "source": [
    "game.do_action(\"A\")\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUDdHKAxvZf8",
    "outputId": "38692fcc-bfa9-47b3-82f8-09bee2842d38"
   },
   "outputs": [],
   "source": [
    "game.do_action(\"W\")\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkTHxvvUvcmO",
    "outputId": "f9447b03-b0eb-443e-e139-607f231c76fe"
   },
   "outputs": [],
   "source": [
    "game.do_action(\"D\")\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XO8vlL-4vd-K",
    "outputId": "a6f786bf-39d5-4a23-d79b-17ea9e94272c"
   },
   "outputs": [],
   "source": [
    "game.do_action(\"W\")\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEa2ngmrvfNm",
    "outputId": "c27d9fca-55a0-42c4-dae5-bf8e402d7295"
   },
   "outputs": [],
   "source": [
    "game.do_action(\"D\")\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGL1X29Fy4n5"
   },
   "source": [
    "If we do some other action that's not part of the action space, we will get an error, and the game will not accept anymore actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZeIHbqoy7yn",
    "outputId": "11d15a8f-f09d-4833-8ef7-3bad0510e618"
   },
   "outputs": [],
   "source": [
    "game = GameBoard(size = 3, seed = 42, target = 8, probability_fours = 0.10)\n",
    "game.do_action(\"AA\") # Not in WASD\n",
    "game.do_action(\"W\")  # Doesn't do anything\n",
    "game.do_action(\"A\")  # Doesn't do anything\n",
    "print(game.board().pretty(), game.state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR6czU96cpxf"
   },
   "source": [
    "# RL Environment Setup\n",
    "\n",
    "We'll set up a function to accept some strategy that'll emit an action within `WASD` and check the game state.\n",
    "\n",
    "We'll also add a timer to only execute the stratgegy for 2 seconds maximum, otherwise it might never terminate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdgjnf-8z_kr"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from unsloth import execute_with_time_limit\n",
    "\n",
    "def _execute_strategy(strategy : Callable, game : GameBoard):\n",
    "    assert callable(strategy)\n",
    "\n",
    "    steps = 0\n",
    "    while game.state() == \"ongoing\":\n",
    "        action = strategy(list(game.board()))\n",
    "        steps += 1\n",
    "        if type(action) is not str:\n",
    "            return steps, \"failed\"\n",
    "        game.do_action(action)\n",
    "    return steps, game.state()\n",
    "\n",
    "@execute_with_time_limit(2)\n",
    "def execute_strategy(strategy : Callable, game : GameBoard):\n",
    "    return _execute_strategy(strategy, game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywh0HizI9ayE"
   },
   "source": [
    "Let's make a generic strategy to just hit `W`. We should expect this generic strategy to fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bkhqoZc0IO8",
    "outputId": "149e18be-dae2-4382-817a-620e7b40ebde"
   },
   "outputs": [],
   "source": [
    "def always_move_left(board):\n",
    "    return \"W\"\n",
    "\n",
    "game = GameBoard(size = 8, seed = 42, target = 2048, probability_fours = 0.10)\n",
    "try:\n",
    "    execute_strategy(always_move_left, game)\n",
    "except TimeoutError as e:\n",
    "    print(f\"Timed out with error = {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkuHVdB09sgf"
   },
   "source": [
    "To allow longer strategies for GPT-OSS Reinforcement Learning, we shall allow a 5 second timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK-LfzsA9wbW"
   },
   "outputs": [],
   "source": [
    "@execute_with_time_limit(5)\n",
    "def execute_strategy(strategy : Callable, game : GameBoard):\n",
    "    return _execute_strategy(strategy, game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRhLV_bZMYxy"
   },
   "source": [
    "# Code Execution\n",
    "\n",
    "To execute and create a new Python function, we first have to check if the function does not call other global variables or cheat. This is called `countering reward hacking` since we don't want the function to cheat.\n",
    "\n",
    "For example the below piece of code is fine, since it only imports Python level functions. We use `check_python_modules`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zz80kvg6M4BG",
    "outputId": "f13fdc0d-ddb3-4c4a-cf65-805dfb31dddd"
   },
   "outputs": [],
   "source": [
    "from unsloth import check_python_modules\n",
    "\n",
    "sample = \"\"\"\n",
    "def strategy(board):\n",
    "    import math\n",
    "    from typing import Callable\n",
    "    return \"W\"\n",
    "\"\"\"\n",
    "ok, info = check_python_modules(sample)\n",
    "print(\"Only Python imports?\", ok)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZzVWgKQ-VIg"
   },
   "source": [
    "For the below piece of code, since we import `numpy`, we should not allow the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z89Jw1KB-Ux7",
    "outputId": "1a4cc701-1677-44b9-d44e-3f3f6dfed8d2"
   },
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "def strategy(board):\n",
    "    from numpy import matmul\n",
    "    return \"W\"\n",
    "\"\"\"\n",
    "ok, info = check_python_modules(sample)\n",
    "print(\"Only Python imports?\", ok)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDSrjOTLVyQm"
   },
   "source": [
    "We also disallow global variable access. We'll use Unsloth's `create_locked_down_function` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcmYAmohVqw2",
    "outputId": "bbfcbbb5-8063-42fe-b349-964554317ab8"
   },
   "outputs": [],
   "source": [
    "from unsloth import create_locked_down_function\n",
    "function = \"\"\"\n",
    "def import_numpy():\n",
    "    np.matmul\n",
    "    print(\"Success\")\n",
    "\"\"\"\n",
    "f = create_locked_down_function(function)\n",
    "try:\n",
    "    f()\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tJKwLUgZsRq",
    "outputId": "13588c11-6685-4627-b2d4-445bff9799c8"
   },
   "outputs": [],
   "source": [
    "from unsloth import create_locked_down_function\n",
    "function = \"\"\"\n",
    "def add(a, b):\n",
    "    def adder(a):\n",
    "        return a + b\n",
    "    return adder(b) + b\n",
    "\"\"\"\n",
    "f = create_locked_down_function(function)\n",
    "try:\n",
    "    print(f(10, 20))\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CzwCyXIPK04"
   },
   "source": [
    "# Data & RL task setup\n",
    "\n",
    "We now have to create a prompt to tell the model to create a strategy for the 2048 game. You can customize this to some other task for another RL task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-2RRE4HMrQO",
    "outputId": "332255d7-1e6a-4cb4-9ede-c8a2f01378fe"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Create a new short 2048 strategy using only native Python code.\n",
    "You are given a list of list of numbers for the current board state.\n",
    "Output one action for \"W\", \"A\", \"S\", \"D\" on what is the optimal next step.\n",
    "Output your new short function in backticks using the format below:\n",
    "```python\n",
    "def strategy(board):\n",
    "    return \"W\" # Example\n",
    "```\n",
    "All helper functions should be inside def strategy. Only output the short function `strategy`.\n",
    "\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIdudFUodN4i"
   },
   "source": [
    "First, let's prompt GPT-OSS without RL and see how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HJxrS76h3Ds",
    "outputId": "fcfe2220-fc72-4af7-f8a1-f243311b0156"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    "    reasoning_effort = \"low\",\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 512,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iknaWZNudTNq"
   },
   "source": [
    "# Reward functions\n",
    "\n",
    "We now design a `extract_function` function which simply extracts the function wrapped in 3 back ticks.\n",
    "\n",
    "And 3 reward functions:\n",
    "\n",
    "1. `function_works` which rewards the model if the strategy is a valid Python function.\n",
    "2. `no_cheating` which checks if the function imported other modules, and if it did, we penalize it.\n",
    "3. `strategy_succeeds` which checks if the game strategy actually succeeds in attaining 2048 after running the auto-generated strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JJGXKdJ-Zl_",
    "outputId": "80fd8078-1621-4c64-a906-5204b444addd"
   },
   "outputs": [],
   "source": [
    "def extract_function(text):\n",
    "    if text.count(\"```\") >= 2:\n",
    "        first = text.find(\"```\") + 3\n",
    "        second = text.find(\"```\", first)\n",
    "        fx = text[first : second].strip()\n",
    "        fx = fx.removeprefix(\"python\\n\")\n",
    "        fx = fx[fx.find(\"def\"):]\n",
    "        if fx.startswith(\"def strategy(board):\"): return fx\n",
    "    return None\n",
    "print(extract_function(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLXEcf_HSJlI"
   },
   "source": [
    "Below is our `function_works` reward function which uses Python's `exec` but guarded by not allowing leakage of local and global variables. We can also use `check_python_modules` first to check if there are errors before even executing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3-B0IIsS56S",
    "outputId": "f3e174fa-2fbf-400b-ec7d-87590be3ef68"
   },
   "outputs": [],
   "source": [
    "ok, info = check_python_modules(\"def a\")\n",
    "ok, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFNXORy-lpO"
   },
   "outputs": [],
   "source": [
    "def function_works(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if function is not None:\n",
    "            ok, info = check_python_modules(function)\n",
    "        if function is None or \"error\" in info:\n",
    "            score = -2.0\n",
    "        else:\n",
    "            try:\n",
    "                new_strategy = create_locked_down_function(function)\n",
    "                score = 1.0\n",
    "            except:\n",
    "                score = -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "`no_cheating` checks if the function cheated since it might have imported Numpy or other functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfHzCVx-nGK"
   },
   "outputs": [],
   "source": [
    "def no_cheating(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if function is not None:\n",
    "            ok, info = check_python_modules(function)\n",
    "            scores.append(1.0 if ok else -20.0) # Penalize heavily!\n",
    "        else:\n",
    "            scores.append(-1.0) # Failed creating function\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slnqWG3FTror"
   },
   "source": [
    "Next `strategy_succeeds` checks if the strategy actually allows the game to terminate. Imagine if the strategy simply returned \"W\" which would fail after a time limit of 10 seconds.\n",
    "\n",
    "We also add a global `PRINTER` to print out the strategy and board state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNi129lYTpZ2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "global PRINTER\n",
    "PRINTER = 0\n",
    "def strategy_succeeds(completions, **kwargs):\n",
    "    global PRINTER\n",
    "    scores = []\n",
    "    # Generate a random game board with seed\n",
    "    seed = np.random.randint(10000)\n",
    "    for completion in completions:\n",
    "        printed = False\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        function = extract_function(response)\n",
    "        if PRINTER % 5 == 0:\n",
    "            printed = True\n",
    "            print(function)\n",
    "        PRINTER += 1\n",
    "        if function is not None:\n",
    "            ok, info = check_python_modules(function)\n",
    "        if function is None or \"error\" in info:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            new_strategy = create_locked_down_function(function)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            game = GameBoard(size = 6, seed = seed, target = 2048, probability_fours = 0.10)\n",
    "            steps, game_state = execute_strategy(new_strategy, game)\n",
    "            print(f\"Steps = {steps} State = {game_state}\")\n",
    "            if printed is False:\n",
    "                print(function)\n",
    "            print(game.board().pretty())\n",
    "            if game_state == \"success\":\n",
    "                scores.append(20.0) # Success - massively reward!\n",
    "            else:\n",
    "                scores.append(2.0) # Failed but function works!\n",
    "        except TimeoutError as e:\n",
    "            print(\"Timeout\")\n",
    "            scores.append(-1.0) # Failed with timeout\n",
    "        except Exception as e:\n",
    "            print(f\"Exception = {str(e)}\")\n",
    "            scores.append(-3.0) # Failed\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCpSxtvSeAG_"
   },
   "source": [
    "We'll now create the dataset which includes a replica of our prompt. Remember to add a reasoning effort of low! You can choose high reasoning mode, but this'll only work on more memory GPUs like H100s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ldf6SjLHVPRv",
    "outputId": "589f7523-9835-49b5-c477-4e1d8b0744ff"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list([{\"prompt\" : [{\"role\": \"user\", \"content\": prompt.strip()}], \"answer\" : 0, \"reasoning_effort\": \"low\"}]*1000)\n",
    "maximum_length = len(tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt.strip()}], add_generation_prompt = True))\n",
    "print(maximum_length)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations! We also support GSPO, GAPO, Dr GRPO and more! Go the Unsloth [Reinforcement Learning Docs](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "2061b833-5b98-4a2b-e7f5-4bc4652d8300"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 1000,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases, TrackIO\n",
    "    output_dir=\"/workspace/outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "349f907c-cc67-4890-e131-397694679634"
   },
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        function_works,\n",
    "        no_cheating,\n",
    "        strategy_succeeds,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQhtuwP4cf34"
   },
   "source": [
    "And let's train the model!\n",
    "\n",
    "**NOTE** A T4 free GPU might take 5 minutes for one generation sadly since it's an old GPU - A100 or H100 will be much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGRxPdSCcfC3",
    "outputId": "f8bb720c-6d69-4f43-d9d1-a404842d2dff"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "# Inference\n",
    "Now let's try the model we just trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BZZHOKiF9Ct",
    "outputId": "4989f8d9-d024-462e-c732-b7734676791a"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    "    reasoning_effort = \"low\",\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 1.0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 or `MXFP4`\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `mxfp4` for MXFP4 (OpenAI's GPT-OSS native precision). We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge and push to hub in mxfp4 4bit format\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"mxfp4\")\n",
    "if False:\n",
    "    model.push_to_hub_merged(\"repo_id/repo_name\", tokenizer, token = \"hf...\", save_method = \"mxfp4\")\n",
    "\n",
    "# Merge and push to hub in 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V15Yhj1V9lwG"
   },
   "source": [
    "# And we're done!\n",
    "Congratulations you just learned how to do reinforcement learning with GPT-OSS! There were some advanced topics explained in this notebook - to learn more about GPT-OSS and RL, there are more docs in Unsloth's [Reinforcement Learning Guide with GPT-OSS](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning)\n",
    "\n",
    "This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
