{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fd224e",
   "metadata": {},
   "source": [
    "# ü§ô Huggingface Course Deepseek R1 0528 Qwen3 (8B) Grpo on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Huggingface Course Deepseek R1 0528 Qwen3 (8B) Grpo</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\"><a href=\"https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt\"><img src=\"https://github.com/unslothai/notebooks/raw/main/assets/hf%20course.png\" width=\"165\"></a>\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "In this [Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt) and Unsloth notebook, you will learn to transform DeepSeek R1 0528 Qwen3 (8B) GRPO into a Reasoning model using GRPO.\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIy3QkjW1O4R"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN75nmdx9lvw"
   },
   "source": [
    "Goal: To convert `DeepSeek-R1-0528-Qwen3-8B` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
    "\n",
    "We also use `langid` for language detection. Our main goal is to force the model to generate reasoning traces in Indonesian, and we create a reward function using `langid` to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuDt6gPSW_Ds",
    "outputId": "a744116f-ffc9-4dc4-f47d-df35fa19217b"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'langid -qq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ad395db5267344c4890ef02e5e4e1d4f",
      "53cd752832ca45438a33b9e3fe8b0cd4",
      "7c5656c2634543418d62009b469a7398",
      "4452f2e00ebe44b687915136e85a7f89",
      "5264d84060b541e3a3c6fe3ba3bfd8a8",
      "5089a4f6e98748e1a7e7c3f1f3b43ae7",
      "9c7c4cf7d8c6463c84f324220d089dff",
      "2d54bb83ff5d4da7a4632a3d66a1373f",
      "a4850b84abbf4cf1b56ba25a17f4dce2",
      "1825f9b1db2046b1a6fd4123fd521317",
      "ca439af43b9a4403aa72be5e13980450",
      "0d54762ae2a24176b84f5d1b1d3afbbc",
      "a2ba29de900642099294c6c56149e9b3",
      "aaf72b0efc4941c9b48d0ac486cb1b37",
      "c91a5c8f46a449e383398fbbb817c6e7",
      "ec18ab0c53f24049aa4ff7fa216df498",
      "c1b407fea8934069951ce8653435c427",
      "7e28a3050b6a4b83aa2cdbc6d89128dc",
      "be74b1ea62024e94b06a3c60cd83ef03",
      "62cee809fc314419b6ee8a23b73ad9f4",
      "d8ab91309cd14a8e8d5a9a98cd3bf6ac",
      "a20385beff1b44678101760cbf2a9289",
      "291ed9bc49bf423fbf542001368bf20d",
      "eec61bdf29354de4a170a0b1cbb7f321",
      "6a4af71abb6248909542c1baf00552ea",
      "36fa4e90471d4fa7b4a2e320e64a7177",
      "f7d94cb30e3040f592780b8b01fb03dc",
      "d29cc3cae42c48bcba3b2f600f5723d6",
      "57f3b440014947bfa60059ff565fe426",
      "2d0472e60093417b8bf639c58fb63e5d",
      "b41f9ce14e5d4bd686350d66434e9649",
      "337f029ea24c4cce9bd4f373d1ff0065",
      "7039cf21937d4fc0aad5e551f2de46b1",
      "f6ac40a611b84dd9b0a96e8511ad43b8",
      "1c8e497ba2ce4e01b50c55c56511afe3",
      "5619359815e4404389994acd44172574",
      "2f3db22fe43445e29512ee47ec31767f",
      "ac1cd0df6fad41a587d54f5dff17ddba",
      "df58eef8b3394b33be59bfff71cf39b2",
      "babd6314d5a54d6e94e7ed0e562817f6",
      "f4fbcebf2045446989fc2a4e16692023",
      "3784857e231d4ddbba219f93401cfa8c",
      "9e89da359da44b8b8ff7833cc1971fa6",
      "fb9a8137dee24668a7bc014a483e2f90",
      "2a784021410c42fc96ccd7ab2a930bfb",
      "78a571587dc540bfb7ee64b3faae3dbc",
      "40f970b0096d49a3ba83f2622ef993c4",
      "00bcea27fb984aef9ca72390f2ccbd66",
      "ef0796c6e52f4ef1bb7f3008e140e27d",
      "5de1492b56874abcab6cacab85ffc3e2",
      "eb832e8bd46744e380197d019c1c0a97",
      "657ad4c6599046ecb768694b7fbac8b0",
      "7f6e2e410b1d488cb57380312ad33062",
      "635a7e8b526f4c9082f66c0a120f13b4",
      "3ea6579b20a84744ad3aff349720329c",
      "8651ad3e607542aabfe3036b6f617225",
      "ce9e5bb7bacf4287898e533cb6b65671",
      "8131234607aa41a3957dc3727dc058e5",
      "315f991cb22e4e56b5c757d1b4305974",
      "3d36dc33e0b546448482cbdea4bfd51d",
      "d9ea4865bf7b435b8ec234efc884fda8",
      "a66048eb5c3f41239070521a5fec2eba",
      "32f51a8516144fa1aaed36a17c199931",
      "deece2b54dc5434c8576b327e850618e",
      "5fab61f28d484012bf8a554aa98077fc",
      "ae37b07560c64ca8b2b58336f55d0b38",
      "46362d5227cc4a4b833e8e0a3845f2e5",
      "d375b1bd4ef24154b80a54a345410040",
      "b16bcab6cf9f421f8309f8011ad30f12",
      "06a12077a35f43d0a37c37cc4dfe48a8",
      "6b489672d84e43df9defd5f587cc62c4",
      "b7980c681c584b7781bf80dbb1c8f262",
      "c8ddd5dfd05a4993bb7e4e5513070fc1",
      "35a1f78ce0184218a539a4809f1eec97",
      "652e6681592e4d72a6ef2a83bfffc8b6",
      "dfb7ccc26bbc4841892458131b2cbc5d",
      "f845df9e290040bcab51166539e2d0df",
      "7bb2161a7ab44bc59e704d4fcbda5f83",
      "8d7a46666ccc40c7817786c8f68f3613",
      "e302cade968948dd89fea51e30d30e54",
      "e783000239564e22a6a10b033a657408",
      "88a34a4c3c444a81afb46f5887430532",
      "d0a32668ce5b45fe9973f2fc3e9980d8",
      "ac5a697f1ee345a3830e109b4de60508",
      "1219e0209cbc497b9656988616799f46",
      "bd222e300afb4ce592932bb69828bdb2",
      "7c201ff993824bf6b0526bfaf344574c",
      "78a1225925f042eabce9df1bcb352d5e",
      "a849fe65867a4dd8a44a44158b36f156",
      "73cf8bef5a944f11bcdb71e2180cd988",
      "5ff7d924a91a45dfb73b8c5095cf06ac",
      "d6df73ea477b4285967cdf71bd87e86b",
      "3666b9de655b4c6b99b349ad20133098",
      "f78a473dbaf44eb3ac496b001ae329d2",
      "4d84285525c04defbb7eb9be22924537",
      "deadaeca7c3c4e3ebdbdfb8059fd3f20",
      "9ca7a471f185413aba80cdf826567ac2",
      "7dce192f46b5482b8229ab998cb8553d",
      "d23d3bed1041494b8807c6240002faff",
      "0d16ec356b0d4a9fa9783935c1bf7e96",
      "8e68dc40147c452783d5cf83c1408116",
      "f63c7cdd63e5416db0f0922db3fd7908",
      "d74a1e6c8355475ca070a7d578d89406",
      "f208dd8658844152b7128e37a766da0e",
      "99998ced16f048de877a86c3a150e43d",
      "f20a935c9d9846fda2da9ffbbd302a6a",
      "1516fa90d103489685e38eb97a3d6c44",
      "aecccb9d09714ae2a878207f2ee45049",
      "8f38456f9549478fba7474cb641d3985",
      "91218d9042634f04ab54a1c3a0961bc6",
      "84bb4652a47e4a61a60f0997e9dd3cc6",
      "67342dc60a5d4ad8bdf5158a75e79e2e",
      "c906225802b84aa48552280e73e741fc",
      "666d00afc5c94f4c9500d81bff1e1457",
      "a317980587ef4feb97d3eb2e0fd896f0",
      "342dfbc3768049edadec012d61d097c1",
      "e0ff47045a6c4ea78d8725b5e0972b61",
      "6126329f463947b5a52d44f5a64d11a7",
      "adf8d5bd602f4f53a608611b6fbec736",
      "33e72fbc4c7e473ead8cab7511346a31",
      "f0a98d994a2543e38ad0d927b44b82dd",
      "ff264341a00d4a3eb6077413f82c164e",
      "891ed1c3179b4bd9a4ed216ae6b2e51f",
      "9abc3fa35cca4b889f0d325eb384c865",
      "682b83bc3766438b84399efcdc866df9",
      "15aa26a0829e4738b14a28f723dc192b",
      "4a7f29671ac64829b8132b16503af35f",
      "e6dd68e9ce7840c3818333f73b2bc674",
      "5348e164820944c4bedd824b56bc6f81",
      "bfb9ee43676c4162bf03d5b7a4aff097",
      "f1f88e24679d4e1a90c86256f1c6dd2d",
      "868129d4d4e04cddb654ada36d037e62",
      "d01ca7d6c1ca4343b327055d5eeba673",
      "d88fda96801d456bb8b6adb75cee10b2",
      "a15356f8c0514e41a6e976896c2ae2d7",
      "bf8052a4a83c4afc91e359fa06b9bf4f",
      "78a3ba4b4cd0414a8a7723295e052023",
      "6ab4305498cc4785ac27d4013bc419a2",
      "e8cca5439ef24aabbc17cb04883ce84b",
      "fa58952bf2a54a3fa1ee27088255b60a",
      "cacd9c7a3ee748f496d1d28ac4069ead",
      "96a5bb46c47b431ea479d3d04e3f9c8d",
      "ffc7702cbb9b4c3d907b8c051c1322c3",
      "16eaff0c30f5448cb89a63e04f86a3f8",
      "8dc958d62a124f6a9f443cafee3aab20",
      "ab163dc587cb479aa5c0c1e7acd48deb",
      "b5b60cfde7d54b63a81c7cbba19dc9ed",
      "39ba1e8385244ca4b86f148dbdf1a3c5",
      "c4544a2669794cee8b6ea0f862dee5bd",
      "3f605c4bd9b24aa7902e9cd1b20734d0",
      "d8fa448f44ed43e98ab90cc686005723",
      "098ddd74c04f4b478f4a5918a2a1c6f5",
      "d4ec246b8ee1444d85e5e45417a074cc",
      "176bed311f074b1db87b90f64f2e1c62",
      "7124c93d897f455ea832d83a47e22398",
      "b66b8205a1ac46968aabba1224b1b5d6",
      "7f0923482366431bb5e742b367b85717",
      "4f52adf53fd44910b7be69356998804b",
      "dc73460dd339407a8ef70d70b0ff2908",
      "18e1cf02ac8e496d9ec6c7043700bdee",
      "caa9215388f844a0906556a35871aa92",
      "39162ce93b574ee48f8d510748b023ff",
      "9846b47582494a01ab0413a339b69ac2",
      "0e31a330d9b441d2b8031d705e4525c6",
      "116438a432534157854e55edd17290b0",
      "68d5074e7b57464eade24db952ffa466",
      "1b54692855d14197a412bd73a5d589d9",
      "b002b5685ff84e7ea96836e778a27f31",
      "3639b294cc494a2a84b997949fa4aa1c",
      "bf202d6af8484ec88a247ca5d4d05a98",
      "6799224cb66b493ba04ea79a8d11810c",
      "4836517a4f53482d9d863e919f229939",
      "19896a921c694bd49e6cd8e58243c9d5",
      "46a39e03b09e4d4ba611be35a4a38a5f",
      "397d2b8d165f4139911acd86d399da61",
      "cd7cc23e780a472fabbd04579bd3fc79",
      "6d6bb6b5551b4c4ea16546cdc9d55491",
      "6d090dfa0a304b858c9fbd75875566a3",
      "86ab8244e6d748acad911e7203bc5a2c",
      "4cb5793862184ef38e2ad7637c750043",
      "71c4b092ce184f2098c9b2d0b23e8923",
      "acae05fde8de4c6daaede3de7b0b3a13",
      "878fdcdafe4d4ec097c4b2313040fb72",
      "d20d86afc1a94664b60b478fbce12f24",
      "a4ec0efb0aac494f832c5b2669d2e1fc",
      "7c045fd658ca416ea071ddb00f848bca",
      "e7bb7f6b7e594b46ab95ea445df94196"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "6a976e17-8336-4d93-840f-d9c0a588ca25"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IodK13om1O4S"
   },
   "source": [
    "### GRPO Chat Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cGSRTJo1O4T"
   },
   "source": [
    "Distill Qwen3 from Deepseek has a chat template that is used to format the input and output of the model. This is used to make the model output in a chat format. Including the reasoning step. We have to use that chat template since the model is trained using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcLdymBEHdk"
   },
   "source": [
    "Let's see how our chat template behaves on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "gZsp5YKk1O4T",
    "outputId": "1e284c18-2a24-4f8f-af8c-43bcab7d4f1a"
   },
   "outputs": [],
   "source": [
    "reasoning_start = None\n",
    "reasoning_end = None\n",
    "user_token = None\n",
    "assistant_token = None\n",
    "\n",
    "for token in tokenizer.get_added_vocab().keys():\n",
    "    if \"think\" in token and \"/\" in token:\n",
    "        reasoning_end = token\n",
    "    elif \"think\" in token:\n",
    "        reasoning_start = token\n",
    "    elif \"user\" in token:\n",
    "        user_token = token\n",
    "    elif \"assistant\" in token:\n",
    "        assistant_token = token\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "You must think in Bahasa Indonesia.\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BciEDYSSYFNj",
    "outputId": "1b63dab1-b0e2-43ba-dbb2-10b2899728a6"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"<think>I think it's 2.2</think>2\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"<think>I think it's 2.2</think>2\"},\n",
    "], tokenize = False, add_generation_prompt = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201,
     "referenced_widgets": [
      "c5560a8612a34eaf96630cd90454cb2d",
      "8ab97b85d9484a6cbf654cbc954f75ff",
      "4b6aac4b3426447bbe45fdd14bcce426",
      "58ef324089874a49a1fe7e5f94ff93f0",
      "bbce5a3db2384bcc8ebe20d736aacd2d",
      "ee569d9509e54678a321902fdf9a040f",
      "fe1d739b0b5941bd9d23bf137b0e98ed",
      "839d4de9dc6c403d8db455c37093b5b5",
      "756b6715039c4c75b38f9f711eca51c8",
      "73a15eb988404223b07a8e3233a1d549",
      "db197b2558d946608af02e62751c1de9",
      "dd08b63a04f6409987863cd204d64796",
      "f49189f73d4e4031a086b27eb3e54e4f",
      "0f4cb21a4bb84aafa59ef9baa8511714",
      "e16262d41d5247b98e5c946f04e2b8b1",
      "e1feda9d8d22452e9d253b6635af7caa",
      "d479e0e9318547099bb549e32897481a",
      "aa51b6a90f304328840503bc39898320",
      "ee904e35b4654c6a8ca8a9bd47924a86",
      "1b857d475d0d400d9ac2aaacc55cfd84",
      "a6f28bc3c2a440efbae14dab1246de1c",
      "325a955756a549ffb375150dfd6c058a",
      "4db31459944a4e84b1335981f70067f5",
      "e9a22bd6ca034cb89c2e218eb63ab520",
      "67c8b33fbc9e482a875e02bb7750916e",
      "82f6146a5ac847a58a4d9ea42756464f",
      "57bda5a2a1fd4366b2cd454f055c9d5a",
      "f48756b6ba4d4189a16d4092bf936677",
      "fa1cf861a0ec4ba19e0dc72fb1a9edd4",
      "4d97073b9a1e4071a13dd95c77efe4cc",
      "eefea66fa97d45e9a007ae77f03dcb1d",
      "b6fad7ea371b4ef798f150915623b72c",
      "43f18a400411445abe16be9b174c5726"
     ]
    },
    "id": "o7-eUrQn-OzE",
    "outputId": "a16658cc-4b2b-4c55-d649-c37cda2dada5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b00gUsS-ROW"
   },
   "source": [
    "Let's look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "siopxjG8-ReF",
    "outputId": "0741a0d3-5771-44b9-bb84-80daf02f86a4"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KGupRQqD-Wcf",
    "outputId": "e8f5aecd-ea06-4dc7-8eb3-c5b5741e2285"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmnXj6hn-Ydi"
   },
   "source": [
    "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8JJGXKdJ-Zl_",
    "outputId": "a159b4a3-bd66-44d6-e7e0-35c7d1731060"
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    return text\n",
    "extract_hash_answer(dataset[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K30CygaU-dir"
   },
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "3ac9f0d0c413423a9493d102eac493d8",
      "b76bfa16f00d49d29ee7e189589515c8",
      "9437a87b15dd45f2ae8a0b7be03959e4",
      "5f8fc9cc57214b1ab17401a8b1acd75f",
      "634a7158765346d590a8959177949538",
      "46547ba4e80b4f23990910da0d20106d",
      "ba299dc68b384db7b7416be003c78579",
      "039664811a6d4b5e8a179ee64f0f5de0",
      "9302d554d30844bd9dd0f1f9bd506a46",
      "e156ddc319704e0680393a9bc8261c71",
      "1a47b8435ada42c6af0728a55deb65a0"
     ]
    },
    "id": "qyEVI972-d3n",
    "outputId": "fce0b18b-7ebf-477c-90bd-f2eda253edaa"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9m8eR9T-gMh"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQwjTjNz-gY_",
    "outputId": "36474c89-bdfb-4225-f1b4-cd3f1edd0d08"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = rf\"{reasoning_end}(.*)\"\n",
    "\n",
    "match_format = re.compile(solution_end_regex, re.DOTALL)\n",
    "match_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OycMneOq-iNC"
   },
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndzHnQ_6-jHt",
    "outputId": "f81976c2-45fb-4fdb-c02e-86297592a43c"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"Let me think!</think>\"\\\n",
    "    f\"Hence, the solution is 2.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRMDAzDk2x6t",
    "outputId": "593d7596-8b7f-4769-eca9-bbb6b15c0515"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"<think>Let me think!</think>\"\\\n",
    "    f\"\\n\\nHence, the solution is 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOjmO5l-kl3"
   },
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFNXORy-lpO"
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfHzCVx-nGK"
   },
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <think> since we always prepend it!\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAUWwtE-s6n"
   },
   "source": [
    "We want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmtI_8gg-uIE"
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        # Correct answer gets 5 points!\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        # Match if spaces are seen, but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
    "                else: score -= 2.5 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 4.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMyfhXh-v3R"
   },
   "source": [
    "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
    "\n",
    "We also remove possible commas for example as in 123,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVW0kL8q-wL5",
    "outputId": "0bb3b3ab-47ae-4d59-c1c1-2c20a3a9efff"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "print(match_numbers.findall(\"  0.34  \"))\n",
    "print(match_numbers.findall(\"  123,456  \"))\n",
    "print(match_numbers.findall(\"  -0.234  \"))\n",
    "print(match_numbers.findall(\"17\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19KD28CXW_EO"
   },
   "source": [
    "Finally, we will try to enforce the thinking process to be in Bahasa Indonesia. This is a simple version of the `language consistency reward` that is used in DeepSeek R1 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PU4QSGHW_EO",
    "outputId": "be13cfd3-fbd1-43b6-8567-5ac06e020038"
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "def get_lang(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"und\"\n",
    "    lang, _ = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "\n",
    "print(get_lang(\"Hello, How are you\")) # This should return en\n",
    "print(get_lang(\"Aku berpikir kalau aku adalah kamu\")) # This should return id\n",
    "print(get_lang(\"ÊàëÂú®ËøôÈáå\")) # This should return zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czn2loIDW_EQ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def format_and_language_reward_func(completions, **kwargs):\n",
    "    scores = []\n",
    "\n",
    "    for completion_item in completions:\n",
    "        if not completion_item or not isinstance(completion_item[0], dict) or \"content\" not in completion_item[0]:\n",
    "            scores.append(-5.0)\n",
    "            print(f\"Warning: Malformed completion item, assigning default low score: {completion_item}\")\n",
    "            continue\n",
    "\n",
    "        content = completion_item[0][\"content\"]\n",
    "\n",
    "        lang = get_lang(content)\n",
    "\n",
    "        if lang == 'id':\n",
    "            score = 5.0\n",
    "        elif lang == 'en':\n",
    "            score = -3.0\n",
    "        elif lang == 'zh':\n",
    "            score = -3.0\n",
    "        else:\n",
    "            score = -5.0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjTfmkTAW_ER",
    "outputId": "4e70daa7-3ac0-4e66-ead5-8541b7905185"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n",
    "]\n",
    "completions = [\n",
    "    [{\"role\": \"assistant\", \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\"}],\n",
    "    [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
    "]\n",
    "format_and_language_reward_func(prompts=prompts, completions=completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfaaAywNHHh"
   },
   "source": [
    "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjBFrttr-y1_"
   },
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOR3wJ_AyLr"
   },
   "source": [
    "Get the top 90% prompt length so we don't accidentally truncate them!\n",
    "\n",
    "Ie we'll remove the top 10% long prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "efe86cb2e7174a149f1c01544b1f9d4f",
      "73b5b814482f4d1faa3cbd8d59520b19",
      "1e53f50fc9de48f48919b3eef416d825",
      "c0b6b9c6c70946f292cbd3ef194647f3",
      "2c4bbe0814524b8dae16f0ea435f388d",
      "4d9c429efca2406ca4dbb606fd888f27",
      "2310e1c0d376470990d063de29eabb68",
      "a58bf1ccd1614069b87c278b14620636",
      "27c53c1f5c934e1eb381ed302f186b85",
      "b8d562a3b0fb40e0a15d87830f90aa4c",
      "b2520312d2a345b3913ef02bca7d060f",
      "4bdd6660598447aab9bd8249d98ec407",
      "6bdca654291e410da6f0859be242a3cd",
      "38b11c99e03c454499f6013ed0326e4c",
      "74c2df2966ab4e30b27a326a6f39993b",
      "92066c72faba4956a29bde4deaa0fed3",
      "a670d8e6c3414a35a369476b77a178ac",
      "976c8cfbb4cd444a83f28a65c75e057d",
      "efff7b47e0c543c1b579ca9f855f4954",
      "1eff3897c2af43fd8be2d2cd0bc7b512",
      "57d307bad8e7431eb9d9562ffafd7d17",
      "f77640ae66ae410d9a1858cf7880a192"
     ]
    },
    "id": "6EgAi4Q5fGE-",
    "outputId": "e3719f82-3c6a-4cb3-9bc1-995cd67689a9"
   },
   "outputs": [],
   "source": [
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "bbabb969-329e-48c3-d92e-c27f35fc7766"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir=\"/workspace/outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "58162095-173b-465e-af98-3c717e8d8424"
   },
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "        format_and_language_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4a69932266bc4a46bf72764abbe0bacb",
      "e9c1cebddffd4753a91c895a9ef8ebb4",
      "e25cf2e1e05a425bae6330c71774efcf",
      "f4fc91122392442da57ec2862cb3d90d",
      "2fa427504316456abd55795cbf15f96a",
      "8b5a4d10321446fb869172f7165c7cd7",
      "fa13a14340074376a0fa8d14e25ed4ba",
      "f08f885e86bb43cd960878dbbc35b99f",
      "fcb1520bc20e40e8b6cd276fd6892f75",
      "b1c1ec8f20a4497da3988ace5424f6fd",
      "76534d6f990243f1a635665d2e47a33b"
     ]
    },
    "id": "qtcz_lpbVC92",
    "outputId": "d9fb214a-a516-47d7-ce73-ef9290f74fc9"
   },
   "outputs": [],
   "source": [
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AL-BcuB1VLIv",
    "outputId": "c23a911b-57b8-4af6-cce4-8cfc3793a856"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SfdI-ERbpiw"
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test. We tested without using our custom system prompt which should not (or minimal) affect toward the model's original reasoning ability.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190,
     "referenced_widgets": [
      "4e1e6351f86a4cf99ea07d6a1dd95b3e",
      "47a7688ba9044ba29ff56f32244b416d",
      "935cb5ca7b2f45f0884a59e3b9f3a7f4",
      "d324dd4ef84047aaaf46c5ccb9607638",
      "7f52de137c0748629d891d0e6223b81d",
      "1fe15cce61e944d58440251cc97946de",
      "7342685a5c6b4d2f8b69958792705d30",
      "a62fed49c7a14e14811ce4dfa4e02771",
      "4b7ef318e5a444a4808910bdd24699c8",
      "eb790732f82f4973a2669eecc057f711",
      "18f1fe2f63134275a131896d4fe1eb6d"
     ]
    },
    "id": "X6lXk47v1O4b",
    "outputId": "c0ddc7c8-fefa-4883-a128-304da0fec023"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g399AC2B1O4b"
   },
   "source": [
    "Next, let's test using our system prompt which should use the new language :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "622c7d8a89964ee3871f60d1d32dd9d7",
      "74655a758460452283c5487e67a09f98",
      "518811664352462bbd70d54d7ff75836",
      "a45642e387ce4d0a8cf69e1f36d0b279",
      "09aa5ac028274fecb5b8a777ea7709c2",
      "c937e93a5bc44a639fd4f7f543f86513",
      "46e960f31b954d779413192779825fdd",
      "872b861723d5466b99002d880af2987e",
      "39ad0bd8a1d74242bd72404859d7ca27",
      "54fb6e96cfeb42b7a64afa1dcecb080a",
      "34ffed32114e410a964ba6bc9f4444fa"
     ]
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "6522d0ea-e766-4964-f0b1-bf1fe0631867"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad5qCZMsW_Ed"
   },
   "source": [
    "Lets compare our results with system prompt but without our LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "f35b16000bb448139b0954c4eeff99f8",
      "ac02d83c54314dd889853d5dfbfcae6f",
      "ccb09cc19f804fbca9e3976116227460",
      "efa40eb719fd472f804af545734a4fc0",
      "17813750f23d44fb9c9305506030709f",
      "0693028d222b43e9852beec995ecb5dc",
      "7fecf7366e9d41cca37f32d736eed6e6",
      "783d83041ef2494287de1352d66e7e85",
      "59b5cb2fc77d46d395b22ef63954a7f4",
      "3b161341e8d64f5392ebcebf8629df1e",
      "e98eff347291445f851d14470b0552cb"
     ]
    },
    "id": "ee10WWhDW_Ee",
    "outputId": "3f7e2b03-c562-46b7-f9d3-27a36af3b473"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYqpfCF0W_Ee"
   },
   "source": [
    "Let's take 20 samples, and compare the the amount of using our LoRA and not using it, and see which one has better amount of correct language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJmztPHdW_Ef",
    "outputId": "2ea0c34d-abea-45c2-b04b-616323f9f1aa"
   },
   "outputs": [],
   "source": [
    "sample_dataset = dataset.shuffle(seed = 3407).select(range(20))\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "47fe26074e4748109e0e3e3b5e437da0",
      "f6b16c88dec2470783aad9327b744a30",
      "346c6c864b5a47d3aafed9779efe8e7e",
      "17a278ccd27b455d8d4c1ef9217720a1",
      "bdcc0fe600254330b0d3b26ffdf54e88",
      "f9d137906cd34198a886ec03d29e1d25",
      "ace6a3ba68d34ae0b402461d0e7fd0ce",
      "ddae813db1f445b39b87d5e37d927258",
      "f2cab422ad1745cc9e2a57f2b7cbe6a5",
      "54ad84288d284fb68fe725a99123d078",
      "09e0d1738e5047969b1b53a8051000b6",
      "102d1dc21059495c8987fd5473187bda",
      "a79a61e313d34c138725d5a36e22b498",
      "43dfed9b87664340847c48d5fbc54a7b",
      "4ae3ec784a2d42c5805ff73dd3c4e599",
      "f8f86f755c0940cf99bec24363177e30",
      "283fea36af804c01b7281548570c1fcf",
      "c530f911290f4b6abf7c80dcd221d14b",
      "0e3a5b3df1c3483f9b49968f2737df07",
      "9078876536734ef8a2554b4741b3583c",
      "e184cfe6204446edac5eebb249ee5a6c",
      "362fb4499d2349ff9700d484f0d75859",
      "d863ad0a6a16440daf5c5dab3c2ae8df",
      "a77f90d13e6c4777bbcc967ef2d867f5",
      "6d352bbed70a4487a0875fc5b53cb417",
      "6e7b2a041abf4f83afed2670e1b7cd64",
      "0a13e3da3fba4407960e565c4337f407",
      "c5a0d96ea1de433ab152dd828bd5206d",
      "758d89d3d4ea4084a18bc13216789357",
      "8f78548a239f4ad797233826ca2d18a0",
      "437b9000fdb64f5685a63f3cf062c6b4",
      "3fe86e05b86545ccaceb6b44f7a646f4",
      "76658f05feaf46d4afd4d21445a98b46",
      "e35c0be4a7bb417f9c79f4195bfe8b80",
      "aeb8bbb7f8de402e9db1bc662115ae7c",
      "04762bc620b44de1828f231355ecdd90",
      "c373be24c8644d21bfbfde16d495f2ec",
      "a184d9c3f4be47fc99d67a24e43c6fca",
      "a3e84ecd74124797a271ea1bce32e162",
      "e29fcc716fcf40929c24cdc89a7a9187",
      "1db747fb25f7480ea51477b12c06b2f3",
      "5f558701f9b843b9b88deb238006a1d8",
      "6d69af9e759a4d7e9ea8dc0a444a2c15",
      "a1cb83589109491792b38e2e977154cc",
      "c8da96ee22bb467ab6c3b2a4c56c600b",
      "ee41fc97fb0f47cbb5abe3e995a18409",
      "1a870f1827594b5a96fd4d155ca748f2",
      "2082e29d6b5e461a8b53c75a9493acd1",
      "200c202ce15545c28791128def355f7f",
      "7ffc6b43a2204306933c9dc3e79f73a8",
      "53f260ad83924d5f936639da54e3b711",
      "ae5c9c693c0b4d3a9390abc0b69d1b09",
      "669d68cc8f5d4922aa50acf9436b91a7",
      "45bad1acdb5347c9b7bcc16aa33022f5",
      "19646a2143d246218fdf3527bbaa4944",
      "835635745d8444c094328b88227f83f0",
      "3b0d818371de4b788ae5d301f3b77d40",
      "72c3038d15584fd186acd123a8f1a70a",
      "94753efe1c3d4e9580d59a7a64159455",
      "7392747572734c958e3509093f5232ee",
      "7bc7fb6cdf0042f597157b85f759069f",
      "cc61f72a69e44703b430a7b4bfac5d11",
      "2659dab28e8f4ce5931ff109bba50752",
      "d87e263201424b0c91afe1ac54572e46",
      "f565b58dd1e745208f6b56dd14e7cba3",
      "0ea5309034cb45ef854fa9d43da0c9a8",
      "3f8078fc9a3b4ba58c01596d280de632",
      "c867662d81734b07bc41ae3b6d989fbd",
      "5ff2f119429e43af958b34862cf88ce3",
      "4b946e624f534c87b52844fbda1c22c5",
      "0cc3177fc6234ac3b19bfbada11976fa",
      "ae6afa77cfd94387b234842e7b66e411",
      "8dd9ea1ecf464c048d6b0620187b2adb",
      "4a498b8bc6ff41619ab51f7ddf444f21",
      "738fbce122464c9c9ae2543b6a9c6221",
      "98d32f0666504cab8e3eb07647170931",
      "492fc78e7c29433b93e3380a62a36fd4",
      "622a9c4668ed4ce496eca13544ebd68e",
      "43cf985162de4a23b8c3edd514a2dc0c",
      "4ed61277a1cf4b83934a8f6270291e33",
      "ae5c4e6818324bfab094faae4a36863b",
      "d57a3a971c7140529aa0219fe755b771",
      "f235d4c09a924bdba005c1455eba2a5d",
      "c553b7b45bfa44c4b733c9951e33de49",
      "c4077902dd614314af0319e82ec8f22e",
      "0c9419519ab24681824826f94ff83460",
      "c3a7975fd0ea4bfea2522beac4ebe11c",
      "ff0a956721284a99ba368a9de27e93e2",
      "6815db6af3634835b02c176b54e49286",
      "4ef815b8a6a2446aa64cd6f6f23f1cec",
      "a8ac6bda67b74bf8bb453825dcc81504",
      "520e49e0ae914a568e7f472853b83263",
      "ee9905818b104b41af14c30cb62947f8",
      "b12de3a1524749d59edeea1d46bfdf3d",
      "015819f39cc845d58c5cdf14b52796f8",
      "68edf6e2eb064339befe44d77ebe5f19",
      "fc1234420dca4ff4a0a7fd3bf39d9cc5",
      "a95bc1bba4614346906d22679e0a5700",
      "21da79535a5e4715b8d0bbe6a84c4156",
      "b5b55ae398044f9797ad9182f0ccc823",
      "23d38bbeef094de7b54440ff122b2627",
      "c714f65b6a7740e4bd597e941f0c294b",
      "7d6f0dd934ce43a6b319e37f37789fd2",
      "3fc498ccb014473ba816a42b39d0119b",
      "1ac39370c6084132989dd868b7454345",
      "2156d69b6028433d8e00edd98b5081f7",
      "a57a955d56e64cefb39c6c9c0674c9c4",
      "39244d0a8b404a07b3aac7dbbeab4ae2",
      "eb59debe975546eaa3b6790cce0cf22f",
      "41573bf3e7a74f67a5e923b3bf320cf0",
      "7d456bcab176487bb92d7945aa208985",
      "8164f92e06e5496081ebc95c3bc5bcfc",
      "9c5a549bfaa14591b64a7ee0c508e787",
      "d5306ecc83aa46f4b9b32d83f6ea39a5",
      "573e549f4c9a4bccad7785d2dca0d085",
      "6280bcf7c44d40d3ae7fd5a55b4435f1",
      "19746b5c9027445fa337e45c65f88b18",
      "c46943f6bdc64cfd9aeb0a54c2814aba",
      "5c4c43f02b1d4f2caad6bc2d0c972ca4",
      "e6f13effff81490f971915386d700b28",
      "4a4deadbd74b4607ad2ebbe67cd943a4",
      "2196b8cd440e478784de47e5dc099275",
      "e75ec870cdf04e85ac6bead094472d83",
      "40c112a0b0434dad8f1986d360a64f67",
      "c4e47c48c38f40eb9bd9f15da84dfb25",
      "b6cc8cbb2e2a4eb389b38a38d59ca746",
      "36deae094c444c52a2d41ccebcba30d9",
      "72e45e105b1a4a66bbaefcb2b99244a8",
      "e57502b7af9341cea46d2f319df6dc84",
      "ca3c7377cfca4defb46ce81e4252056b",
      "7bf5b846f29641a1ae06a001a6566af0",
      "ba3d68f9730f4531931440e82fc20259",
      "4cebd3a225554c0a8e0c4a5323e5b5f3",
      "c68eabf0e9084148af917ad69d0f291a",
      "11dca6b14f654a3f881d7942fcb76b11",
      "730bd5d31b5c469e9d01103a61044b5d",
      "76bef67505e447d79bad5d759d5ae5c6",
      "25cf2302cb954727b2a7fa76c9a3e559",
      "d3ea78e924944f1eb399d6a0fc46bba3",
      "d9b0563fa75046859b311a6a6b924754",
      "52ffffe2447f4d46851cf2091d2489a5",
      "c1dcdfff861649ce83651e1eff1a3c49",
      "971653b364ea4526869e4f451ac4f1e3",
      "b1ed69dc1a2247f7bb088578a98c9c79",
      "036640f29e2d4f92aebd5e155b19644f",
      "d37d413784864f008b6d90701d2f158b",
      "2ffb26fcb39b4faf95680890dff651ad",
      "91e6b34c909a47dda1486851524aa258",
      "ffbf7936ce484cfcaf0648fcfc920286",
      "d5fd8382c3104e9badfd19deb219c2fa",
      "a2ce361e5e324aa084ad7d171e7c6cb6",
      "d0e3c9f8274a43649ac0f8edaea63788",
      "19da85bb1cc64ca286eba0b8d53914ed",
      "395e110f29b34500804a0bdffddef7dd",
      "b2dd5fd61aaf4235b9011856df2b0f46",
      "c162fd2ae9274ef691982e22f4382054",
      "2824756c51aa46a3a80201be5c51700b",
      "b479c1a1c21f42279350f1ef46c96b84",
      "24ce824a8bdc44979717dc8fc6bdc2b1",
      "4964a52db09e4e7b8ab80f10e6063fa1",
      "aad966f64273400ea4c947a51da9a81f",
      "db3576555492431bb423feba4f76b8d0",
      "1f86b4dfe21e47ada984ac313efa0817",
      "136529877dbe4bb5a814c774e629e1b0",
      "63769f20ce454f859db7991e5ee26a1e",
      "4a2e7e51398b46f5b1edbe6c583b0fee",
      "f4d67f931ad3433897fadfa57d893311",
      "73199a8803954d4c8f838fc48005ba89",
      "3e1536954e9d4902a82a8df5c7a15ce2",
      "c67c69acb6c74d6c8d0e3606cc6c5771",
      "262a15c6398b4c13a6ff6b39c38f30de",
      "0e7ca1e4190c4561ba8d1123023a10c9",
      "30d4eb4f89d24825ba8bdb11c083c20f",
      "e1ba725e5a924bc38dde0b304f68a290",
      "1e5d173e008c4d8ea8e0375d9080cf7d",
      "1beb336970074f3cacd897cfe56bb48f",
      "3dd37a0d116b42809a3d3c97a9937408",
      "46bc292c79de49e39582bd380b7cb24d",
      "d56efd640e23428eb12271af3bcce0ff",
      "9c666db21edc429ca402c5571697b5f6",
      "4959b758cb304a84817bcd67838f55c5",
      "f49ac93865c54972a31484fa11585aae",
      "819a7b4d4a2d449297e93555ddd66301",
      "019363eaeada42b59a2da39e082741e2",
      "e495962ffa0b4548b64720a4bc51f838",
      "c61a2d13dbd744109e025a7d5a24f1cc",
      "27fb8fc28cd84e809830d2f8ae6be768",
      "8ccd3bc5d689460da73a88f9501d924a",
      "9a59dbcb79754437aed83b9ea1d639b9",
      "52201754b0bc4091a0c04876a5f99a07",
      "810aeb2be35e453da30605bcc4c69de4",
      "9ee433f566024fb69732263a5bfdc1fb",
      "be5df40a08e4417798473164d0de1838",
      "d2b91ac293db43e98589924b004eb8c3",
      "da3e2830ceb14506aab3659f1b763394",
      "574c168e6eef4fefa6e1c50c00fb5898",
      "25ee11ebb92d4dd9a3c99335719a1372",
      "a0a25b11371b4578af152f0c0ebda4fe",
      "59b478cd6e1e4b4cbcdc7e086d4403a1",
      "359a6b6e73ad4f00a12b26babf3556f5",
      "ca34502cd5844bb1ba71e69fa1531657",
      "8166ac4b22584cf399fdb1f198a48d1d",
      "4151f29ec0e34c338289db98596b022f",
      "f318b8c1e9424a15a601a9ce3ea12c38",
      "f3177b640bf345aab255c08c8c808fdf",
      "2b024e8e1f1741d5808d13dcf431a61e",
      "822befe58307424f8d9b07f98565d561",
      "36857dee08bf46678c092bf6f1de1468",
      "33946ac898f84e0e9be4f3ec398b1720",
      "d3d84f255b1e45ac9b2d9cba4810993f",
      "a8b206b878de4a2bb7af14bd282e2987",
      "63179fa57c984dce9b6dac7524c19447",
      "f069a144805649f6b7358d855dccb7dd",
      "864b4d9cb19a4910abfb448689a55603",
      "700cbeb0644d40cc8954b93fea69bcbf",
      "d523f309efad4cfaa156f7f670bac10f",
      "729e27a0233b4f92b13d72df1b21b2fd",
      "c1130e6bb0c5407aa9d75ae046978be3",
      "845828ad19654d219b93303c33db8b0e",
      "30699b746bd440b6b2dca85173c304d9",
      "565bdcc9c19043a88bde9b0f9fc910f2",
      "a984b3e68ad4400fa386e728f497beab",
      "fd2d7856006340ad94d4ab083f231279",
      "af4f189380ba4f118c4976d5379164b2",
      "cc5bd76b7d2b485b8cc4cca9789f8673",
      "b924d99d60bd4d3fb8697556ec41fcfb",
      "9745b8f9bd1547ea8002b4fb0173c722",
      "e41c1f571c4e4036ba6491e564c4a763",
      "c49d6c830dda4284b13d1d99aef88a58",
      "d9b0bc02f27e47948ddadf67797434ca",
      "36d8576d8ec84d78a73249b9b111fa77",
      "5ff63896f6d64d4a80a858583389d0c6",
      "bbbb033fe303485a8898a739a2ac35e5",
      "469897e960394259aaa7fb39f572dfae",
      "71522dccde854cb0bb73fa03d1ceb453",
      "7d50d57bfd5448aea2d741e07e50aae4",
      "d99029215cbd465586da796ed65117e6",
      "93ad654c0057407daeabc7b17ddae867",
      "38b90c0e78734bf18cd9bf0dbfe5be7a",
      "5ff8b298d4674a4eb6dbaaeef4322c56",
      "72feff0edadf43feaf280db5bebe7ab9",
      "d2d4857de45b43bbb0559a5303c34932",
      "674a9a58dce84f5fbc2e8dcf65688775",
      "0b474fbed20747f0ba7839fd69957095",
      "b264371e632e4886889f4ed9c8b92560",
      "636caf57d5b34d66b2a9175585e052f6",
      "a3bad7d0f6f74c79adc956e392c1f5c1",
      "438a1b373cbb47978490ede44c7a1da4",
      "1192002b51a84875a22143b484e882a9",
      "6466e87253f746a3bf927a70100453f3",
      "c7acd7f863ea4b01999ba26691f6929f",
      "628cd4e9b98b495abf3a5f6214d8ba71",
      "094e9a7f211e4371ab0ccb98f0f03846",
      "43d9c172daa742d7a055e5af64e1890c",
      "7e92cbe0211d497b9657f715ab7636cc",
      "46c83b4371164bd381052cf494922f69",
      "43f2b15c83044b0a8799f4d21be491b4",
      "3df6cc80de344feda6393649f943b715",
      "039196b481e040b1a5536dc5219b38b0",
      "b4fff616bb62448e8be771f4727f2d4f",
      "2ffa0b0732f84680a8b1ef607b719b87",
      "f81e6ca8ea364070a88ff187187d7522",
      "377ca9656bb54bb2b2f43b1290854396",
      "6ec9691a69b240c0842450b17a3dc1ed",
      "cb48ecc9c9d64749b93eab50553a643b",
      "b9267d3d61344b0e8796de908733f912",
      "08daef067520467ebaa027cedc8a895d",
      "85edf75bd18c4381842ae6f3fee8512c",
      "9fd9a19e00884434a3ce89863109b5eb",
      "9232db089f834931b8acf50bd5dc6cbc",
      "616906ac2b2c4540a1b712d8f76da854",
      "2ee5dd0c00584d389c4a64e633eb59c0",
      "f4e3619de3eb49fc812a2dd52df2b59e",
      "ac62107beb59424d911992fbb050a1bc",
      "3aceddff8236439ab3e4db30b918b174",
      "9406c36e359942e98b83e60fbe408d21",
      "5836ddba34a041bbbfe66aa5d20568d0",
      "29752439eb5940f7915d35712ad5f54e",
      "8617aaae738348a0b5c0e5e4d334fd4e",
      "75ecfa2ebd62408a8ff62871d82f47bf",
      "b22ffc37c2e54886a38a31b28027b51f",
      "5c7efa43d8f0469d929ae34f5425d3ce",
      "85b6f496f3ee4628bdda3e5ff90a626e",
      "6fd9f07536004cf583e9b45d6a4254ca",
      "49e8bec8a16b4afb908bdcb0a1cae9e8",
      "1125cd031b7e48a7bf1574dd00cb536d",
      "790aa8865a434c548004a3c75729ac2e",
      "d1b203b5a91a4e30a44844014606cf49",
      "b01fed9deffd499e81f858a3a6271b0d",
      "d8056d29ca3a49a78bedd271ccd62877",
      "8c83d20bf2d64190b8333b90bbd30e00",
      "8144b3ee3e06484db0139c7f1dd8a127",
      "1583db27212246e78edb8f1734a29bcd",
      "0165d1185a3c48f19a691a3446a80550",
      "072bdd89f1044a1bbf0be4df161d0e3f",
      "f063d9877ba14768ab3b0903454d75a5",
      "75bb5a928b4848bfa1e7372afb3d2b6e",
      "d2de8b6317a844d8b9040b62a986106a",
      "bf61b54f22b14a7893fc3e8ab8286068",
      "211318b770754f1ca00fcd98f0c2a708",
      "0033cc4de608444a85bbfa6ac98db926",
      "8e7120d574de4f6ab942881c510fd0f9",
      "dac762b385f34159ae5418607b1b99f2",
      "ca3227078b474e55bd06eb8733c2a148",
      "ee8f105b1d7742479663eceb2ad7b909",
      "7a736584b9704b36a849ad39046805df",
      "ce44c8896ec54825aa19f1068c82f5a8",
      "286eef7338c04b468a5aa0d7947e2dad",
      "50818e80e8664f75b81e2626d1732c8f",
      "50e2cd0062554ac3ade86556a4794fc4",
      "e92614519ab04c2dac1da67c09a8234a",
      "aa31e529f957420287dfbc8908b817de",
      "04a054f8353c42f0845994baf5f8c7ae",
      "72770cec3112462c8c4553eab5f797b7",
      "373e437bbf374e03932d31a4f923ae71",
      "b5b947f7ee8d46bcb9a378ce427cfe30",
      "5e1bb37a7ab04844a110de9afec78d38",
      "074d4db7152b4eee865b0da7931424f8",
      "ad84eecca5f34785bedc78549fc89f1e",
      "c3b6d7c4559c4605b577c557d8fd19a6",
      "0891771ea54f48a6b2a825cb26f72248",
      "7a2b2a28d8864dc4a3e16a4fbb589410",
      "4d9e15cc91874a2a9ba4a2b3ec45523f",
      "693a1327de424fcd8022509a49da4304",
      "10fa9a7512304a83b08cbb36eb46e520",
      "61ea423cfe2f48bb8b4478c099626a60",
      "daa71dfede8a460fad2cabf486091e81",
      "3b3d7ff2f32845498d98dbc70961031a",
      "103ae87c16ac40caa5f881629e72f04a",
      "f8d861f487124aee9685b522f43c0e64",
      "08413cf2615c4fd38294ab7276559f0e",
      "93231f2c440940108ba9dc87dce1c754",
      "1d13021c96e74c6c907707d3e181ec86",
      "f0232892bc4e41be991ce54034cc7abc",
      "c454441dd0794a1e8756bdd72b961519",
      "9eaf4e8079bd4305b4a695db020fdf12",
      "17fd0089d6164d81aa733be84398e54d",
      "11433cf089f047019243f73145aa3582",
      "a8a86036f08241a1b437da0c2b024243",
      "4aba135a18b64b9ca22bde3658b8a662",
      "a6495b1a1ad04ce1afb1e2f469f0e561",
      "dff371d78c5846b98e4cd353bdd7d68e",
      "60091d1c1dde40bdbd66388528d50db8",
      "74ff6a4b1ae241438e004c1e6c61b383",
      "6357afa15d4b45a0884530e83b33d7da",
      "fe0d69d63a0c48f185e90ced28939371",
      "1846f4cecaa34ee1a3050fdca05c19d2",
      "63c66aa8811e4940bd0e590f697ee0d4",
      "fcd8145f440a48c9bd9c0400465ab07d",
      "902856e8f30043a3b73722ff00cd1290",
      "30d03fda9db6496ba3ceb9679c4931ba",
      "79e9d9a4f5b0418eb3e2aa837d85780f",
      "567610dc9de94bc19165af68da5eb265",
      "db6a92356f844e348415d582d1ffb6d6",
      "54c7e48a210a4785925bb117c82fb66f",
      "bf00ba2c72d44ed9afad6ed1e04ba1ce",
      "c70ed24c6f104e8cb5f52ecb937a97dc",
      "3c2a0dc765d643c2b1b15cfe9346fe6f",
      "bef2574ab0514c509bea09170b221eb7",
      "14ed2bebe6804f8a9727c3dbc96a30aa",
      "e4fadcc9836642f5aaeda766d6aa5197",
      "5b8c462f3c2e4de1b14ed4de3a54ed49",
      "edcd0ba8c8424186a164bfb276095cf5",
      "edf53a46faec401da1f2d06a03f979f3",
      "fb24d2f21ef342a690f34fa39857772e",
      "4a2a273978074e17a8bcc365188a75ef",
      "5e54a659200d472e98b2190e965aee45",
      "b4b51345cd714db193598fe413b69ca2",
      "2f8865a29dd54cc59ac96ec2a7e89612",
      "f2e34d77b2644dbaacf98518dff660df",
      "1e1f1a51e36445ee8305def8f4d26733",
      "d6c61329d0524f99b707e7ddb0fb3963",
      "6e2c9affc0ce40aab66cc6fe4cc9029e",
      "329aa43ebb354435bae90ede556614b4",
      "cb2d4a153bc24592a10a5beb7dcb567e",
      "acf1419005004ec8b7a3ab0cb5b5be20",
      "625953dfc8bf4ed5884701e5795ac134",
      "e7697815ca02401fb448676ead592633",
      "a1e7f48a470c4e23a285870c745395a8",
      "ccc0c170d40d4acba18b5a2e34ec6130",
      "89f860310d1144b4a3a6aad99a2388bf",
      "454f9e86feab4312af6bdc456522ea14",
      "fc1d80decdbe43239fcb100d4ae29987",
      "da6904ae4c0d48c9804a8d34eeac2fa7",
      "7876fdc2d58b464a8832f4123cd0ac84",
      "4286ea80ab9b44d3bd74be460223cccd",
      "80b99cdefc0c4a19a03e5b60977c5252",
      "da5a5587ec6e4c9a81ea978502e5311b",
      "bd7419fdcc404a469731b9761a51b7c2",
      "56317622da284301b15654c89fb0496a",
      "26aa2d4e43e24287bfa9e457d68132ab",
      "3b988dc9683b4f11a8770fbdff99b4f7",
      "2a71ebeaf0784fbeb052a1468ff0ca4d",
      "3b5b15ec158a4f4c961399321dce9234",
      "1b57bc90e7f342e08b93d6704cd5971f",
      "8328e7bf86e34d60b480662c69cf4fbd",
      "1de42c1e24334732a15d4e35bfb5b3ad",
      "e42e9008c9f8419789c3da3728f8a8d0",
      "c84f99822c8f4dab84cceea70c2875d9",
      "57b5d6997ee742afb9f95f15108b105f",
      "657cb831767d4e0ba12b5618c6873a53",
      "90e56cfef4c64a79b67d8e280644a4c4",
      "d6b0c8873fed4355a3c3f8e1b1aad2dc",
      "37043db10f0848a09303744b39711a6e",
      "b6bd0e5ef0964405bf110326f8b4dd76",
      "045c33daa1aa4c0d99988fc8d30ddb1b",
      "0ec8d78fc4744e038766a6fd26ee86f3",
      "55ce376b87bc40688abd725562790b5a",
      "0379f30bd3954b9fbb9153adfeb2668f",
      "9206620cd48341a7b7c089462718736f",
      "b512e866fba74546aa8483cc9b5a1cf4",
      "06e6bd6db4ec4fa597b8b4af7ac727cb",
      "5c7c9cbedf85418ca94ea5c2e486a587",
      "fb49049931414cf3892c9b1ccf0c2bea",
      "8ea31eff9315444d9fcef28853d91127",
      "69ac9ce2a2ce4dd9bbef0c1baff0d211",
      "45419da06ff94bc898956c50e786570b",
      "c48f9e4435954f64a25fef8ad1a9f459",
      "9f7a5df5e5ec4151b63bc8f48891b46c",
      "b549783acd574ad1ae2006d61b810375",
      "cadcb886bb42450d9359c25b5c7c75bd",
      "2ac9ee865773429ca80db057e7337fbd",
      "6243ea4cd1db43edacae8a3f257abf8d",
      "a2709220d060416eaa11f23bb7475729",
      "6603c5d36b8e403696bd78c01f6e81ed",
      "33428b2324c94d66890d5abad6340086",
      "5dc36030f0154b36913b0570422aee6d",
      "2468442fd53445ce8143523b979cbb2e",
      "ab319482a6634fa0b6b5c29b08fd067f",
      "002df8624a9948dfa28234d313056317",
      "45f90258a1b54457a5ac3d71de942faf",
      "c5dc4d2c4f104d9f86cc640b6d9cdaca",
      "758249fa6bb74a008eb604c4dac514d5",
      "efe454c14cc34b4691bd247e8a2a4b91",
      "18326325bc15408b866905bdd876209b",
      "9c4bfc9f5932407293993cf19572e52c",
      "f957ba6a1f0f4035a41e2018263668a2",
      "0f62f523bc2f4919b9e4d4f324513b55",
      "c2ef33d1fae24ef489b39591004d46d1",
      "f6f5e0ca1b074a66a3d6987e9b740788"
     ]
    },
    "id": "4jD4c_LWW_Ef",
    "outputId": "d5bd9ed6-61ba-408d-9a78-b97d6550e79e"
   },
   "outputs": [],
   "source": [
    "with_lora_id_count = 0\n",
    "without_lora_id_count = 0\n",
    "\n",
    "print(\"Comparing language usage with and without LoRA on 20 samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, sample in enumerate(sample_dataset):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": sample[\"prompt\"][1][\"content\"]},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "\n",
    "    output_with_lora = model.fast_generate(\n",
    "        text,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=model.load_lora(\"grpo_lora\"),\n",
    "    )[0].outputs[0].text\n",
    "\n",
    "    output_without_lora = model.fast_generate(\n",
    "        text,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=None,\n",
    "    )[0].outputs[0].text\n",
    "\n",
    "    lang_with_lora = get_lang(output_with_lora)\n",
    "    lang_without_lora = get_lang(output_without_lora)\n",
    "\n",
    "    if lang_with_lora == 'id':\n",
    "        with_lora_id_count += 1\n",
    "    if lang_without_lora == 'id':\n",
    "        without_lora_id_count += 1\n",
    "\n",
    "    # Print progress every 5 samples\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Processed {i + 1}/20 samples...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS:\")\n",
    "print(f\"With LoRA - Indonesian responses: {with_lora_id_count}/20 ({with_lora_id_count/20*100:.1f}%)\")\n",
    "print(f\"Without LoRA - Indonesian responses: {without_lora_id_count}/20 ({without_lora_id_count/20*100:.1f}%)\")\n",
    "print(f\"Improvement: +{with_lora_id_count - without_lora_id_count} Indonesian responses with LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxaYP7QBW_Ej"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
