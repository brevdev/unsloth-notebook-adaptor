{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402b1796",
   "metadata": {},
   "source": [
    "# ü§ô Oute Tts (1B) on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Oute_TTS_(1B).ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Oute Tts (1B)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Oute_TTS_(1B).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CADH_fo_U0SB"
   },
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NPPXizTU0SD"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCDFTFmBU0SD"
   },
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJKbAJ9pU0SE"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JxnTqWNU0SE"
   },
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkWYsztAs9Ky"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
    "\n",
    "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "d2651d0c-6e13-4f41-ffec-50a90ec962d3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any for long context!\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-OuteTTS-1.0-1B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # Set to None for auto detection\n",
    "    load_in_4bit = False, # Set to True for 4bit which reduces memory\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "95059308-9e48-4a29-e171-146ed175b944"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"v_proj\",],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "\n",
    "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio**, but maintaining the correct structure is essential for optimal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Audio,Dataset\n",
    "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a6834d987f2b45c0ad1cb3b7391c0cbd",
      "18f05ab3b6f544069c7d382388568185",
      "0a574030ccd04bad830536da4d29aef2",
      "f0b17e37a6e14c2fbae30abc5d2ef735",
      "b33ab645567442308fa811b7164be240",
      "30e4205b0b3b43c48e40efe2ebb77fe9",
      "9af802eafcb54385a951d5d6fceb0021",
      "088353353cce4cfbb11aadd6c1f93121",
      "37d26f9def91453ba032c64b6890a6ab",
      "05bbdc3a68f04b9693f9deced6d9615c",
      "2b7d9cae19ff4645b124e9d1b85d5372",
      "58b884585d514bc386006389af5f2b7d",
      "3f4b39499e6b4412a737043b2d3a0b17",
      "1cf79b943ed54fa6a9fbb65a7fdad8d5",
      "4a24203b646c4d2ca14df09ac93dd0b9",
      "810be1af583647848c50c7970768fbbf",
      "14c5f884b2e349d48247ea5c6018f104",
      "96a60da35bd34f2a92a67d41f7df8b3f",
      "2fd528d17ae2492a870812245e386816",
      "35c7ce3e4c5d404f9e1ea8deae0c1bc9",
      "6c7885f9bf1944e9932753c59a9ee017",
      "43c08122c3354278a86543d74a0030f2",
      "0fd5f926f9fa4ac9a399b2afca11aa9c",
      "6f404cfeab58461ea7089baca0047f9e",
      "c1570b40db4d49c78599917af19ab073",
      "025f32dc68a541daa84c1fcb18325457",
      "5c84c17d11914dbfa4901f26be6a421f",
      "e545441bfacc4c0fadf75d2a0dacced3",
      "54ec9d87d125465c9e2a47169ad24e9d",
      "cbc87882adeb44bbb69177ca4c5bc427",
      "e17853f100304394aa034e55c171a2a5",
      "df4d9a9b9ffb448e8eace9af210b2795",
      "4f705ebf6ad74ff8af68ddca76b995b8",
      "03fd8e39f74a4bc1b9fa82c60377e0d0",
      "59a64fbc245a4028921423e53965a45f",
      "17e5c1623ed84c949c625617f52c085a",
      "4c1ff9eb693c469da82ad88472a7e056",
      "d3e77c689cd44c218e0b2dd77c3c618d",
      "aff2d360a0f44684bda296d52c3012a3",
      "1300f4e702694a7eb62d07caa7916071",
      "fb51aca25cd44a0396d8601d6ec0a8ff",
      "521bbe861a4c414db40526241efe5337",
      "52cad4bbdedc4d55a391b3d4a76237df",
      "959270534e614bbba51f1e2d968473c3"
     ]
    },
    "id": "zK94B-Pfioto",
    "outputId": "ede6a777-6944-4550-edc7-b90a91fc54c3"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "#@title Tokenization Function\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import tempfile\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "sys.path.append('OuteTTS')\n",
    "import os\n",
    "import dac\n",
    "# V3 Imports\n",
    "from outetts.version.v3.audio_processor import AudioProcessor\n",
    "from outetts.version.v3.prompt_processor import PromptProcessor\n",
    "from outetts.dac.interface import DacInterface\n",
    "from outetts.models.config import ModelConfig # Need a dummy config for AudioProcessor\n",
    "import whisper\n",
    "from outetts.utils.preprocessing import text_normalizations\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "class DataCreationV3:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_tokenizer_path: str,\n",
    "            whisper_model_name: str = \"turbo\",\n",
    "            device: str = None\n",
    "        ):\n",
    "\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Create a dummy ModelConfig mainly for device and paths needed by AudioProcessor/DacInterface\n",
    "        dummy_config = ModelConfig(\n",
    "            tokenizer_path=model_tokenizer_path,\n",
    "            device=self.device,\n",
    "            audio_codec_path=None # Let AudioProcessor use default DAC path\n",
    "        )\n",
    "        self.audio_processor = AudioProcessor(config=dummy_config)\n",
    "        self.prompt_processor = PromptProcessor(model_tokenizer_path)\n",
    "\n",
    "        print(f\"Loading Whisper model: {whisper_model_name} on {self.device}\")\n",
    "        self.whisper_model = whisper.load_model(whisper_model_name, device=self.device)\n",
    "        print(\"Whisper model loaded.\")\n",
    "\n",
    "    # Renamed and adapted from the previous version\n",
    "    def create_speaker_representation(self, audio_bytes: bytes, transcript: str):\n",
    "        \"\"\"\n",
    "        Creates a v3-compatible speaker dictionary using Whisper and AudioProcessor.\n",
    "        \"\"\"\n",
    "        if not audio_bytes or not transcript:\n",
    "             print(\"Missing audio bytes or transcript in create_speaker_representation.\")\n",
    "             return None\n",
    "\n",
    "        # Whisper needs a file path, so save bytes to a temporary file\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp_audio_file:\n",
    "                tmp_audio_file.write(audio_bytes)\n",
    "                tmp_audio_file.flush() # Ensure data is written\n",
    "\n",
    "                # 1. Get word timings using Whisper\n",
    "                whisper_result = self.whisper_model.transcribe(tmp_audio_file.name, word_timestamps=True)\n",
    "                # Use the provided transcript for consistency, but Whisper timings\n",
    "                normalized_transcript = text_normalizations(transcript)\n",
    "\n",
    "                words_with_timings = []\n",
    "                if whisper_result and 'segments' in whisper_result:\n",
    "                    for segment in whisper_result['segments']:\n",
    "                        if 'words' in segment:\n",
    "                            for word_info in segment['words']:\n",
    "                                # Use original word casing/punctuation from Whisper's output if needed,\n",
    "                                # but strip excess whitespace for consistency.\n",
    "                                cleaned_word = word_info['word'].strip()\n",
    "                                if cleaned_word: # Ignore empty strings\n",
    "                                    words_with_timings.append({\n",
    "                                        'word': cleaned_word,\n",
    "                                        'start': float(word_info['start']),\n",
    "                                        'end': float(word_info['end'])\n",
    "                                    })\n",
    "                else:\n",
    "                    print(f\"Whisper did not return segments/words for: {transcript[:50]}...\")\n",
    "                    return None # Indicate failure\n",
    "\n",
    "                if not words_with_timings:\n",
    "                    print(f\"No word timings extracted by Whisper for: {transcript[:50]}...\")\n",
    "                    return None\n",
    "\n",
    "                # Prepare data dict for AudioProcessor\n",
    "                speaker_data_dict = {\n",
    "                    \"audio\": {\"bytes\": audio_bytes},\n",
    "                    \"text\": normalized_transcript, # Use the potentially normalized transcript\n",
    "                    \"words\": words_with_timings\n",
    "                }\n",
    "\n",
    "                # 2. Use AudioProcessor to create the speaker representation\n",
    "                v3_speaker = self.audio_processor.create_speaker_from_dict(speaker_data_dict)\n",
    "                return v3_speaker\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during speaker creation (Whisper/AudioProcessor): {e}\")\n",
    "            return None # Indicate failure\n",
    "\n",
    "\n",
    "    # --- V3 Changes: run method is now a generator ---\n",
    "    def process_dataset(self, dataset: Dataset):\n",
    "        \"\"\"\n",
    "        Processes a Hugging Face Dataset object in memory and yields training prompts.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The Hugging Face dataset to process.\n",
    "                               Expected columns: 'text' (str) and 'audio' (dict with 'bytes').\n",
    "\n",
    "        Yields:\n",
    "            str: The processed training prompt string for each valid row.\n",
    "        \"\"\"\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "\n",
    "        # Iterate directly over the dataset\n",
    "        for i, item in enumerate(tqdm(dataset, desc=\"Processing Dataset\")):\n",
    "            try:\n",
    "                # --- Adapt to your dataset's column names ---\n",
    "                transcript = item.get('text')\n",
    "                audio_info = item.get('audio')\n",
    "                # --- End Adapt ---\n",
    "\n",
    "                if not transcript or not isinstance(transcript, str):\n",
    "                    print(f\"Row {i}: Skipping due to missing or invalid 'text' column.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                audio_array = audio_info['array']\n",
    "                buffer = io.BytesIO()\n",
    "                # Ensure array is float32 for common compatibility, adjust subtype if needed\n",
    "                sf.write(buffer, audio_array.astype(np.float32), audio_info['sampling_rate'], format='WAV', subtype='FLOAT')\n",
    "                buffer.seek(0)\n",
    "                audio_bytes = buffer.getvalue()\n",
    "\n",
    "                # Create speaker representation\n",
    "                speaker = self.create_speaker_representation(audio_bytes, transcript)\n",
    "\n",
    "                if speaker is None:\n",
    "                    print(f\"Row {i}: Failed to create speaker representation for text: {transcript[:50]}... Skipping.\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Get the V3 training prompt\n",
    "                prompt = self.prompt_processor.get_training_prompt(speaker)\n",
    "\n",
    "                processed_count += 1\n",
    "                yield prompt # Yield the processed prompt string\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                 print(\"Processing interrupted by user.\")\n",
    "                 break\n",
    "            except Exception as e:\n",
    "                print(f\"Row {i}: Unhandled error processing item: {e}\", exc_info=True)\n",
    "                skipped_count += 1\n",
    "                # Decide if you want to stop on errors or just skip\n",
    "                continue\n",
    "\n",
    "        print(f\"Dataset processing finished. Processed: {processed_count}, Skipped: {skipped_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    _MODEL_TOKENIZER_PATH = \"OuteAI/Llama-OuteTTS-1.0-1B\"\n",
    "    _WHISPER_MODEL = \"turbo\" # Or \"small.en\", \"medium.en\", \"large-v2\", etc.\n",
    "\n",
    "\n",
    "    data_processor = DataCreationV3(\n",
    "        model_tokenizer_path=_MODEL_TOKENIZER_PATH,\n",
    "        whisper_model_name=_WHISPER_MODEL\n",
    "    )\n",
    "\n",
    "    # Process the dataset and collect prompts (or process iteratively)\n",
    "    all_prompts = []\n",
    "    print(\"Starting dataset processing...\")\n",
    "    procced_dataset = data_processor.process_dataset(dataset)\n",
    "    for prompt in procced_dataset:\n",
    "        if prompt:\n",
    "             all_prompts.append({'text': prompt})\n",
    "    dataset = Dataset.from_list(all_prompts)\n",
    "    print(\"Moving Whisper model to CPU\")\n",
    "    data_processor.whisper_model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "37465088c410484e9fc136617fc939c8",
      "186cdf30cfb94ab29d043f2eeea628ec",
      "31637128f2e741ad93067d5cc295dc9f",
      "57575b5735eb4d2dbd0a89e925587964",
      "135f6dc595254051a21f9a3cb77366e0",
      "14e8d11447ea41aaa393e8ef3bdfec1c",
      "86876167afec4699aff52d32b2dbbc78",
      "3d891c6f331f411e81a7caf5341286e0",
      "42fc0a225a364a288c198835ffbcce92",
      "fb15eb3739d44a7bae430ccffa3ad65a",
      "40f1a38a3dea445795016161b087e92b"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "dc50f5ca-73df-4f1b-d663-1aaf4d6fc36d"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "c019e821-81a4-482d-8ab0-5414fd05159f"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "7821f3e3-13d5-4159-d091-e8a783cd44d3"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "2448745d-d65d-451e-9f1a-d9ba03d69c9d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apUdB40Ep6Ki"
   },
   "outputs": [],
   "source": [
    "input_text = \"Hey there my name is Elise, and I'm a speech generation model that can sound like a person.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "krYI8PrRJ6MX",
    "outputId": "b1162ee6-6f07-47cb-e2e0-09b603b39e77"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "#@title Run Inference\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "import torchaudio.transforms as T\n",
    "from transformers import LogitsProcessor\n",
    "import transformers.generation.utils as generation_utils\n",
    "from transformers import AutoModelForCausalLM\n",
    "import re\n",
    "FastModel.for_inference(model)\n",
    "\n",
    "def get_audio(tokens):\n",
    "        decoded_output = tokenizer.batch_decode(tokens, skip_special_tokens=False)[0]\n",
    "        c1 = list(map(int,re.findall(r\"<\\|c1_(\\d+)\\|>\", decoded_output)))\n",
    "        c2 = list(map(int,re.findall(r\"<\\|c2_(\\d+)\\|>\", decoded_output)))\n",
    "\n",
    "        t = min(len(c1), len(c2))\n",
    "        c1 = c1[:t]\n",
    "        c2 = c2[:t]\n",
    "        output = [c1,c2]\n",
    "        if not output:\n",
    "            print(\"No audio tokens found in the output\")\n",
    "            return None\n",
    "\n",
    "        return data_processor.audio_processor.audio_codec.decode(\n",
    "            torch.tensor([output], dtype=torch.int64).to(data_processor.audio_processor.audio_codec.device)\n",
    "        )\n",
    "\n",
    "class RepetitionPenaltyLogitsProcessorPatch(LogitsProcessor):\n",
    "    def __init__(self, penalty: float):\n",
    "        penalty_last_n = 64\n",
    "        print(\"üîÑ Using patched RepetitionPenaltyLogitsProcessor -> RepetitionPenaltyLogitsProcessorPatch | penalty_last_n: {penalty_last_n}\")\n",
    "        if penalty_last_n is not None:\n",
    "            if not isinstance(penalty_last_n, int) or penalty_last_n < 0:\n",
    "                raise ValueError(f\"`penalty_last_n` has to be a non-negative integer, but is {penalty_last_n}\")\n",
    "        if not isinstance(penalty, float) or penalty <= 0:\n",
    "            raise ValueError(f\"`penalty` has to be a positive float, but is {penalty}\")\n",
    "\n",
    "        self.penalty_last_n = penalty_last_n\n",
    "        self.penalty = penalty\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor`):\n",
    "                Indices of input sequence tokens in the vocabulary (shape `(batch_size, sequence_length)`).\n",
    "            scores (`torch.FloatTensor`):\n",
    "                Prediction scores of a language modeling head (shape `(batch_size, vocab_size)`).\n",
    "\n",
    "        Returns:\n",
    "            `torch.FloatTensor`: The modified prediction scores.\n",
    "        \"\"\"\n",
    "        # Check if penalties should be applied\n",
    "        if self.penalty_last_n == 0 or self.penalty == 1.0:\n",
    "            return scores\n",
    "\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        vocab_size = scores.shape[-1]\n",
    "\n",
    "        # Process each batch item independently\n",
    "        for b in range(batch_size):\n",
    "            # 1. Determine the penalty window\n",
    "            start_index = max(0, seq_len - self.penalty_last_n)\n",
    "            window_indices = input_ids[b, start_index:] # Shape: (window_len,)\n",
    "\n",
    "            if window_indices.numel() == 0: # Skip if window is empty\n",
    "                continue\n",
    "\n",
    "            # 2. Find unique tokens within the window\n",
    "            tokens_in_window = set(window_indices.tolist())\n",
    "\n",
    "            # 3. Apply repetition penalty to the scores for this batch item\n",
    "            for token_id in tokens_in_window:\n",
    "                if token_id >= vocab_size:\n",
    "                    continue\n",
    "\n",
    "                logit = scores[b, token_id]\n",
    "\n",
    "                if logit <= 0:\n",
    "                    logit *= self.penalty\n",
    "                else:\n",
    "                    logit /= self.penalty\n",
    "\n",
    "                # Update the score\n",
    "                scores[b, token_id] = logit\n",
    "\n",
    "        return scores\n",
    "\n",
    "generation_utils.RepetitionPenaltyLogitsProcessor = RepetitionPenaltyLogitsProcessorPatch\n",
    "AutoModelForCausalLM.generate = generation_utils.GenerationMixin.generate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    formated_text = \"<|text_start|>\"+input_text+\"<|text_end|>\"\n",
    "    prompt = \"\\n\".join([\n",
    "        \"<|im_start|>\",\n",
    "        formated_text,\n",
    "        \"<|audio_start|><|global_features_start|>\",\n",
    "    ])\n",
    "    with torch.inference_mode():\n",
    "        with torch.amp.autocast('cuda',dtype=model.dtype):\n",
    "          model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "          print(\"Generating token sequence...\")\n",
    "          generated_ids = model.generate(\n",
    "              **model_inputs,\n",
    "              temperature=0.4,\n",
    "              top_k=40,\n",
    "              top_p=0.9,\n",
    "              repetition_penalty=1.1,\n",
    "              min_p=0.05,\n",
    "              max_new_tokens=2048, # Limit generation length\n",
    "          )\n",
    "          print(\"Token sequence generated.\")\n",
    "\n",
    "\n",
    "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
    "    audio = get_audio(generated_ids)\n",
    "    audio = audio.cpu()\n",
    "    from IPython.display import Audio, display\n",
    "    display(Audio(audio.squeeze(0), rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "5727fb16-0597-4c50-c5be-284878cffc5a"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1YjyrnPU0ST"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
