{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacdaad9",
   "metadata": {},
   "source": [
    "# ü§ô gpt-oss-120b on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-120b.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">gpt-oss-120b</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">A100-80GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">80 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">reasoning, fine-tuning, large-model</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-120b.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "\n",
    "# Create cache directories with proper permissions\n",
    "for cache_dir in [cache_base, triton_cache, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o755, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "We're about to demonstrate the power of the new OpenAI GPT-OSS 120B model through a finetuning example. To use our `MXFP4` inference example, use this [notebook](https://github.com/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941,
     "referenced_widgets": [
      "e476384aafd84a629697368c0692a32a",
      "7c7fbcac548445dc83bc1594538268c0",
      "e5b4fd72fd7b41aea5636d2b836d36b0",
      "f801fcd916fd43ef9acf4d00bcc737e1",
      "ff3dae29702b4c18a050b05a5ce9a6b6",
      "fa1a111cd38b430787973d00aa7fb6b2",
      "686e576b6f6d4ddb9d8ffa062f458fd1",
      "373f40337db1437db5d7531a57f86166",
      "3a659f8cd91a40a6bebde3c94f814d62",
      "85b256bafce74ab6a8723a30567ebc69",
      "54d67ab93a794744a8dc445566e1d30c",
      "47070ece0ed6449b8f2ac33e7ca44285",
      "f9948a0fe7e4430ba3cb74e832ee2ed7",
      "df15c30a623841ffb7d4e7a8ce20c49e",
      "5994af8436784b52b6d25f70b90cad6e",
      "8c81352aedf447609bdbce5deeb6d02d",
      "5b4c911d2ebb4990b3d5a5667be97cbf",
      "83c3b4790d3c4c5895189d08113873a2",
      "f82d30156cff4493ab73aa387a32b489",
      "ca8f203f3a414654b6e5a368f3959fb5",
      "5be4cf77b7b54a879f25ce6d9767b665",
      "2340d6dbd52c4b9299c9fb094260c207",
      "3f19c43cc99b4be894ff43269d8fe36e",
      "0704104c994a4add9ce95247b3f7744e",
      "4665d620f52c4b1ea6e6d35d0cfae1b0",
      "7ee052712ffd4905a2be2ab745a61491",
      "5fc549a6a24443edb8d56c2a3948b663",
      "b7e818fa735c4e55b65ee127f5a8a887",
      "5b0dc22931a145d59226839064d00454",
      "02ccea32f25c4eb988d3d07868e58f52",
      "3c340a075b86452da64cce89eb2f80a3",
      "59aef35ff10d4487a0113ac9111a7991",
      "d0f3f21779094df587b40f3e3f1f1ed4",
      "b04832c189e84cdda36350bc56eec63b",
      "bef59ca3cdae42309df970a11ac89fd0",
      "b6a31b5206c44ebabb455629efd2e03a",
      "3098eba8f0ee4d33a83eda9da70ee5e9",
      "33572c6e26a64bff9aad038214c925d2",
      "6b74eb02a4a9494c9c00c6d581cd1db0",
      "46828ff2892d4daf9f273e8eace18005",
      "64a6f00c22f5453986f7d07a69f9a5be",
      "063a971ba233438fbc01a43f8b53a012",
      "315ac62960154b5fbd0f87b0c08117cd",
      "d3ad137910b84abf98f42f0872af8838",
      "a52adf0663cf4cd98a5015a6ef51bf78",
      "6e2a52471c984f17afcfba609fd7111e",
      "5e369073b77940a3a8763e1a49b54eb0",
      "67a78c45ed134e749d695a851f31b613",
      "294543c47ea341ffae0b697fdadba3e4",
      "56b49a0bfedb4e58ba79d85921585f11",
      "939c9be6a2a74983953c247f14d35637",
      "61af1b57865048e08591900e0105ffe9",
      "d01569aff7ed4635bdf7401f2369c59e",
      "18f8ccf0b8384f97ac1938304122ed15",
      "cdace945b9f746279ea592ad41dae6da",
      "3f6d768b768e41028129068f4349dc1d",
      "e9a1d88830a64a43a2afedd71e7195f3",
      "96978dad8c33400a97f80da088d6c600",
      "0eb557933e1547ef9586399ce8823aaf",
      "8acfff13179544abba02eaaa1fccebd6",
      "46469544879d4b57ad3b84031ab5fec0",
      "909ae8928d8948d598f8f984b6007d30",
      "9cf075e72e6b47208cc368067e230150",
      "b4cf0f09f82f43a68adcc8f0489c667e",
      "b965b16c81e24795a96ed4dd2c599db1",
      "52ecccbe22cd46c78ff761d8d19efa96",
      "70ece304f256423f879bf4144cfd103d",
      "1d0f98c16f2d49008f4a5d25fe8069f9",
      "8e657beb44a04641b21254b0a72b97a3",
      "6d8cd1c9aea84c8095245f167a116edb",
      "911414503a674244af263626ecdeb59b",
      "a7b541cdeaf8495fa8745bc64a230344",
      "19edc539c1fa410b9455d045636785ea",
      "1347606c5e0f4e06970387ba46d956ec",
      "924b14cecc814ef9a999257ee34ee4fb",
      "66fe6a5765b14ab9b342dfda60ebe536",
      "cea0f3fd5cc543c4ab76a1abdbf91723",
      "462a20d8867c462ab0c6ed7c446eb987",
      "d5928d5f784540e99b8702727abb6a95",
      "a0c64689d7af4ff89362a4dadd8ac51f",
      "934e8e130a3747dc9c3640bb588d4b41",
      "ab0bd61f633f4cc7ad3574da0e396f75",
      "8bd8f82f8b654b6589fc217f12796c02",
      "0c81c74019ae4ba58363b772ddf8e64b",
      "ad7953515229463d83322b4dafaf3534",
      "27374ac3c0704b509a234ac828a2b646",
      "6a688e202c394c5db62d64ab7967d251",
      "e5d10ea1162a4f1a9a924dc019b6271c",
      "5a71c76978b5464a9b85a1424ffbc289",
      "daaff163efea4aac9bc0f93ba2d1ea33",
      "cdb4ca76ccea47baa8489872d76af304",
      "668eba36d78b4f6ca447ac0be4333168",
      "5e3073aee29c45599ea54a580983df04",
      "7a64154dc38148928858b8f522ff1064",
      "793312235cc74640bad111dc26f39c66",
      "c81e19279255417787cd689b739d99c6",
      "a8cf73604fbd4532a7562dfbbee9d326",
      "badd0588bb4c4228bd894efd054ea774",
      "6caa1a62107f49b7b29a8d17adc4af5d",
      "5b5b3c67c77d46a1bc4abacb48daf9ab",
      "af974d5224224617a4ddc4f22ece8dc5",
      "cf76fdbcb1544d2c97c2c7170253be48",
      "90bc8cd224f04bd8b2b31f6662d3b1e6",
      "7ac1a6b989ba46df8e19aa56ea5f57e7",
      "0171b2eb6fa44ba79ee40ab8991a0833",
      "d29c3508ee834e6992966e4ea0a71a58",
      "0ec96f38296643d7aa6586fe031cfe14",
      "29e7349dc28441b6b01338842626420d",
      "de56fcb3c20940b9a7b81b38f5c0f120",
      "e719e495dce44b0481e239d6e6d56b31",
      "279874443989476c8f9b1f4a9d0898ff",
      "e740a0fa376e44278efd33e67f593c12",
      "aecf8be177a14d5286ecdeb896fd5c1e",
      "ab2fbb09542b4d5ab9bb5efa1964025d",
      "9b8ccba718cb4ddab460b8c60f6b58e9",
      "cfdf0d65bec44b20931674b31b1331d9",
      "41d7d9b3a5ef46baa12d5a28af6f5e58",
      "ff045d06267345d9a55f202271b439ce",
      "808a6582a22c4852a6616a67b1b6ecf8",
      "552ddd5cddc845a89484f9013d732be1",
      "b0d23fc17aa34eaba631d51d986956b2",
      "33fb95022d34419190ebde2f9460beaa",
      "21b761e840524e70a7ec2a91d61c934a",
      "29b27cd0a4ba4e5c86aedb4ffbf1cfc7",
      "b1bd640b5b0844ecb75b258ef6804a47",
      "c18e2ff3793543348a838563f8bb8438",
      "b670c6191a84480fa803a8733465a17d",
      "291837fde2f84068b62bf16eb6e94131",
      "9de79d172b82484c99021418969cc352",
      "5cdf20e13eb94223a55816a328237fb4",
      "2a4f5b891af84112a1468d339c7bee90",
      "d4ff3eb199a64d86bc1548df420a3dc7",
      "d8ceb8429808427bb4d7a83ca22cc360",
      "316f0a4b34474d73bb114a4d9ca1e465",
      "b4474df5ddc946c5ad3461505719b4e0",
      "15200719309c4794b80796faf3650d67",
      "bf6266984d44496fa8ac296486465c24",
      "38c2828a8c6a47c78f0b64314dceb7e7",
      "121c2b9877a442ff986ca9c15e04f406",
      "4edddeb7d3e24f7b899b95043aacdcee",
      "1a0ed626c0ec4a68abba16d7614cd503",
      "e0a8bcf3a8e24dfa8b25e07cd68b9a8a",
      "4e010f7c5f0c42b1b0db0e70a169ca07",
      "0525caac4c9340e49b14891733d60709",
      "cf43ec5f2f794b6795f181734643fef9",
      "a60a48fe7c2f4c858b2a260e861ff376",
      "3d020440331b46338529230fd1e5abd3",
      "73deb13d8e944a0ab53d6561b24607fd",
      "79057599c1d94dd8a8bd12b283172e5d",
      "f35ad79756f64f409bb1a1ab684a3c96",
      "593e7b2d04e1472f81bc9151c81ca1c2",
      "339c5c40db9e4b918bfb7746bffa126e",
      "6434d7daa7334d2c9d28870062d817e0",
      "5399929d0dd74145b9bb4cd925cfc200",
      "dd7d3975db7b40478e9ae81e7be1f1a8",
      "f3289085f0364e9fafa3c9fa6a0bd660",
      "c0e9f95a8fa545e09742a91d7c75e01d",
      "fe86585f14514cf588b1b0b3d28c0f76",
      "f0739b7c246f48639489ffde45a4d34b",
      "1f8952da489147e5a357b0b9a7e79dc5",
      "e409b3253b534c1bb5fa6b30819e210f",
      "47003b3c40a9422f856d565a51d77aad",
      "7a9e43bd870c46da8ea33c0006498897",
      "1a1e4707bbed4dd28eecf8b5c7d3ae99",
      "dc62b77f0054461a8ebdf23995beb8a1",
      "4ae544541d60407dbcd59a8c6d987666",
      "af46029fc63c4c548d39c064cfad976e",
      "a503ab293cde4eefa07b4c666a498bca",
      "fe3e178409b04cc1b8f977186e96ce1b",
      "351057428f16474ea7403710a35ab6e7",
      "cd3b27f7ee954f0583200e09638737e7",
      "c50e44fc3b9e470da146440be1a61ead",
      "2b2e81218d944cb9940a975c0c0ca65e",
      "85f76121a82a40b7aeb5159c3e5f6444",
      "bc7289d8ae174bf58e80181e9e2d90b3",
      "5c298d54e3784c1ca794dbafdb0aaee9",
      "6a0a5b750f1e419ca4ec11e2f2b4d25f",
      "112325caa07a4c2e90ea98fb8e3b64a9",
      "965299b7d04248e1907c7fb4f84a099b",
      "f7037116dc85419bac90ae5594f1c7a2",
      "0da54b50b50f499aa2806f37fe509744",
      "76db80bbab1049e0bf391f5f63e90515",
      "4a83299169544a48bc2ad02ab4a38837",
      "86d4b3d2f866455eba6053ec8f8cea56",
      "dd9d00723ee04483832c5a6e1f141453",
      "e50f3b62d5dc4627b8b09ba7c8444530",
      "fab06d1315714dcbb8fe01d9fbe1bdc3",
      "5ef19f10477747ce9f99cab1b456955b",
      "e1f4b3d4caa24e579164b62672f23905",
      "4bd2a082c9be48a2888fdcd243ed7342",
      "0130d039358d418b8d81af068e232fbb",
      "4001c25e4c654187b89ede2b10fc470e",
      "d58a164f639d4bef8bba14ec2fffb93e",
      "76ad85ab72af4c71bd6b8017080550c2",
      "be2ac118090641b3b43fe0b32c6eba06",
      "79ac793b70164238b6056be894c57a7a",
      "7252b5689ae64a27b466b9e6d1ca3a23",
      "04267020c5ae433c9541710ffcb0c63e",
      "a07ea59219024386aaebcb549ce5ea7e",
      "bafab3aa40e1497ca83a8da9a6df0e3d",
      "5ef93034263d43858fb1e09ff1fd0637",
      "edfee0d664064f6d8a59be0859f8971a",
      "869a58dc00e249bb8e1ae98470738687",
      "4e63f5baa9984066ad4a7fb30b2377a4",
      "140b995e70b144e2928c3b33cdc28b57",
      "2f838c3353b64020844709ced52f3f29",
      "265aa566e4d945d0b5cbc61ecbd26042",
      "c022acb6a706461d97fe6e58d3a6a102",
      "6150964708554bf1b6877cd9c1e9cceb",
      "a9208bb735a041ff9ab3662dd31c72bf",
      "908fca9309284f318ec41d918a59e879",
      "6d48162f746b4bb591cb9ffd5950f75d",
      "f0119a8eb82e40c18b54d00a2bceb833",
      "1aa56de0f40f490a921b74f13ecdbdd5",
      "4d5a0749dd6f4d398c7f3014af2c1e60",
      "49a7cb262add4b98895b8eabe7341314",
      "7f08fec0abb444209cca9f0b2166a7ec",
      "8adea05e8f454040b0cb6d108521d94c",
      "5eb726b2b1634f94a180faee53c581b5",
      "f2e178895b9c4b9082cbc97f87d1671c",
      "3a3844a204104034ae863c7b4fb4a455",
      "6592200b71cf4aa99dc63a8a3be68a82",
      "a24036b919944085bca2553cb44c21b9",
      "00c034a88ee54507b93ffe428fc70eff",
      "7a5c3be47b514169ae5a6bfd1e6ee391",
      "0e8cf736937242eea8077f0db6de4c0c",
      "0f2e3a3b1fa044989b5f45875b580a5c",
      "0b50bfde419b4550b1bbe6872a05c540",
      "8852dcf8583b404eb1245a4e29460f1c",
      "4d27cc429aa043eab0a664d410f3f4cf",
      "2a42121e29334a6b9bd4a825b4f62aec",
      "243b093abb5240f7802a8f8c3020dea1",
      "3f4db5f5d43d439fb8bddb79af982b8f",
      "eb54414facbc4ab488ff3bcf40c25a4f",
      "84bc55fc785546418ce9fe6eab5dd06b",
      "f0a1343ea03e4876abd791c2be628590",
      "ba640f22c7514726967c922c82492920",
      "49b9b92928c840978edb27926fdc2d99",
      "a39411c5463646e98841c3c651773706",
      "d13a22d9db4f43f8901a521da553124e",
      "e0433bb94217452d938f160a100a1836",
      "5d2d6883090641ea8e8c4b265a19c727",
      "1e52d77bf9054789ab324b8b10411286",
      "9cbd7dd5350341b2b382086c61bad64c",
      "83647ce225a44bdfbb6a71633dd8628b",
      "d4fa02bbd58b4a07b0b49a0258ed31e8",
      "aae364da6c364cf3bd3929d9e7894ebb",
      "d99b532998a64f79895deda607f94f26",
      "74a174443efd4f3ab8ec2fc6c7520ea0",
      "2c135a7f12754a759c091fc0033abcc3",
      "2dfd49cf55e64c28885e1d48a037bdc2",
      "d0ee2190bb5d41e588b01140645410fd",
      "1db80555964347a9afe7824876403e6b",
      "4c1013eb0be1482fb67959126967105f",
      "05eee6752d874e64be382dbdbd623f2d",
      "e2f859683b6d4b21b314619288287017",
      "83b7853c848443b4812318a210650b2d",
      "288caa60a9f64037861bc39c55c31fe2",
      "fe947d63189e4051a98e7d7bc185e3e8",
      "8eb0359c0deb4f409c39cd34c7d76bcd",
      "359b574b66d64be0a2e50d3ae2a566c0",
      "94d98d0a7bba4c349f99df185bca6b3b",
      "b22b01e3d9f24d9cbfb8db68d9e036fd",
      "2a87399fcffb4320afa906788c35d09e"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "8dde6890-9849-4d91-b428-023f87ccad55"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-120b\",\n",
    "    dtype = dtype, # None for auto detection\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVqtZVxxnVgf"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_LK81NRnVgg",
    "outputId": "35f483f6-97ea-4e5a-92f3-3c7c2c28fe14"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-sFShVvnVgg"
   },
   "source": [
    "### Reasoning Effort\n",
    "The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n",
    "\n",
    "----\n",
    "\n",
    "The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n",
    "\n",
    "* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n",
    "* **Medium**: A balance between performance and speed.\n",
    "* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxCi64FnnVgh",
    "outputId": "12e01164-b39e-4a6a-952c-04ed01c38eb5"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"low\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlAzq_RinVgh"
   },
   "source": [
    "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaPPyXN1nVgh",
    "outputId": "6e9a5a51-3ce6-44d7-ea20-695d1dbb5a14"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"medium\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0iuyJt7nVgh"
   },
   "source": [
    "Lastly we will test it using `reasoning_effort` to `high`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrjUXjN8nVgh",
    "outputId": "cd814064-b260-412b-c7a1-8e20d57c5e2c"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"high\", # **NEW!** Set reasoning effort to low, medium or high\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6BnnYcbnVgh"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91gfk9L3nVgh"
   },
   "source": [
    "The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "d4ca435428234bfc8b8238c60983e284",
      "35a3b262293c464d905f812665af3eca",
      "cc622b3f9e46479187c857beaf1c42f4",
      "fa3efee10a4a412fbc32d07398482178",
      "dc629fb6cc38438b94ba2179382efae9",
      "2d1a8c0c010a4d61abc43f0bd214cec1",
      "2f9feeed866745798a28357694026485",
      "52405e42de074ee3b75d63d8c0f61cd3",
      "1a12cce38ea64e51bea7d92591d31cb0",
      "25e71622f85b465a9e06902e0494fd99",
      "62ef2c09f0de4e31a27859edba428d46",
      "f06680ae7a8e4c79970e687cd0ea3bc5",
      "91b8e29b218744078d431d3052b75efa",
      "02e9296bb95d426aa2eec7f11cbb1ddd",
      "99496923368043dbaffc9d13b176802b",
      "4dca33c8cb3a41328f76edfb4accb075",
      "3ebb47eaf7394410b6586ff75428e649",
      "c9fe6941e0b54226ac5c501a7c8be857",
      "f1fe7d9fe4d6464a8b0d34ab60d90d96",
      "bcb3d8be2f7e4c2c86b500a0be079705",
      "3365292d85ca47a2803883274ee97cd6",
      "79b4c855b1dc47499ff56cba24d57e40",
      "fd993ea5a8374c7aa6f19c0102402441",
      "60fb3d4e24c3419cad61771233875f9f",
      "514570efd8474aeb95e928a6545d53cc",
      "68520eb1288744079dfc9ecf1b93ebca",
      "efbed3ae743949cdbc210ce3d2781bdf",
      "4b6c2e191da54f0ab1c8ebc6172e80b9",
      "fdf9541038b346e4835f27d33de82b6b",
      "b0b1430645db49f0bac90e8b448d2bed",
      "ff9bfc6aa2a340b4907efe3681996d0b",
      "a978cda800b940a68d54401a8091f04e",
      "3963f7c72cbe49aa99558d3be30dba8b"
     ]
    },
    "id": "62QfuPXBnVgi",
    "outputId": "01b3477c-cc2a-4850-8974-0bead8a390fb"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoVaL_i-nVgj"
   },
   "source": [
    "To format our dataset, we will apply our version of the GPT OSS prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2e178c26115a4f01b306643c23fdf09b",
      "f02bc4d6336a4119b031ca4b6ae156ef",
      "6438b6e52a664a5fa28efa817a169ae5",
      "7ca15b9a7e504e23a48f4ea84906bb91",
      "245eb86d49434e6cb95d472a6b8125f2",
      "a8850caed7024168a5cd6c9f42a444e0",
      "6fe6c24f99e543cf8722cecdfde6e00e",
      "3ef65b702a994d0e85e96abf38cf039b",
      "0edb286266bf400abf30d761489dfeb0",
      "6ef9e1aa0c5144c7a9635df2fbb607cd",
      "6ba50eec1e99499895a1f912b4b4148f"
     ]
    },
    "id": "FW-l11GBnVgj",
    "outputId": "8a7bd6fd-f145-47b8-9a9e-0f8e132905ff"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1EmcWNinVgj"
   },
   "source": [
    "Let's take a look at the dataset, and check what the 1st example shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNgkE5QxnVgj",
    "outputId": "6cea0a80-c8f7-49bf-ab08-c635b363e024"
   },
   "outputs": [],
   "source": [
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ3i-AMFnVgj"
   },
   "source": [
    "What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtdsxyl6nVgk"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c07cd40723364759a16352a37e781306",
      "9ff86be68eff41f2ab7d7a99119a5c57",
      "d8ce0ee48a464397aa75daafa1e946d8",
      "09b0d6f4ebd342509cdaf8cd9cbc29d2",
      "90e02b491b08455eb231e257186e733a",
      "e486f2a47ecf4ce8ace0222799595604",
      "d74d53a7afc5415187b9d951b60a80f0",
      "ccd32f234a6b4f6a8f7f6138e3732253",
      "664e72bc86b64a7387aaf1da5cb43f97",
      "53fa9095af9742b184c24854d48e40b4",
      "513aec1607ca4a7f91aeb8f443ebf5d0"
     ]
    },
    "id": "O-XZLeLYnVgk",
    "outputId": "63cfc034-1d25-40f5-c38b-7a58e6fb29f7"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF6so1JTnVgk",
    "outputId": "1725505f-92d9-4ee0-9859-68784ce21a24"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aFaejiSonVgk",
    "outputId": "cbb28f84-d5b8-468c-d8f8-2dbda3ed7017"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G3eBV3EnVgk",
    "outputId": "d5285731-bb45-485a-fe0a-a2c716b5b337"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuK0hVOsnVgk"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdVCmTuBnVgl",
    "outputId": "fddce451-62c4-41ae-e6d1-c950f6da097b"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"medium\",\n",
    ").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e1j8KRb4AwO"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** Currently finetunes can only be loaded via Unsloth in the meantime - we're working on vLLM and GGUF exporting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds7ByU7e4KF7"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gpt-oss-lora\")\n",
    "tokenizer.save_pretrained(\"gpt-oss-lora\")\n",
    "# model.push_to_hub(\"hf_username/gpt-oss-lora\", token = \"hf_...\") # Save to HF\n",
    "# tokenizer.push_to_hub(\"hf_username/gpt-oss-lora\", token = \"hf_...\") # Save to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELyXzRpl4hr0"
   },
   "source": [
    "To run the finetuned model, you can do the below after setting `if False` to `if True` in a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCMDSxvD4SKu",
    "outputId": "14783ae1-ebac-491a-eb2c-cf32fcc25477"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"gpt-oss-lora\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 1024,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"reasoning language: French\\n\\nYou are a helpful assistant that can solve mathematical problems.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict = True,\n",
    "    reasoning_effort = \"high\",\n",
    ").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(**inputs, max_new_tokens = 64, streamer = TextStreamer(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbYDlfGAdnrU"
   },
   "source": [
    "### Saving to float16 for VLLM or mxfp4\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16, `merged_4bit` for int4, and `mxfp4` for mxfp4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.\n",
    "\n",
    "**[NOTE]** Due to the disk limit in Colab, we have to save it using `mxfp4` format (4-bit) or we can use 'push_to_hub_merged' function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgeHm3BidoQy"
   },
   "outputs": [],
   "source": [
    "# Merge to mxfp 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"gpt-oss-finetune\", tokenizer, save_method = \"mxfp4\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"mxfp4\", token = \"\")\n",
    "\n",
    "# Merge and push to hub in 16bit\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/gpt-oss-finetune\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"gpt-oss-finetune\")\n",
    "    tokenizer.save_pretrained(\"gpt-oss-finetune\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/gpt-oss-finetune\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/gpt-oss-finetune\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVLJBz_6dtj1"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWfUhxQwduKe"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"gpt-oss-finetune\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"gpt-oss-finetune\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/gpt-oss-finetune\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"gpt-oss-finetune\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/gpt-oss-finetune\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/gpt-oss-finetune\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
