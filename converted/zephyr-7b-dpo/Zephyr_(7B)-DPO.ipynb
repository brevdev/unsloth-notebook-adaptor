{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f0c84f",
   "metadata": {},
   "source": [
    "# ü§ô Zephyr (7B) Dpo on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Zephyr (7B) Dpo</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "\n",
    "# Create cache directories with proper permissions\n",
    "for cache_dir in [cache_base, triton_cache, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o755, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8-BWi7MzkRz",
    "outputId": "0626186f-6e70-47b7-db59-b8e7591d0de4"
   },
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "4c7a9e6327fc4b839f1e2eee705e1574",
      "904fef1ac7404cc8a33e65ffe8938c8b",
      "f7562580501845e9a67ab9a8b4a96ba0",
      "96f9847627504ae7abe7461e4c0b3cff",
      "5165aa69bef148af9717d90cc5a870b1",
      "e6dfcf0456824556ad09fe7945674a14",
      "80568bca0c604eaaab2933fc76da2bf0",
      "079284acd5c6454eb03d8876a62e6f12",
      "1d917d774780421fbdca680009a706ac",
      "342cf2a9d6674b4d94fdbaf3b63b9b79",
      "16ace0251b954bc08a33eaf3b2a47216",
      "2cb8afe9c6e2441fa4e232146ecd08b4",
      "bc745fa3da114007af35ea15dbd7ab7d",
      "127ed424448840cea8b14fc9997b9cde",
      "803f5904c00d4cde8729226139e3c258",
      "4a273a69979b466dbf1c987552101fbf",
      "68893d4db1f74557932d8b24e0f72820",
      "3ccdc55cf6384695967eaf79325b1aef",
      "859bc87dfda746819298e213e4e0b067",
      "32478ec41d044773b70eba24e2ac0b43",
      "5aeb33ba799149c1ad6e580a2a1c0a51",
      "a2e31ba35dcf41c79c72dfd431da781f",
      "f7c089469828416aa89ade0cfa5a04cc",
      "3a52a2ec426b4fc2aaad23c7d8ce6d2b",
      "c298081665e844539b8d366325dbdab5",
      "65b1731e23364d4ea0bb292550d4a403",
      "5f6fd4c19b34458aa8f2019a25bfd605",
      "33dda465ac2f42fb8846171a974193ac",
      "bf6bc9099d1a475884be6f15115f01ad",
      "cdc411110d1b405ea12342b57fd8de30",
      "84d445570b31490fb658b08f819dd6a7",
      "ceb3830da80146da8fbe6423d25fd8a3",
      "d58599afce36484c8508c16beba13e36",
      "d43eea508a4d496e9460f01931659b52",
      "b146c0bbd15644678198977d63edebb7",
      "aa0323eef053447b9836931535984912",
      "effec7acec3d4b5384bcf481e3263b6c",
      "03ec8d28c0f34116b42e4e0dc65e3dd3",
      "9fd5df9115cf4deb8a36daf4a654bfc7",
      "cd6e07c3984a4e0e920d87c6ebd6ad1c",
      "b0831169530b402cac31b1e60484fbb2",
      "ccabf636e15e471cb82bb431b5ab4b5c",
      "e4120afe895742848cf756fd58cb42f8",
      "fe6eabd38a614113b36bafd1f8ce6920",
      "9b12628a46c546aebe2c749085ba2e61",
      "63d48b48f4ce449e85a55477df0b6967",
      "068c416eaaed476eacffb8f2f343e08f",
      "9fd3fe23f14b4403a18ec7d73b64dc6a",
      "cf639313cba7442893fdf880d368616d",
      "4bd0cf4d31604194a7761ea0345cc0a4",
      "783ad9a7097e4fcdb768c8ecb1f21227",
      "2557c3090a8d4760ae844fb518e1b555",
      "b508872df72d45beb26a625c7499cc8b",
      "cbc6fe3cc12a4d79ba08b67f23e5ce01",
      "d0d340fa13a54bcf9f1130cc2fc09cc8",
      "5d7fce052f8e4937adaaa670be8c1ae4",
      "77939b191e82425da0a5908fdbca2a2d",
      "30dc6079591b4908a57f5e285ebeda68",
      "960bd84fc3fe4d8eb7e79ecba0f3edf6",
      "bb7c7c1866af4641bc07d1ff0a5cd069",
      "0dc6aefaa24043768d656f5a61d79545",
      "d27056ab29e04c448fc62f9f18cbb82d",
      "926319902fe84c5095e0b5bd4b3e8187",
      "9f660e1b7b9c4ae7922d51353783acf9",
      "82d00942a39e465c8a6f869ef97cc250",
      "b9e2b829291247619a7b37c9f544d395",
      "3f6ccae685e34d05be9fde5538dc48cf",
      "3df8638084104cb7a0530a96ec6201af",
      "06d1fd85a45541609d11ec7f0bd3b444",
      "a9d05a8c3aaf4fe0a2f061c3eeb48b44",
      "8095a44a62804d6d86c24d62b9e1a1c1",
      "f61b85b63d5147db9f1040f30f4a3034",
      "e152d696872242ab8e0bc00ce9b50142",
      "623e912cfe404e69a64992fbb1c1bc47",
      "69f9e4bf820047658699d55d84d682c0",
      "a8306ce933304694aba188495743c2f4",
      "1cd708f679de4d319af26797aecca94d"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "f04a3ea4-0d0f-424d-8265-0fb99fd17ef3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/zephyr-sft-bnb-4bit\", # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AqkY_wHdKyOl"
   },
   "outputs": [],
   "source": [
    "# @title Alignment Handbook utils\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n",
    "from datasets.builder import DatasetGenerationError\n",
    "\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "    task: Literal[\"sft\", \"generation\", \"rm\", \"dpo\"] = \"sft\",\n",
    "    assistant_prefix=\"<|assistant|>\\n\",\n",
    "):\n",
    "    def _strip_prefix(s, pattern):\n",
    "        # Use re.escape to escape any special characters in the pattern\n",
    "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
    "\n",
    "    if task in [\"sft\", \"generation\"]:\n",
    "        messages = example[\"messages\"]\n",
    "        # We add an empty system message if there is none\n",
    "        if messages[0][\"role\"] != \"system\":\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "        example[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True if task == \"generation\" else False,\n",
    "        )\n",
    "    elif task == \"rm\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            chosen_messages = example[\"chosen\"]\n",
    "            rejected_messages = example[\"rejected\"]\n",
    "            # We add an empty system message if there is none\n",
    "            if chosen_messages[0][\"role\"] != \"system\":\n",
    "                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            if rejected_messages[0][\"role\"] != \"system\":\n",
    "                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "                chosen_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "                rejected_messages, tokenize=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    elif task == \"dpo\":\n",
    "        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n",
    "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
    "            prompt_messages = [\n",
    "                [msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]\n",
    "            ]\n",
    "            # Insert system message\n",
    "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "            else:\n",
    "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
    "            # TODO: handle case where chosen/rejected also have system messages\n",
    "            chosen_messages = example[\"chosen\"][1:]\n",
    "            rejected_messages = example[\"rejected\"][1:]\n",
    "            example[\"text_chosen\"] = tokenizer.apply_chat_template(\n",
    "                chosen_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_rejected\"] = tokenizer.apply_chat_template(\n",
    "                rejected_messages, tokenize=False\n",
    "            )\n",
    "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
    "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            example[\"text_chosen\"] = _strip_prefix(\n",
    "                example[\"text_chosen\"], assistant_prefix\n",
    "            )\n",
    "            example[\"text_rejected\"] = _strip_prefix(\n",
    "                example[\"text_rejected\"], assistant_prefix\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo']}\"\n",
    "        )\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    data_config: dict,\n",
    "    splits: List[str] = [\"train\", \"test\"],\n",
    "    shuffle: bool = True,\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads one or more datasets with varying training set proportions.\n",
    "\n",
    "    Args:\n",
    "        data_config (`DataArguments` or `dict`):\n",
    "            Dataset configuration and split proportions.\n",
    "        splits (`List[str]`, *optional*, defaults to `['train', 'test']`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "\n",
    "    Returns\n",
    "        [`DatasetDict`]: The dataset dictionary containing the loaded datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(data_config) is dict:\n",
    "        # Structure of the input is:\n",
    "        #     dataset_mixer = {\n",
    "        #             \"dataset1\": 0.5,\n",
    "        #             \"dataset1\": 0.3,\n",
    "        #             \"dataset1\": 0.2,\n",
    "        #         }\n",
    "        dataset_mixer = data_config\n",
    "    else:\n",
    "        raise ValueError(f\"Data config {data_config} not recognized.\")\n",
    "\n",
    "    raw_datasets = mix_datasets(dataset_mixer, splits=splits, shuffle=shuffle)\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def mix_datasets(\n",
    "    dataset_mixer: dict, splits: Optional[List[str]] = None, shuffle=True\n",
    ") -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Loads and mixes datasets according to proportions specified in `dataset_mixer`.\n",
    "\n",
    "    Args:\n",
    "        dataset_mixer (`dict`):\n",
    "            Dictionary containing the dataset names and their training proportions. By default, all test proportions are 1.\n",
    "        splits (Optional[List[str]], *optional*, defaults to `None`):\n",
    "            Dataset splits to load and mix. Assumes the splits exist in all datasets and have a `train_` or `test_` prefix.\n",
    "        shuffle (`bool`, *optional*, defaults to `True`):\n",
    "            Whether to shuffle the training and testing/validation data.\n",
    "    \"\"\"\n",
    "    raw_datasets = DatasetDict()\n",
    "    raw_train_datasets = []\n",
    "    raw_val_datasets = []\n",
    "    fracs = []\n",
    "    for ds, frac in dataset_mixer.items():\n",
    "        fracs.append(frac)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                # Try first if dataset on a Hub repo\n",
    "                dataset = load_dataset(ds, split=split)\n",
    "            except DatasetGenerationError:\n",
    "                # If not, check local dataset\n",
    "                dataset = load_from_disk(os.path.join(ds, split))\n",
    "\n",
    "            if \"train\" in split:\n",
    "                raw_train_datasets.append(dataset)\n",
    "            elif \"test\" in split:\n",
    "                raw_val_datasets.append(dataset)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Split type {split} not recognized as one of test or train.\"\n",
    "                )\n",
    "\n",
    "    if any(frac < 0 for frac in fracs):\n",
    "        raise ValueError(\"Dataset fractions cannot be negative.\")\n",
    "\n",
    "    if len(raw_train_datasets) > 0:\n",
    "        train_subsets = []\n",
    "        for dataset, frac in zip(raw_train_datasets, fracs):\n",
    "            train_subset = dataset.select(range(int(frac * len(dataset))))\n",
    "            train_subsets.append(train_subset)\n",
    "        if shuffle:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n",
    "        else:\n",
    "            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n",
    "    # No subsampling for test datasets to enable fair comparison across models\n",
    "    if len(raw_val_datasets) > 0:\n",
    "        if shuffle:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(\n",
    "                seed=42\n",
    "            )\n",
    "        else:\n",
    "            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n",
    "\n",
    "    if len(raw_datasets) == 0:\n",
    "        raise ValueError(\n",
    "            f\"Dataset {dataset_mixer} not recognized with split {split}. Check the dataset has been correctly formatted.\"\n",
    "        )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ-Cp2V6kDcr"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We follow Huggingface's [Alignment Handbook](https://github.com/huggingface/alignment-handbook) for [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) and use the [Ultra Feedback dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized), and sample 0.5% of it to speed things up. You can sample the full dataset for a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553,
     "referenced_widgets": [
      "5e875e8a442a41e8afef4ad494ef6897",
      "7adc769f384346b3a5835c73542a2d62",
      "fccd2f68b05d40c9b359133fbf383b1c",
      "2a197f7c6c4746e1a7fded1e0539bf29",
      "37a2eb8b91ed42a1be411ecb9079a082",
      "833097685a694d1e9cd5aa016b6b7d73",
      "4e48c220077d4546a51c02df9397a084",
      "f6200f86a4084c5f978f62481100b7a3",
      "530cd47150094e7fb925eb24ec071b5e",
      "9f6197721b90446091bf8b97de656d4b",
      "35edd118e5e54d19b87aa7e966e3c9fe",
      "3b258a09c7e0462e9d9fefa6746ebb9f",
      "ade70be3dc49443581a295e89faa65c6",
      "ad03a2ff7f974fa3a0f1be2dee56c9a0",
      "727d7d710957497c8bb91d52a04a00f3",
      "347ae8c1baf54004953d0eb9325f39f8",
      "6b419485aabe49c6bed15b5bf1316e43",
      "9eabdceae6484ba7b543ed1fa4f65141",
      "6cce73a8bdda4852968c807354a6d212",
      "7953ace8fec34f358506307a3efb208d",
      "ba6f8296f866419384de64f6e2cc727e",
      "d3fcca8f0d8c41a3b78bd2470542e950",
      "243999bf48174e72ae981effb01117ac",
      "77e6abd454c94559825201f34d5de044",
      "7522d496e9dd41828a4daddb9dd37883",
      "13b208926d4a4bae854e057bc9913100",
      "78ab252184ac46eba3e82c076f6b914a",
      "80308f7c18b244a698614fa86bc4c564",
      "9473389066664ed4aa2089c78adbb20d",
      "4a6807afa14049c781c7549467b85e1c",
      "4cc5ba58138d47129b776b04bc2d6646",
      "90b49ecfaf4a44409b0b405271c9797c",
      "9032335e102f465ea2e1adcc47462a8c",
      "6730cd77f7fa416b97f20fc5f5660488",
      "5a40fa44b76f44f88a24835c1bf00e56",
      "57f9a1fd43024e8bbe7c0df0867c8e12",
      "8f6c6f07bd254516ae4af2f80d2492f2",
      "5c28f5682c37476384f6a985d42e8360",
      "900f8248c6074ea5a93ef9372da4e958",
      "39a4f06a16f149789507232e4e28acb6",
      "c558b2bb11274910888b08138d9ee195",
      "3635323c2caf4f7ab2034306cdac61cb",
      "d855ad7e63ae45089832de406a3b00e6",
      "bf25365768a0441e8e4121ea1966971d",
      "757089cbb90f4d538e4a98a9ecee7dc6",
      "d28fb551bdac4910a04bb39b500b9228",
      "82cc0b43ea6d4ff4bddce1a1536fef62",
      "379878ecc7e44b448425c4155e106477",
      "5f8ada6bd56b4d79861a8f2bc36b7785",
      "3456b1a43ca742e3bd5bcf695a7b548e",
      "56da48c3277342b3ba6841d25a387202",
      "007871aa8b7f4de7bb0f96ebe08e5283",
      "0611164c2c6f4778adcbda713521155b",
      "a4199d34e668433a98b0fed23ab84541",
      "2b850e92569c4236a683b9df2de49775",
      "97a28a558b2a4ef997fb35547ef2f51d",
      "3760dc92a7b64410865257af9f95782c",
      "2189b92750854b1f9eaa70dc23d16c55",
      "37944b0ab46c4de7acb5e06b91f5da52",
      "461aeb76622449b6a0bb7a794ea16ced",
      "e98afd2ed7b44211932dc55a2a0c3e3a",
      "dfb40aa450f844e28319893415573906",
      "841f91867ff5489d92dfb1bbbe76ca70",
      "4631d92c2f3a44589478a981a4143836",
      "e6c183829cfb45e389348c0d5a1d943d",
      "feefb2db48a84e7f9808accd7fd68df7",
      "e10b53d12a134bc6a674bc568e1ab7bf",
      "80cbef017c5041aab8fe651fe5e5f0a0",
      "53d29032d6a64f958dffaced43b46f58",
      "1093b380f208425aadf0085c0aebed80",
      "4032d6814ad44a2f86c2f3b9cbb7afad",
      "35b0f9e4188f4c22bd2ec62fb8c7e228",
      "3243cc3a35e742048fb4d62f1c448752",
      "df9f065bf72b44f289a57855e5f38620",
      "7303bc3b53094dcd8fd4966f18622f20",
      "0003c40814494b279a4f21e422723caa",
      "124aae7bb4a94223b8ae7f6b8d20961a",
      "65ea6829bd484590a9e99f07bbadf52a",
      "404791f1b4034ed9ad68752480b55731",
      "84610b3317ec4cad9bfe284ab48f3a0f",
      "2ef3a65aadc24bd085f95acf4c3de5f5",
      "a818ce650c9c43deb8f674dd9e08c1ef",
      "3d786f96440a4a76b1b09f80c3d1c0e9",
      "ee568de931f1427f9fb252f24a6bdb01",
      "a77440b9604140a78d99cca3e9845a1f",
      "56021b483de347999853740bce5b4066",
      "c1e6987ebc5f40ce90d4c94056c74333",
      "7631d285c62c4c848930eef7390ef58b",
      "03b41c21d7634543a1b29eaadbf1cd40",
      "ff122787d9174a718b5d3223f53edf22",
      "70896fbd2fab4db29322fc78e2cd84c5",
      "4673095a06fc47b581f46754117e83fe",
      "435cd70149224c18aeff12ad5d2926b8",
      "1aa72a1d1d0d4ce4b36b10c8ecf021a8",
      "a004e9208a5f441e9e4e60cc1f6d7aa9",
      "1407812065e84df28e3cbfce5bc26fc9",
      "a86aecff032345c5a3f176851052bcf0",
      "cc8d776ab3b4482ba8a2cab6bce3f310",
      "5052e40d06ed4eefa2f6df4be951aa3d",
      "0c7cd9bca9994ee4abc817035db4f115",
      "7f622c1b4fef4ebb8c290cfc164f3a95",
      "e05e528fd80548c1b7147d1756d980c9",
      "073e4cf5a7f944048d507076589bcf7e",
      "1794dcbd585747adb0530f76ec4d5be8",
      "31f8708ee9444a82923272f09da1fe91",
      "2fad7fdcdb574f75bf994a363f8c31c9",
      "59086cae36714c9bb971cf48e9d3f78b",
      "0715885acccf4440aed4955f4aaff0f2",
      "6adc4e60532f4558ade289ad526aa87d",
      "25634e71ebbf4a41baf59a8cb8540bf3",
      "1d8bd9a5ec064157a2099a23621ccdad",
      "16ca05cd0a404e36b737bae31235fac6",
      "f4065d3723d649bfa930624fca67ddd9",
      "92a0e4161e0a4d7e88aaf764f4df92b7",
      "6deaa7e61cf443e99dce5869e1281c0c",
      "05d95e4de7b84da282818e23c8c22345",
      "04ac7654040b4002a1f359c4cd000cc5",
      "4fd685420f4e41edb9d9c97e6ac89585",
      "1a319e2196e3465989cbf69e736fbd3e",
      "a70c5ea79cba4f4bac16c1bebe5aa0b5",
      "dd2a3af3a6144a9ba59fb130090c0283",
      "baa1b968a1de4d029730f6a5a2301101",
      "4d080de8a7dc4809abafd654f1f62691",
      "16708105f96343c482ea0df18d42cbf3",
      "47fb8b7a47eb47179ccb0e2e1b313fbd",
      "f9fb3e9d512944f295b81b249119d025",
      "ee9b1c6c792143da997c0b563a95ffce",
      "7bc84a52af5d4b1398b0ef212155e857",
      "4ad4cbf343684ded9a8fc5f8274a261c",
      "e5d62477070e475c93ededb4de412063",
      "9e26503e85234dda982f4a731dacabbf",
      "c0e6950f66d84e5091051fbe173b2b3f",
      "b6c12557e5814d4db2ca39372d44a167",
      "59d72ddb22ee4efeb86af48c83dd8110",
      "ae4e761cd74642788667953acf5fd0aa",
      "40904407149e4ceea2f3bdf769aba0d1",
      "da08e39e0cbd43b68dd1058954a3d93f",
      "f1b1def618354651b0c7662b055d7cb2",
      "02672649160d44c3a21f0dbb4f1142c2",
      "c19e821b4a7f458085202f1d2bd148dc",
      "86215ff3b19748859771c04a654a92a6",
      "853df198b21147f390d68af8043610b1",
      "86af0415540c41658b4f125e3f372c52",
      "e6ed1837c16b47d3913f123a7c09d833",
      "f9af1d0e95b846e7923b0fbf48df969d",
      "7c58d359b4934b8c8b99b9aa13abb661",
      "d22351f5e6844faca41027ee44ead93e",
      "162e0a2a77b743868d3e6c53f2220b14",
      "22f984300d814049bb2cfa3c68c6461a",
      "1dc30290e4c4478695246cef4b785563",
      "da199afde5ed47cdad0801ce45cf34a7",
      "76cc2729929048d0a9ea919735fea53e",
      "2ab8ed5174004d60ba0ecfc7505e3c54",
      "061f919ac1c74f078b5c909ff889e894",
      "e63c3a8c021b4d30b0c93b99a7dba155",
      "45cc7c0a9c664353ba11a5049c0353b6",
      "514e5d566f734683bec2319b987c16dd",
      "21687939a40a491395c37bec223c37fa",
      "440101772f084da5b3e5724fb28c52d1",
      "32f7c34cd24c4d83976c96ee0cdc05cf",
      "57e3acfbf42c41d2b2a3774247e046b6",
      "bd508aa803f94073af7a91332e738668",
      "ddd131d700884af596f2b77c921f48c5",
      "8c785cb20818402a829e54ff3467e03f",
      "3d17ac67bb8a4fc9b4709f89de589fcd"
     ]
    },
    "id": "r6bUnxe6N3pf",
    "outputId": "ff62990d-3e46-4a6d-826f-d4533b596a98"
   },
   "outputs": [],
   "source": [
    "raw_datasets = get_datasets(\n",
    "    {\"HuggingFaceH4/ultrafeedback_binarized\" : 0.005}, # 0.5% sampled\n",
    "    splits = [\"train_prefs\", \"test_prefs\"],\n",
    ")\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs = {\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n",
    "    num_proc = 12,\n",
    "    remove_columns = column_names,\n",
    "    desc = \"Formatting comparisons with prompt template\",\n",
    ")\n",
    "\n",
    "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
    "for split in [\"train\", \"test\"]:\n",
    "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AxUmeAGkjDd"
   },
   "source": [
    "We shall print a random item from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oF63zQqNlNJC",
    "outputId": "5e4e2858-9ac2-4023-f554-13bfeec367a8"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "row = raw_datasets[\"train\"][8]\n",
    "pprint.pprint(row[\"prompt\"])\n",
    "pprint.pprint(row[\"chosen\"])\n",
    "pprint.pprint(row[\"rejected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86wyNoeMj-Ph"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "677aef23-d549-44d3-aace-9efda302d1e8"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kyd_iyz7DUM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the DPO model\n",
    "Now let's train our model. We do 3 epochs on 0.5% of the dataset to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-2BFpDWzo1K"
   },
   "outputs": [],
   "source": [
    "# One must patch the DPO Trainer first!\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "1410214ec6fd4e19a200b5e03464ceee",
      "0ba04e31d310422f9efaef634afb1296",
      "6ac7582965d341c383c806ae3a8b513a",
      "971371cfb1bd447e94f9f2e38f9f50c8",
      "3d4dbe5814d24b7dae1a7462a33e7f40",
      "6b4743fee43b40638d8f2df36fa9fed9",
      "77f706dc09eb43d7aa4522f8a0b36142",
      "175c7bb9e8c94b8daa3287dd773b6837",
      "a10418a202e14a96893e24267bc194c9",
      "8138d9f175df4a28b2e58e9c852cabb8",
      "43b39a8abcb94847bc21ec16fa373ed9"
     ]
    },
    "id": "QtoqUw80QDV0",
    "outputId": "40764ccc-6502-4be1-fb4f-e386f5460147"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 5e-6,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = raw_datasets[\"train\"],\n",
    "    # eval_dataset = raw_datasets[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length = 512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EWGFqAo5Q2me",
    "outputId": "1bcba65a-8253-4a49-a180-fde8e26ad4b5"
   },
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
