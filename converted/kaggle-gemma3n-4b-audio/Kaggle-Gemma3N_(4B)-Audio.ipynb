{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0485e662",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Gemma3N (4B) Audio on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3N_(4B)-Audio.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Gemma3N (4B) Audio</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3N_(4B)-Audio.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pip3-autoremove'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'unsloth'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers==4.56.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps trl==0.22.2'])\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps --upgrade timm # Only for Gemma 3N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212,
     "referenced_widgets": [
      "6e6ab95a75cb458387c4586bdb27cb33",
      "43945fb5a39d4c1d8a898e52f7c44cf7",
      "cf23f433919c4575a8daa80a584ce0e2",
      "eae6e611e7394dafbb6a96897d8c5d06",
      "a25180e7e1ae45e58029f6d808697493",
      "dd596d7ce92847178805247ed2ea2902",
      "56256ba8e73c48898b147d2b91f4fd85",
      "6dbca67a4bcf491fa0cdc07f10cca9c2",
      "ecbee91d79834a49ab0cb3e33522d8d2",
      "2d0675287f864fbdac1ca1439bc50077",
      "8ef02bdc9627427d83a6324cb59c445e"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T12:16:21.155888Z",
     "iopub.status.busy": "2025-07-20T12:16:21.155077Z",
     "iopub.status.idle": "2025-07-20T12:17:36.514669Z",
     "shell.execute_reply": "2025-07-20T12:17:36.513831Z",
     "shell.execute_reply.started": "2025-07-20T12:16:21.155861Z"
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "9adc6b1c-8362-48be-f445-a35136904322"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, processor = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixr4dyTHVIcI"
   },
   "source": [
    "# Gemma 3N can process Text, Vision and Audio!\n",
    "\n",
    "Let's first experience how Gemma 3N can handle multimodal inputs. We use Gemma 3N's recommended settings of `temperature = 1.0, top_p = 0.95, top_k = 64` but for this example we use `do_sample=False` for ASR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:46:24.659070Z",
     "iopub.status.busy": "2025-07-20T12:46:24.658306Z",
     "iopub.status.idle": "2025-07-20T12:46:24.664411Z",
     "shell.execute_reply": "2025-07-20T12:46:24.663791Z",
     "shell.execute_reply.started": "2025-07-20T12:46:24.659043Z"
    },
    "id": "UsfUPU-oVQYu"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    _ = model.generate(\n",
    "        **processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\"),\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        do_sample=False,\n",
    "        streamer = TextStreamer(processor, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZrmFRZpZtGf"
   },
   "source": [
    "<h3>Let's Evaluate Gemma 3N Baseline Performance on German Transcription</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:46:26.982493Z",
     "iopub.status.busy": "2025-07-20T12:46:26.982188Z",
     "iopub.status.idle": "2025-07-20T12:46:28.166576Z",
     "shell.execute_reply": "2025-07-20T12:46:28.165889Z",
     "shell.execute_reply.started": "2025-07-20T12:46:26.982472Z"
    },
    "id": "GHBGeJhYcorh"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Audio,concatenate_datasets\n",
    "\n",
    "dataset = load_dataset(\"kadirnar/Emilia-DE-B000000\", split=\"train\")\n",
    "\n",
    "# Select a single audio sample to reserve for testing.\n",
    "# This index is chosen from the full dataset before we create the smaller training split.\n",
    "test_audio = dataset[7546]\n",
    "\n",
    "dataset = dataset.select(range(3000))\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:46:38.870979Z",
     "iopub.status.busy": "2025-07-20T12:46:38.870740Z",
     "iopub.status.idle": "2025-07-20T12:46:38.890720Z",
     "shell.execute_reply": "2025-07-20T12:46:38.890034Z",
     "shell.execute_reply.started": "2025-07-20T12:46:38.870960Z"
    },
    "id": "7IF_zeqScorh",
    "outputId": "52003ba8-7e84-4c49-dd21-efccb8a13b06"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "print(test_audio['text'])\n",
    "Audio(test_audio['audio']['array'],rate=test_audio['audio']['sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T12:18:05.249062Z",
     "iopub.status.busy": "2025-07-20T12:18:05.248355Z",
     "iopub.status.idle": "2025-07-20T12:18:37.319606Z",
     "shell.execute_reply": "2025-07-20T12:18:37.318802Z",
     "shell.execute_reply.started": "2025-07-20T12:18:05.249040Z"
    },
    "id": "BJr_D4O9Z2Zh",
    "outputId": "4a2e511a-0afc-4528-8832-bd8dd6aaea03"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are an assistant that transcribes speech accurately.\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio\", \"audio\": test_audio['audio']['array']},\n",
    "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc0nI6Gzcori"
   },
   "source": [
    "<h3>Baseline Model Performance: 24.32% Word Error Rate (WER) for this sample !</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw5XPyYFajyM"
   },
   "source": [
    "# Let's finetune Gemma 3N!\n",
    "\n",
    "You can finetune the vision and text and audio parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T12:46:48.481871Z",
     "iopub.status.busy": "2025-07-20T12:46:48.481594Z",
     "iopub.status.idle": "2025-07-20T12:46:55.013627Z",
     "shell.execute_reply": "2025-07-20T12:46:55.012955Z",
     "shell.execute_reply.started": "2025-07-20T12:46:48.481854Z"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "07ca3dc5-2957-402c-e7e2-062a0ee8887a"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 8,                           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,                  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,               # We support rank stabilized LoRA\n",
    "    loftq_config = None,               # And LoftQ\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "        # Audio layers\n",
    "        \"post\", \"linear_start\", \"linear_end\",\n",
    "        \"embedding_projection\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We adapt the `kadirnar/Emilia-DE-B000000` dataset for our German ASR task using Gemma 3N multi-modal chat format. Each audio-text pair is structured into a conversation with `system`, `user`, and `assistant` roles. The processor then converts this into the final training format:\n",
    "\n",
    "```\n",
    "<bos><start_of_turn>system\n",
    "You are an assistant that transcribes speech accurately.<end_of_turn>\n",
    "<start_of_turn>user\n",
    "<audio>Please transcribe this audio.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Ich, ich rechne direkt mich an.<end_of_turn>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:47:03.723745Z",
     "iopub.status.busy": "2025-07-20T12:47:03.723405Z",
     "iopub.status.idle": "2025-07-20T12:47:03.729197Z",
     "shell.execute_reply": "2025-07-20T12:47:03.728434Z",
     "shell.execute_reply.started": "2025-07-20T12:47:03.723714Z"
    },
    "id": "o8caH7vlcorj"
   },
   "outputs": [],
   "source": [
    "def format_intersection_data(samples: dict) -> dict[str, list]:\n",
    "    \"\"\"Format intersection dataset to match expected message format\"\"\"\n",
    "    formatted_samples = {\"messages\": []}\n",
    "    for idx in range(len(samples[\"audio\"])):\n",
    "        audio = samples[\"audio\"][idx][\"array\"]\n",
    "        label = str(samples[\"text\"][idx])\n",
    "\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are an assistant that transcribes speech accurately.\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio\", \"audio\": audio},\n",
    "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\":[{\"type\": \"text\", \"text\": label}]\n",
    "            }\n",
    "        ]\n",
    "        formatted_samples[\"messages\"].append(message)\n",
    "    return formatted_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8b84b25a72c749f89b97ed9eb0be628d",
      "588d23ae9e58450eb366fd63e3b7b698",
      "7357a1ab82f64220899ce92feba3276c",
      "f6765aeb1d9a426c82304cedc1be474b",
      "97196c6a33ff4efba8c3a0f28885c29d",
      "9d544cff64354e6695e90cfd70743d72",
      "3e16f926611b458fbfa21f75bd8de2c2",
      "dc6f8115279042b89728d3018a884e28",
      "c4a90b4b3bd74658867fbbea9808b1e3",
      "54cedbd8ef984f6a89590f1651804c03",
      "9fdf6d3312be4f51912561ae1cb5a893"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T12:47:08.489955Z",
     "iopub.status.busy": "2025-07-20T12:47:08.489357Z",
     "iopub.status.idle": "2025-07-20T12:47:09.221727Z",
     "shell.execute_reply": "2025-07-20T12:47:09.221018Z",
     "shell.execute_reply.started": "2025-07-20T12:47:08.489932Z"
    },
    "id": "k7CQ3jvDcorj",
    "outputId": "77d7b072-2459-4b73-a822-fb8de1f37ba8"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_intersection_data, batched=True, batch_size=4, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:47:09.614840Z",
     "iopub.status.busy": "2025-07-20T12:47:09.614240Z",
     "iopub.status.idle": "2025-07-20T12:47:09.620543Z",
     "shell.execute_reply": "2025-07-20T12:47:09.619824Z",
     "shell.execute_reply.started": "2025-07-20T12:47:09.614806Z"
    },
    "id": "xnT9__SDcorj"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "        texts = []\n",
    "        audios = []\n",
    "\n",
    "        for example in examples:\n",
    "            # Apply chat template to get text\n",
    "            text = processor.apply_chat_template(\n",
    "                example[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "            ).strip()\n",
    "            texts.append(text)\n",
    "\n",
    "            # Extract audios\n",
    "            audios.append(example[\"audio\"][\"array\"])\n",
    "\n",
    "        # Tokenize the texts and process the images\n",
    "        batch = processor(\n",
    "            text=texts, audio=audios, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "\n",
    "        # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "        # Use Gemma3n specific token masking\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "        if hasattr(processor.tokenizer, 'image_token_id'):\n",
    "            labels[labels == processor.tokenizer.image_token_id] = -100\n",
    "        if hasattr(processor.tokenizer, 'audio_token_id'):\n",
    "            labels[labels == processor.tokenizer.audio_token_id] = -100\n",
    "        if hasattr(processor.tokenizer, 'boi_token_id'):\n",
    "            labels[labels == processor.tokenizer.boi_token_id] = -100\n",
    "        if hasattr(processor.tokenizer, 'eoi_token_id'):\n",
    "            labels[labels == processor.tokenizer.eoi_token_id] = -100\n",
    "\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We train for one full epoch (num_train_epochs=1) to get a meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T12:48:17.004874Z",
     "iopub.status.busy": "2025-07-20T12:48:17.004079Z",
     "iopub.status.idle": "2025-07-20T12:48:17.279559Z",
     "shell.execute_reply": "2025-07-20T12:48:17.278695Z",
     "shell.execute_reply.started": "2025-07-20T12:48:17.004848Z"
    },
    "id": "95_Nn-89DhsL"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        # use reentrant checkpointing\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "        warmup_ratio = 0.1,\n",
    "        #max_steps = 60,\n",
    "        num_train_epochs = 1,          # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 5e-5,\n",
    "        logging_steps = 10,\n",
    "        save_strategy=\"steps\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\",             # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for audio finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 2,\n",
    "        max_length = 2048,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2ejIt2xSNKKp"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "# Let's train the model!\n",
    "\n",
    "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T12:48:20.209164Z",
     "iopub.status.busy": "2025-07-20T12:48:20.208832Z",
     "iopub.status.idle": "2025-07-20T13:42:42.607026Z",
     "shell.execute_reply": "2025-07-20T13:42:42.606099Z",
     "shell.execute_reply.started": "2025-07-20T12:48:20.209142Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "471d4e93-5a88-4ce7-83d4-5be372a33a2e"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64` but for this example we use `do_sample=False` for ASR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-20T13:57:34.004664Z",
     "iopub.status.busy": "2025-07-20T13:57:34.004306Z",
     "iopub.status.idle": "2025-07-20T13:57:59.332316Z",
     "shell.execute_reply": "2025-07-20T13:57:59.331671Z",
     "shell.execute_reply.started": "2025-07-20T13:57:34.004639Z"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "614f37fa-6c95-428d-f87f-27d11d7fb4b2"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are an assistant that transcribes speech accurately.\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"audio\", \"audio\": test_audio['audio']['array']},\n",
    "                    {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T11:44:41.343411Z",
     "iopub.status.busy": "2025-07-20T11:44:41.343040Z",
     "iopub.status.idle": "2025-07-20T11:44:41.347705Z",
     "shell.execute_reply": "2025-07-20T11:44:41.346945Z",
     "shell.execute_reply.started": "2025-07-20T11:44:41.343390Z"
    },
    "id": "4zWF5gWycorl"
   },
   "source": [
    "<h3> With only 3,000 German speech samples, we reduced the Word Error Rate (WER) from 24.32% to 16.22%. This represents a significant 33.31% relative error rate reduction ! </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3n\")  # Local saving\n",
    "processor.save_pretrained(\"gemma-3n\")\n",
    "# model.push_to_hub(\"HF_ACCOUNT/gemma-3n\", token = \"...\") # Online saving\n",
    "# processor.push_to_hub(\"HF_ACCOUNT/gemma-3n\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastModel\n",
    "    model, processor = FastModel.from_pretrained(\n",
    "        model_name = \"gemma-3n\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3N?\",}]\n",
    "}]\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 128, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(processor, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3N-finetune`. Set `if False` to `if True` to let it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3n\", processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6O48DbNIAr0"
   },
   "source": [
    "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV-CiKPrIFG0"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"HF_ACCOUNT/gemma-3N-finetune\", processor,\n",
    "        token = \"hf_...\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3N-finetune\",\n",
    "        processor,\n",
    "        quantization_method = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgcJIhJ0I_es"
   },
   "outputs": [],
   "source": [
    "if False: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"gemma-3N-finetune\",\n",
    "        processor,\n",
    "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        repo_id = \"HF_ACCOUNT/gemma-3N-finetune-gguf\",\n",
    "        token = \"hf_...\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnz9QOYTMvbH"
   },
   "source": [
    "Now, use the `gemma-3N-finetune.gguf` file or `gemma-3N-finetune-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
