{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa57b0a5",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Whisper on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Whisper.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Whisper</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Whisper.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N3jFCdBF00M"
   },
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGRw17p6F00O"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKnzedECF00P"
   },
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt3oL-TOF00P"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dfs7muBF00Q"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pip3-autoremove'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'unsloth'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers==4.56.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps trl==0.22.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'librosa soundfile evaluate jiwer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487,
     "referenced_widgets": [
      "c892d3ce361746bfac00c8f6ead328c6",
      "8e9b589012ee45f6b529927016ea7a1a",
      "beeea807ab17488f8c5afec6aad209c4",
      "72389328819a49f6aeb2cc76b27fc110",
      "ffa30844c04449259890744ee32769c4",
      "445f0d7a7a924eaf9f68c35b40335ca4",
      "2d2c1fe0e2b44281992070743644c205",
      "a36feb4e16df450281d3ba035a377cb6",
      "1da463827e5e4139b084240abf34e5ed",
      "8e238771c75648a7a7bec07ab2111e0d",
      "16d8e4c8e4af45cb87d3f6f67f6dabca",
      "fd39ef43a0734193a8dc11f96df6df4a",
      "4a017aa504714aa49d3551b76abfb7ad",
      "d156fa95a69845fca80b133f46de8603",
      "52d509b671314f589e09c1af94291d86",
      "6768bfe45e414c4a8c2a84b0f82c3147",
      "788de8404435424da2a0ebf5ef170514",
      "1acd2799df35462eaae311e52ee18b24",
      "d263bd69951549afbfe0e3fb8162e907",
      "173f49df29584317a4474211331e8013",
      "21a2b3eb1a9c46eda3b3c11350cd3185",
      "849ad821796e45c8bc4bcd81d51aec1e",
      "2d063de2bf0d48719034ac49841ad381",
      "138e151db2384548a5b64c10badac49a",
      "b59628da1e004f53be981bb8a5599c48",
      "5d57da0a482a49c19f1da8a6a9983744",
      "a003ea17d77a417398fff564356e7190",
      "bd9bfc1ce33b47518b3f2b7d9fc5da59",
      "ab5ccd985cd7433e85d59f3f931c1c72",
      "2d4dd2b11253490d8380164dac7af20a",
      "642e4381ffcb4adf95c51e8d5cf46ed1",
      "1c4fc33e469c4b77bfcea7ec29387f31",
      "6b03d01093be4acaac4803e94ae0c72c",
      "145427062325485abca5ddbaaa9a4df7",
      "02df1166af03418ab7a9fda227603c9b",
      "5f67b0457c9e479588f5ce9d4e648d38",
      "830ecf35cc5b4c84b086dd8b3fe73e26",
      "5fba4e07ed0c4683a41f49609f652c24",
      "ad0d383415544b06b477c321c9276304",
      "6226735a1771497f88e929d366a8a56f",
      "899b093f564b4365afe746d690245711",
      "54eb73ee072e4334a44c847de0629b4d",
      "c7984b7ac792420c8983c23f8fde298a",
      "5906f8142a4d4d9d89130d282c14a1c3",
      "1d95a3e1ec884421a21199c5ae798343",
      "7b045cac3772417b9f0594beceb4fb1e",
      "3404013ee51040718a9a03ed5e73a63a",
      "8b5e55de87c047699e3c62874ed155ba",
      "54cab3c047bc43d499f23d104a6043ab",
      "9ec9f3a5ff954020bd7996cb95810500",
      "744e09fb36064529b86e141978565308",
      "4f9180a0a8854dae8a35c5e3cd15b420",
      "0364553c570d44418f553d9afb76a1ba",
      "e77e6d6290bc4d1e8679904f1801c6eb",
      "b17c8d852ff244179376a9e9f9c44a17",
      "dd1d74b363d34bd08b0c181242f44f0d",
      "e82c514515e94a1291a19bf7fe95121a",
      "25a4f707a36844dea0597c96bc7917d9",
      "c84630fab82a4e6fb519c23e61cf4806",
      "257b3f2eae45481faf6294a9466ac566",
      "daf2c9e4aec54b508d285372723d400c",
      "26fbc20b1b8f4e628365af592d73264b",
      "a1036dfe03924f64a6c64ec83039ac17",
      "b8362a2f8a9142b49dd352f86f2da0d3",
      "2f290e77933c4c109c8f3fc9b5d98b83",
      "eb9486a8e57b472fb834d4d7c1bd8dc8",
      "e997bbef44134d18b8ebd3a3bff41e4a",
      "1d6a9ffa3a5a48b09cd73d55b9ef4549",
      "3332c6f86adb4b98a1f87f8a11b74c61",
      "a8f1b213051543299a3224c009913867",
      "0f66d863d9814be79ee1b13bb31ac63b",
      "1029f0da18e840ff95db45464841bee0",
      "3501fbba64624f67bb7378016b75a6da",
      "861296f232b742a7a9017faa1f681561",
      "6608b01ccb054a0b9232a204252fa314",
      "7fe02791d32e4e5cb0bb5e3b947a3b1d",
      "0c4537dc76b04bfbb81f90c842295dac",
      "43b28881a0a745a382dbe815613a2193",
      "a93107da37ff405daeb57c5f60686a77",
      "0966ce25d6634a2cb261bd0611bc7dea",
      "d77e739e68c445819a331dfb8028cee5",
      "9bd1b7653fa04ba281845de5cd023bfe",
      "9721fdf3b4124369a6d184ed33d962a5",
      "2ed11cdeab66418b90cb1d32ec54bb6c",
      "8e52e1ff443442fdb5bb18051565daaf",
      "bcd3bb54f02e4deaaabee72a4c7bc9d6",
      "2ca31525d2b14906b9cd523e8ff21450",
      "8f902846af6d4ac789dc27d9cd1eb31d",
      "1cfdda4a52714e1ba1945202bec38c4b",
      "577d135fae054f838d25f2ae491191d4",
      "6f9cbf63104d4fa19f01b44d3e11bf20",
      "68f0be395985434fbd0cc821475dfc2a",
      "df8f299afbcd484b92eb583c2dc647c9",
      "cf2cc31304d94780883423362bb72aac",
      "6b41de8b6c2a48fca0a1d40b4524877c",
      "309ec2e6915a4c40ae29012e5f852f58",
      "6a37095f692c4b728a72e17303db9242",
      "48aa1ed7528c47c5b9bfa0715e57e747",
      "da6173372f8048ce95406fb4afc7753d"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "ed808bd6-4b47-4146-b41d-c8e413395cc7"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/whisper-large-v3\",\n",
    "    dtype = None, # Leave as None for auto detection\n",
    "    load_in_4bit = False, # Set to True to do 4bit quantization which reduces memory\n",
    "    auto_model = WhisperForConditionalGeneration,\n",
    "    whisper_language = \"English\",\n",
    "    whisper_task = \"transcribe\",\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "de4c48d1-1dd5-47ca-f70d-d9f4631a671d"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    task_type = None, # ** MUST set this for Whisper **\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "\n",
    "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio**. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149,
     "referenced_widgets": [
      "df36cd96b36f4cdd9322604303e79fdb",
      "565d4bed5a0941a9a9daaa45daed0fc8",
      "38fe88720b564070bfd2f32b6a11b46b",
      "efd5bd00f4024c47bc7ea71bbc78b78a",
      "a722a26143644b17bdb5bd63fade7585",
      "032f90a3113c475fb040564398348a74",
      "e6b5553ec3aa4189b0a9be9f22d415ec",
      "22c957c6487844d09efc0a9995e56afc",
      "e4a0d7d237894887a2fbb2325d3ddf34",
      "61360d1b4c5141cd809c790dec74c850",
      "e2a249736e6a43778f46f8c5fdd79f7c",
      "7bdeabde0e574f2cbfda546e1a66f4fe",
      "fda328a49a034a97b60f970e92abb348",
      "7c22e5164d884dd3a2743f54d5a9e8ff",
      "73c48713844b4e7baa7e107e4ccb15d6",
      "e74e72b35f784b80b6e57fab849f1aa0",
      "caf0f383cc6e4a06b0192fdd59f7fc69",
      "3ebfc562fbda4c099c0f5cbe4cdfb787",
      "5415627072184b988f73443bb94c03c3",
      "85a355bf9d5f41608ee5ecb0c74fdc50",
      "b8c3efc67fb641fba7955f821b908054",
      "9c84500b892f469abbe43c37826344b1",
      "82baca07cf814b99b22fcb10e73b76c0",
      "a9bb01649a1247cdbdb4fd130f58fe28",
      "b0ce7f64c3c2471ca05425645cb3853b",
      "65fa1d1c9f6f41e1aa8e67d6db3bac99",
      "18cb8dd7b84447f080eedd5e8af670dd",
      "995e8fc5800f468d81bc27e0d30bd25a",
      "c41ffd9d24db46209c03f2b777cd4cf7",
      "56913052698141f18d2db65659dd52bd",
      "a4dfc2467efd43e78073991ef07686c9",
      "e98052c7d80b4906b8f7ccfc1eac2d73",
      "f6dcb3106a8f405a8a2f55160a746a97"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "2b6a0228-70c9-4dc4-88ff-7c436198f7a1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "#Set this to the language you want to train on\n",
    "model.generation_config.language = \"<|en|>\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.config.suppress_tokens = []\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    audio_arrays = example['audio']['array']\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    features = tokenizer.feature_extractor(\n",
    "        audio_arrays, sampling_rate=sampling_rate\n",
    "    )\n",
    "    tokenized_text = tokenizer.tokenizer(example[\"text\"])\n",
    "    return {\n",
    "        \"input_features\": features.input_features[0],\n",
    "        \"labels\": tokenized_text.input_ids,\n",
    "    }\n",
    "from datasets import load_dataset, Audio\n",
    "dataset = load_dataset(\"MrDragonFox/Elise\", split=\"train\")\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.train_test_split(test_size=0.06)\n",
    "train_dataset = [formatting_prompts_func(example) for example in tqdm.tqdm(dataset['train'], desc='Train split')]\n",
    "test_dataset = [formatting_prompts_func(example) for example in tqdm.tqdm(dataset['test'], desc='Test split')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8ad38901c81d4c7da0462720afaf883c",
      "54e81fc6544b4c31ac70a0efc99e942f",
      "2e55f8584907444ebbe90d0580211cbf",
      "70dc822dfde040488efd7e61f4024f4c",
      "06681df6f18344d8836f4a0cddbad2ef",
      "b3e6f15e04914c90aefa419686e4f8f8",
      "11612a5e0039485cb9f483c83ba1d20c",
      "bc98b634571b4f7e80025f18deed36cd",
      "5d8dba18aabb4feb8ba612aa85e10e8f",
      "e05f182e8cbd484083ce5deeec7b8b87",
      "9fd603fabb304f12b435485b882bef2f"
     ]
    },
    "id": "_w6weK8CGScg",
    "outputId": "1595dd2d-fb6c-4207-88ab-e6026457e315"
   },
   "outputs": [],
   "source": [
    "# @title Create compute_metrics and datacollator\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import pdb\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    pred_logits = pred.predictions[0]\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface  `Seq2SeqTrainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "d338d8b3-b21e-477d-d511-aa65862705c2"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from unsloth import is_bf16_supported\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=tokenizer),\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer = tokenizer.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        # predict_with_generate=True,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        fp16 = not is_bf16_supported(),  # Use fp16 if bf16 is not supported\n",
    "        bf16 = is_bf16_supported(),  # Use bf16 if supported\n",
    "        weight_decay = 0.01,\n",
    "        remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        label_names = ['labels'],\n",
    "        eval_steps = 5 ,\n",
    "        eval_strategy=\"steps\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "8563a1fd-fbb2-4d5c-d06d-120b6139439d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "a55b5d8d-daa7-46e3-bbd4-d4c49b8bbf95"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "09b01664-eebc-4d58-c581-4ac0c7511f6e"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Because we finetuned Whisper for speech recognition, we need to have a audio file.\n",
    "\n",
    "For example we use the Harvard Sentences audio dataset https://en.wikipedia.org/wiki/Harvard_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "fVdUTgPZzLeG",
    "outputId": "a6e41c32-0100-4890-d997-30268ca7ee6d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['wget https://upload.wikimedia.org/wikipedia/commons/5/5b/Speech_12dB_s16.flac'], check=True, shell=True)\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(\"Speech_12dB_s16.flac\", rate = 24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "140ac0e0-af5c-46a5-9878-49d9788f3f3f"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "FastModel.for_inference(model)\n",
    "model.eval()\n",
    "# Create pipeline without specifying the device\n",
    "whisper = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer.tokenizer,\n",
    "    feature_extractor=tokenizer.feature_extractor,\n",
    "    processor=tokenizer,\n",
    "    return_language=True,\n",
    "    torch_dtype=torch.float16  # Remove the device parameter\n",
    ")\n",
    "# Example usage\n",
    "audio_file = \"Speech_12dB_s16.flac\"\n",
    "transcribed_text = whisper(audio_file)\n",
    "print(transcribed_text[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "09305133-223c-431a-9ea1-af1765a7f7a2"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = None,)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFb36CXJF00W"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
