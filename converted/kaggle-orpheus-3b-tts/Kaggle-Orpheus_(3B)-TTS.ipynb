{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e988b2f2",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Orpheus (3B) Tts on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Orpheus_(3B)-TTS.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Orpheus (3B) Tts</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Orpheus_(3B)-TTS.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pip3-autoremove'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'unsloth'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers==4.56.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps trl==0.22.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'snac'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkWYsztAs9Ky"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
    "\n",
    "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
     "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
     "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
     "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
     "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "17fcd6fc-3688-4966-8b31-3e6a78aedb74"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/orpheus-3b-0.1-ft\",\n",
    "    max_seq_length= 2048, # Choose any for long context!\n",
    "    dtype = None, # Select None for auto detection\n",
    "    load_in_4bit = False, # Select True for 4bit which reduces memory usage\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
     "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
     "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
     "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
     "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
    },
    "id": "6bZsfBuZDeCL"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "\n",
    "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
     "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
     "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
     "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
     "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
    },
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286,
     "referenced_widgets": [
      "a8cde83e4c224ff5a519cbe9e850426e",
      "98c97d3a37d347df8e7c245b518638b2",
      "c7b3b2ebc503446794f84cc429843cea",
      "529bb592062e48a38ba67b5d56dbb2bd",
      "8937c2575e9f48cb8498dc6554889a77",
      "f04c54b9ac7e4af1b94276c109ed349e",
      "bf9ea54f70f6427cbfed6e58a6463583",
      "ff5a6cf329e249158b528680706a7b23",
      "7c682a7156b74ddfa9f518d570f6f4d7",
      "1c99dbc970074f9788d87847019699d4",
      "75dd059f3d13436b9fa15a26c3d56b61",
      "560ee8cad0e34f4b9b09da34bf941828",
      "0872c0061d074f79bf57d494ad593558",
      "3a77f42e5a704915a8a0646af884820c",
      "56be0b606f6a4a14bd95613b769937f3",
      "31fc6968c3034152b998079f00021081",
      "4145e0f0ee3e45eaa45706496c45ad92",
      "de11a736d05e463fa25eb5c134a33a42",
      "3e39ad59e64b451c954e3c51a1129348",
      "b34415eb1750447291eeace247253e23",
      "3deacc4ba8a540ef84bdf83d63c5446e",
      "111b14c261a4468daa3f68b8dbc79629",
      "77a1e88343c048faa678a96fa98e52ea",
      "109016db5a1a444fa12f08968edd2973",
      "3673581f82004bb9abb7573a0201ee6c",
      "7eb0c506a20747dfa1c540c6e3d07880",
      "de56bc26ffe3483ea2bea4a4a1e4375a",
      "1104a11ea07348c09d9d6ea18a7a0c4f",
      "c86528d68fb642c6b349e1f72309e9c2",
      "8a7502f1149f47529d5048d0931a9c6c",
      "2cb04c7709b24d3aabbfd6f91bd012b9",
      "54560d7f982a4af0b680c276cb74e8de",
      "6a92469e5ff545e09ca55265ae57cb59",
      "ff4093fe5c8d498d886bbfdec769a4f1",
      "f5199871f0dd40c498abfb9ecec75484",
      "0dc2a1b908fe415fa9a9c27296a10217",
      "5c052294982d4da39950473ab7fd832f",
      "9331cc89e34a44979af1bffb02d758a1",
      "5638ccc92d1342b2a63a3945b957a762",
      "3d2aa872ce1a41fd81cdce2bd75a946d",
      "fe99495940c24e709e79d4af9991a2df",
      "0e25d617040e45f0bf623406b773a5fb",
      "09a90177afbf47dfa75d09e562f78ef1",
      "07fb4f7f1aab42f4a0d184ee2c5644ec",
      "76ed56386b72474897983442d7bfe16f",
      "f379390579104a6badfde97384e5f916",
      "ebbe3e94eff54fae93e251630db30f69",
      "a6b1ed22ab6f42c39b92baff2c4a44fd",
      "284753e8808346b29be6f900919f9332",
      "eb257008691346d3a48ca7a694a6d9fe",
      "6ac48a8dfb674c7090c8831841c3bd4d",
      "ba22f10aa5654ad0a7efa1f7d596bf43",
      "a531278935a54a398db4f296ee88dca3",
      "6f85d840cc744c268da6e7470ffe8a58",
      "1db03134e899468bab57cad43f33bb6f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
     "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
     "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
     "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
     "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
    },
    "id": "zK94B-Pfioto",
    "outputId": "cd93441c-9b13-497f-dae7-e858a2f9dbcf"
   },
   "outputs": [],
   "source": [
    "#@title Tokenization Function\n",
    "\n",
    "import locale\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "import torch\n",
    "from snac import SNAC\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "ds_sample_rate = dataset[0][\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
    "snac_model = snac_model.to(\"cuda\")\n",
    "def tokenise_audio(waveform):\n",
    "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "  waveform = waveform.to(dtype=torch.float32)\n",
    "  resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)\n",
    "  waveform = resample_transform(waveform)\n",
    "\n",
    "  waveform = waveform.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "  #generate the codes from snac\n",
    "  with torch.inference_mode():\n",
    "    codes = snac_model.encode(waveform)\n",
    "\n",
    "  all_codes = []\n",
    "  for i in range(codes[0].shape[1]):\n",
    "    all_codes.append(codes[0][0][i].item()+128266)\n",
    "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
    "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
    "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
    "\n",
    "\n",
    "  return all_codes\n",
    "\n",
    "def add_codes(example):\n",
    "    # Always initialize codes_list to None\n",
    "    codes_list = None\n",
    "\n",
    "    try:\n",
    "        answer_audio = example.get(\"audio\")\n",
    "        # If there's a valid audio array, tokenise it\n",
    "        if answer_audio and \"array\" in answer_audio:\n",
    "            audio_array = answer_audio[\"array\"]\n",
    "            codes_list = tokenise_audio(audio_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row due to error: {e}\")\n",
    "        # Keep codes_list as None if we fail\n",
    "    example[\"codes_list\"] = codes_list\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_codes, remove_columns=[\"audio\"])\n",
    "\n",
    "tokeniser_length = 128256\n",
    "start_of_text = 128000\n",
    "end_of_text = 128009\n",
    "\n",
    "start_of_speech = tokeniser_length + 1\n",
    "end_of_speech = tokeniser_length + 2\n",
    "\n",
    "start_of_human = tokeniser_length + 3\n",
    "end_of_human = tokeniser_length + 4\n",
    "\n",
    "start_of_ai = tokeniser_length + 5\n",
    "end_of_ai =  tokeniser_length + 6\n",
    "pad_token = tokeniser_length + 7\n",
    "\n",
    "audio_tokens_start = tokeniser_length + 10\n",
    "\n",
    "dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"codes_list\"]) > 0)\n",
    "\n",
    "def remove_duplicate_frames(example):\n",
    "    vals = example[\"codes_list\"]\n",
    "    if len(vals) % 7 != 0:\n",
    "        raise ValueError(\"Input list length must be divisible by 7\")\n",
    "\n",
    "    result = vals[:7]\n",
    "\n",
    "    removed_frames = 0\n",
    "\n",
    "    for i in range(7, len(vals), 7):\n",
    "        current_first = vals[i]\n",
    "        previous_first = result[-7]\n",
    "\n",
    "        if current_first != previous_first:\n",
    "            result.extend(vals[i:i+7])\n",
    "        else:\n",
    "            removed_frames += 1\n",
    "\n",
    "    example[\"codes_list\"] = result\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(remove_duplicate_frames)\n",
    "\n",
    "tok_info = '''*** HERE you can modify the text prompt\n",
    "If you are training a multi-speaker model (e.g., canopylabs/orpheus-3b-0.1-ft),\n",
    "ensure that the dataset includes a \"source\" field and format the input accordingly:\n",
    "- Single-speaker: f\"{example['text']}\"\n",
    "- Multi-speaker: f\"{example['source']}: {example['text']}\"\n",
    "'''\n",
    "print(tok_info)\n",
    "\n",
    "def create_input_ids(example):\n",
    "    # Determine whether to include the source field\n",
    "    text_prompt = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
    "\n",
    "    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)\n",
    "    text_ids.append(end_of_text)\n",
    "\n",
    "    example[\"text_tokens\"] = text_ids\n",
    "    input_ids = (\n",
    "        [start_of_human]\n",
    "        + example[\"text_tokens\"]\n",
    "        + [end_of_human]\n",
    "        + [start_of_ai]\n",
    "        + [start_of_speech]\n",
    "        + example[\"codes_list\"]\n",
    "        + [end_of_speech]\n",
    "        + [end_of_ai]\n",
    "    )\n",
    "    example[\"input_ids\"] = input_ids\n",
    "    example[\"labels\"] = input_ids\n",
    "    example[\"attention_mask\"] = [1] * len(input_ids)\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(create_input_ids, remove_columns=[\"text\", \"codes_list\"])\n",
    "columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "columns_to_remove = [col for col in dataset.column_names if col not in columns_to_keep]\n",
    "\n",
    "dataset = dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "**Note:** Using a per_device_train_batch_size >1 may lead to errors if multi-GPU setup to avoid issues, ensure CUDA_VISIBLE_DEVICES is set to a single GPU (e.g., CUDA_VISIBLE_DEVICES=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
     "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
     "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
     "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
     "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
    },
    "id": "95_Nn-89DhsL"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "29905f3f-2821-4a42-ea31-9c9b0a20a2a9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
     "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
     "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
     "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
     "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "096b807f-8b64-4f35-ad26-c57689872edd"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apUdB40Ep6Ki"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\",\n",
    "]\n",
    "\n",
    "chosen_voice = None # None for single-speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "execution": {
     "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
     "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
     "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
     "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
     "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
    },
    "id": "krYI8PrRJ6MX",
    "outputId": "39c3f6a0-6c72-4392-f935-bd4398696b0f"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "#@title Run Inference\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Moving snac_model cuda to cpu\n",
    "snac_model.to(\"cpu\")\n",
    "\n",
    "prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
    "\n",
    "all_input_ids = []\n",
    "\n",
    "for prompt in prompts_:\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "  all_input_ids.append(input_ids)\n",
    "\n",
    "start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human\n",
    "end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human\n",
    "\n",
    "all_modified_input_ids = []\n",
    "for input_ids in all_input_ids:\n",
    "  modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH\n",
    "  all_modified_input_ids.append(modified_input_ids)\n",
    "\n",
    "all_padded_tensors = []\n",
    "all_attention_masks = []\n",
    "max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
    "for modified_input_ids in all_modified_input_ids:\n",
    "  padding = max_length - modified_input_ids.shape[1]\n",
    "  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
    "  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
    "  all_padded_tensors.append(padded_tensor)\n",
    "  all_attention_masks.append(attention_mask)\n",
    "\n",
    "all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
    "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "input_ids = all_padded_tensors.to(\"cuda\")\n",
    "attention_mask = all_attention_masks.to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      max_new_tokens=1200,\n",
    "      do_sample=True,\n",
    "      temperature=0.6,\n",
    "      top_p=0.95,\n",
    "      repetition_penalty=1.1,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=128258,\n",
    "     use_cache = True\n",
    "  )\n",
    "token_to_find = 128257\n",
    "token_to_remove = 128258\n",
    "\n",
    "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
    "\n",
    "if len(token_indices[1]) > 0:\n",
    "    last_occurrence_idx = token_indices[1][-1].item()\n",
    "    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
    "else:\n",
    "    cropped_tensor = generated_ids\n",
    "\n",
    "mask = cropped_tensor != token_to_remove\n",
    "\n",
    "processed_rows = []\n",
    "\n",
    "for row in cropped_tensor:\n",
    "    masked_row = row[row != token_to_remove]\n",
    "    processed_rows.append(masked_row)\n",
    "\n",
    "code_lists = []\n",
    "\n",
    "for row in processed_rows:\n",
    "    row_length = row.size(0)\n",
    "    new_length = (row_length // 7) * 7\n",
    "    trimmed_row = row[:new_length]\n",
    "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
    "    code_lists.append(trimmed_row)\n",
    "\n",
    "\n",
    "def redistribute_codes(code_list):\n",
    "  layer_1 = []\n",
    "  layer_2 = []\n",
    "  layer_3 = []\n",
    "  for i in range((len(code_list)+1)//7):\n",
    "    layer_1.append(code_list[7*i])\n",
    "    layer_2.append(code_list[7*i+1]-4096)\n",
    "    layer_3.append(code_list[7*i+2]-(2*4096))\n",
    "    layer_3.append(code_list[7*i+3]-(3*4096))\n",
    "    layer_2.append(code_list[7*i+4]-(4*4096))\n",
    "    layer_3.append(code_list[7*i+5]-(5*4096))\n",
    "    layer_3.append(code_list[7*i+6]-(6*4096))\n",
    "  codes = [torch.tensor(layer_1).unsqueeze(0),\n",
    "         torch.tensor(layer_2).unsqueeze(0),\n",
    "         torch.tensor(layer_3).unsqueeze(0)]\n",
    "\n",
    "  # codes = [c.to(\"cuda\") for c in codes]\n",
    "  audio_hat = snac_model.decode(codes)\n",
    "  return audio_hat\n",
    "\n",
    "my_samples = []\n",
    "for code_list in code_lists:\n",
    "  samples = redistribute_codes(code_list)\n",
    "  my_samples.append(samples)\n",
    "from IPython.display import display, Audio\n",
    "if len(prompts) != len(my_samples):\n",
    "  raise Exception(\"Number of prompts and samples do not match\")\n",
    "else:\n",
    "  for i in range(len(my_samples)):\n",
    "    print(prompts[i])\n",
    "    samples = my_samples[i]\n",
    "    display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))\n",
    "# Clean up to save RAM\n",
    "del my_samples,samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "030a6e13-9371-4717-c5c5-d4e3563e0cca"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHjt_SMYsd3P",
    "outputId": "bd8cccb7-6b95-45bf-80da-de120988447e"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
