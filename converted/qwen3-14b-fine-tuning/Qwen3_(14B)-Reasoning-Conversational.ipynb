{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecff3fc9",
   "metadata": {},
   "source": [
    "# ü§ô Qwen3 (14B) on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(14B).ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Qwen3 (14B)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">A100-40GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">24 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">text-generation, fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(14B).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajq1W8ipjyK"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573,
     "referenced_widgets": [
      "145cc80490fa42258fe6e5a643b57dd5",
      "c3887400b6d34c30a54c6af94c2b612d",
      "7efa9f507c8546bb9b0b5d47be527c25",
      "74267e9cd64d44b58bdbd7dad03b681d",
      "dd2adf3e30304398b8340253dbd4489a",
      "7c6caf15c5de4a2fb699d034a6ab776c",
      "ebe9490475bc437c92ebc03187e53382",
      "46b625ee9ac041338432f548ee3ab51a",
      "fea4c81baaf84d9b806c1853216c89e2",
      "a151688c648045f399e32dc55dd9a1b3",
      "55b26ab90b1043cb8ed36aab316a45ad",
      "247ee0a6f4e64e5ca0a435d243690d0a",
      "4c1c1bc584cb456c9d07f4ac267cf2c5",
      "cc9479fb190742fd865613680e87e535",
      "ae9317b34ba341c4ac40797da3ac2478",
      "5012456fc82e47aca5a69f52a36cb8f5",
      "e84a5ae6c71d42ea8187ebbdcdb4791e",
      "2c04e534215f4d40b103be09e300b744",
      "d7ae6535329c45009bf5664fbd30bbf5",
      "0e112f4ad6974529a4c4f583e6a73950",
      "9c9b22a85e2049089b2194770637c91e",
      "fb2ce6370b1d4d5d9dbdb98aa236da71",
      "2dff2433978a477582b995bc6abafe68",
      "b998e55dd17b44d7a8c23ac427619036",
      "5210d369018a44d4ad2fbd26190aca9b",
      "64b00e376bb142c88a690757f27b8294",
      "09ce664a0a404087a9fe53a5c2d5316e",
      "1a09813436c24b5ea68e652254288ee0",
      "a469a1af99124ff6974265f45563deea",
      "36eed804652e4c6fb7fe2e969228decd",
      "7b8c883b15d84ca2a12bfd3f7a87101d",
      "d7afd9c01f68473387f009b05d829b3b",
      "f34c9b4f069f4fb281f4e0145f0e818e",
      "a99b117d78a8493c9f6ae80f5bf6ea5a",
      "b74e4e5971684b00b40ca42cf2aebb9e",
      "d9dcda48a37e41f7808a933cfd939ded",
      "23791684cb5349c4853cffb4e72ebe1d",
      "c6514d1ee44141d3bf83fde032d39a46",
      "6769817deaaf45fc8a0efb945570f412",
      "d7465d2830f64de996d4d3a306b97c82",
      "a68cbb7211b448c78cd418a90b908037",
      "eb052cc7147d474597529c462497b731",
      "2aefab33f6f345869c19899ca2952df1",
      "881919ffdd7940839c8b40edba7f8c01",
      "b8c184cdaa6b4d72ad28a604d1a8fa39",
      "179e343762ba440cbc094e0e85b4ee84",
      "ecd44b2e03794d5783ffbf07d8b8619f",
      "28d439d23bf54688963517fbdb482339",
      "7e7ac790b8af4cfb978bfe8ac8499900",
      "3cb88502cb1643a8927049baf40d56b7",
      "c37a4b08afa84b64b40ebbe08cbfb018",
      "2de7e147b2e14db38243c7656a29f0e4",
      "bce369933ce540d7b6ec670b04d7f1a4",
      "0940df31fc9047ccae4870b7d2c89b3d",
      "0b3d64dd05f841d68ad472ce933e36d7",
      "2068eb23121440ec83d8cd117b4c6ba5",
      "7054b1bcb73e4515afd29b42a32b20c5",
      "09cd31746a174e96bb346e1afc7b3c8b",
      "802bdf3c4293448bb00b625722f1a9f6",
      "f7c27321d53047479c1ed45b5d5109f6",
      "55a9f6bcb83d4a98a9efcf18253b5091",
      "5e18861f3fa24666869822349d1de377",
      "64eb3128b25448b48268ec61cac289d1",
      "bc277d60ad4a419a94ded28a1c27a9f4",
      "57ae3d07d1244ba495573895ff11e28e",
      "0e997f45717c44e1944d510ae6c51fb9",
      "9d02fcd7882345dea97be6decb5293a5",
      "7b34f538a42f4a9ba9de379ae57f0134",
      "1f9eba179bf847dcb47dbda34611459a",
      "844cd70bf80d40f79c520caf1d7e2a5b",
      "fa8721e596b74611945a1d76e9deff8f",
      "968919dd5d5f41ce91da5cdea02b438b",
      "31d4fb1807bc4ec0840d63ef0bf2e0f9",
      "f8f15be1afc44472bff560ca7316873a",
      "a40e10f372644d80b2830d0f42fcde6c",
      "2f4608614780453a826e3ad59817d7ae",
      "c89e5721ed7e48b295ccc74b841ec61e",
      "e3ecd73a9b86479e8596d8bb1ef5791d",
      "911bd3f394ce4394be92e50782d6ae39",
      "ce05df3e26834837b2db818657a9d175",
      "b25399bf6ae4414ba2836733f59e4ab0",
      "a81ef4374fde4d10a1cdc5daeec34dda",
      "dd1b79ecec114d28ace8c1b8b1fd4d5d",
      "d4b3982b73d046478f7c9b561ed42298",
      "63abaade8f464ed6bbd51d77014dfe66",
      "ef69f52b4c7a485b8708f171bb6acbd6",
      "92be8fd7a814466c983d689bd5d6e1a9",
      "7a3a67547e2043a0ac74b49c47009954",
      "196f35f21b97476a9814acd96dbec717",
      "3ad6055d2ba2481c83b1b136e5898986",
      "8147cc77ce3941c290982d21c42f9f66",
      "f1af17f7a7ee405dabd07f41b6b786d7",
      "aeb720eea25149deb34e6844a8bdd2c5",
      "7d574f195dfc4bc4b1ca86863d97e597",
      "c8faa5be5b88425298b5655a8f507a2a",
      "93a91db5fd6147c5a7982496f09c5b8b",
      "5132d82c92ac41d2b592bf6eebf92c19",
      "4b19bb0b66b248fdbcd01b6264e2ff02",
      "689f645af24947e8920b631d2aa7c3ad",
      "d9c143d31e494981b188be41bd52118f",
      "67a67c8affbe4ec38f99cbb0a3c52dcc",
      "0d0852d9ebb2409ea51650f538dd1621",
      "904f626a367745979ac664f4d2ea6409",
      "e0260495036b406fbf462b3c387205d2",
      "a86e54867d5141dfb1e31ed2d706434f",
      "9864096e9bb54228bf1d9e581b8485bc",
      "062f278ab1c94d8099e06074e0cd360c",
      "3ad57df96bec4267a220a739cfc72bc1",
      "e976be66ed774ce880463df39d0c25ce",
      "67e43763f2f7457487d8b5bfca51b8e4",
      "b1542f5b57f14b98be813e6b2540bb80",
      "34b0ab119eab40eda8b7a551642e0e31",
      "c519c7b69d70467c875a3d228e6b6fd2",
      "cd8774e4577a4652863469d3cb75f2ee",
      "40b0b562564b4e969c02902b1bbea6e8",
      "71c37d1f12294e858ceb337d2e37e375",
      "00d671d686af43c38b12a9448c5bbf06",
      "f85353b1b37342859c9aa71442781e95",
      "73d6e9b4704f40aab25fecd24154f9bc",
      "2167f3dc7050467a9f19f3f885cfb09b",
      "569ad7b2350d46549b2f385a126426d5",
      "6910d714dc854e959839e57b5123af86",
      "77f86f331a19461d94e4840213b256f6",
      "fcd0ee642395431e92a681ba8d829b20",
      "12d7cac449954aafa26c6b5bcb6e031f",
      "4ba6022d4efc4c2ebfdf87b288ed9fd4",
      "0fab32e1222f4431a255a36df0a22a35",
      "7013b9ae0d8a4bbfa744a5a3e3c18a1e",
      "3ddcfbdbb0fc49b0b53a296cdb2348b8",
      "2b72ac21d79c459395682f6692c4325f",
      "26dbcdc380e1404687020cbd9bf38513",
      "2abd7191bff846198b2fb033da32f8c8",
      "60c6d4d43a6445e78051c96a462d7c20",
      "06c646d514b448628424dc8c772994de",
      "7c33f0f45eca43cb8015416999b884a1",
      "3c2e07218b764906a278d6a367153548",
      "bf94cc1837ba486b895656b41c1e9c99",
      "8be1618a9f8840af8d89103c37ef02ef",
      "f0b8a9e00c0d4cc1aff1e3a748a8f039",
      "b1cd702821234a9f87933d099ef5273c",
      "6be394c3849a41a3937322325836d604",
      "db172381abdf4bb0a84417882fc462be",
      "0ab824fcbf0e46f7bbe75af9f662c31a"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "ad27d22a-d0a6-4659-be63-60bf3b3561b7"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-14B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "d50f06c8-4905-4f4e-c6da-980145c41e29"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
    "\n",
    "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
    "\n",
    "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "6ec851dec2af487b93f0a626cacba0e9",
      "f3cb03ec368f40fb965beb086b78fb78",
      "bc1b4decb2154b8fb66347408d419ce3",
      "942778de9f014522a928a1408cfcfb05",
      "847e20d244014914a420c20b660c99cf",
      "a48262b1d98a427ab79d589907521d88",
      "de968455db9840f09ced1a8443f4beae",
      "3e3384d811f94fb9a2d065ffdffaf731",
      "ca410b0da67a466abd7c94ebd0762872",
      "50ad501db7af4ef394540eaf36e1793b",
      "8450b0ecd0934facbc4f16fc471f778b",
      "2cfa55ba5d3345cda1cf83cde925a7fa",
      "6367d6b72c37406cafa63daa77263ca2",
      "8f14d9c07bf846a682c1c44f4a2d2410",
      "6e020b0421e84d9a9a8f0b68aa10cbee",
      "13779546fa624d448862777f344bb2a6",
      "96d0f3f2bbc8468fb34e0b61454f8ce8",
      "5496d2ce66584dc3aed8ca0a31e10463",
      "b72d23e8d05449a8bf8a7511ada49704",
      "6379559e06cd4668b8c2322cb7dc7f53",
      "0f195b35b0a7416c8d395c17dea7fafa",
      "32000eaf412840388188523be1033cab",
      "05997290010e49259742f1a560a6aac3",
      "140760e267d84afc852df8cb470d86d5",
      "b22bcb2a7b24497581994b71b02f755d",
      "3c5c945a8daf4b3f83741ecbc26804b2",
      "2170ab68e5724b4b9c77c039c57f8e0a",
      "0340990c29e64f8d9a9ba37f09c55659",
      "5deffaef73d14116a8b1bcf8d46df2fc",
      "e0969d7420e541229f1e0a9de683c367",
      "1c2573a314d94fc1a2f414adfa9d259a",
      "df721834c3de4656909069aed0670d36",
      "4015d6ed6b9d4c1394850ff0ad704149",
      "502b4fc680b248079121ec2a51062b8c",
      "132dbcd182314c10a906d16f43f94896",
      "1b2a213c48f04667a243cc6dc6c43b0e",
      "eeb29e4ed2ae401f9d21cbe941d6cb1d",
      "dcffa04c1de94d3691a14d10df2ee24b",
      "231a411ab9c241a3b9b8a98617ff8ad4",
      "c688c828118f41859a3d71c54b62354a",
      "afb3e794edf046a9b594fcbb99acb8ea",
      "b1a6d94a50aa4ac29965cb1693c9c5c3",
      "12d7229f7d81488e93689d4269ee48ac",
      "d3da0aa9fb884983b6554f3563b05eeb",
      "d127b4248d7e4114a38df6961b0f28c4",
      "dcb04c84d5854f878555a92bcda62550",
      "644f70a125854e418fbb8469d287edd2",
      "65fa509db52047b791159e65ce9108a4",
      "50524c8582eb4814a598654901c35a82",
      "9f6ddb6a1c264fc79f907c54078bb769",
      "a9e3708d145c4bedaa07e5347019fd01",
      "aca910f4f81547b1afdb9a27eb3f4819",
      "42724261ea1a440c8ca92a32df1e52d5",
      "5e276c9e2cf14d38ab5971a32b4e369d",
      "eed62b35a7974086b8d27d9c3e459a91",
      "ea7c149f9c274f8a94970880d99edbab",
      "a7078409a02740cbbd6d59b61a4a8c6b",
      "528e29551b7a4f77aed08c653e49ee57",
      "153259764d2b45e0bc02938d6909d40a",
      "9c35626d415d4468a30e0513cc7a5234",
      "691a8c5c682a4b32b19c7e71c1672189",
      "5a2c1801935444f2a807b2d483c91ad7",
      "247f2a4074cc4fb0956ef06b10b5e5da",
      "d760efb7d31a4bbbb29c7af2478e1cbd",
      "3d9deebcb56f40e3ace1f52e862c1c31",
      "100a0b5f637b4dc1a1da400e8f363678"
     ]
    },
    "id": "5kyTw2n1edte",
    "outputId": "3bd7207b-c08d-486b-949e-ee4868a366a2"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "reasoning_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "non_reasoning_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTZICZtie3lQ"
   },
   "source": [
    "Let's see the structure of both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjgH3lt0e2Sz",
    "outputId": "2be3f7c6-7353-42c9-d676-95949db681d9"
   },
   "outputs": [],
   "source": [
    "reasoning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zoaygOAe3I2",
    "outputId": "b883173b-8212-407c-f375-64eae1ed4772"
   },
   "outputs": [],
   "source": [
    "non_reasoning_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX8H3urDe00l"
   },
   "source": [
    "We now convert the reasoning dataset into conversational format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "def generate_conversation(examples):\n",
    "    problems  = examples[\"problem\"]\n",
    "    solutions = examples[\"generated_solution\"]\n",
    "    conversations = []\n",
    "    for problem, solution in zip(problems, solutions):\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : problem},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5b3081e536ac4a258fe5db59bb927ca2",
      "acbef56d18ae4b86ad0d4b79210db204",
      "2857e876d47a49bbadf3494b1864c1bf",
      "87ca88b487414767af6533125e688186",
      "b1e286de944749ec8f5b4ce03df7b7e5",
      "cde3541ff2444f55a1651b8992e53da1",
      "7b21ad18c43d4509810204b8a4787b64",
      "2b9748337a1c4291ac882194b16eb032",
      "d1b000b08af549689d1be5f0289bf2c3",
      "a1e4e03e40b2480388b67dcb7d2120c3",
      "8bf8b1e60de142a6845aaadbe64fcb85"
     ]
    },
    "id": "gbh19fTOfHDB",
    "outputId": "66be18a9-9818-4ce3-d4c0-58bfdad79b7c"
   },
   "outputs": [],
   "source": [
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTexROzQfJn5"
   },
   "source": [
    "Let's see the first transformed row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "mkj4c6NrfIz3",
    "outputId": "03e0a2e7-1cc5-45ec-ed3b-01828e68d688"
   },
   "outputs": [],
   "source": [
    "reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OMhyEXkfM5e"
   },
   "source": [
    "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
    "\n",
    "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "8cdf258cb0554d64a4aceed488b6361b",
      "53f8155387394238b443d54cffe9a363",
      "852a10d472ad48a19203ded79cf30662",
      "9ced22ec1fc54b4e9e7b7ab1aae0c940",
      "a6a8ff65be444bc3999c28f53d9b46f4",
      "09d254a8222d42b093ff8f229a6fe503",
      "daa8753d49d7458cab39dd1524b20356",
      "c2d8db7e564a40499d58f0d9c11b01a7",
      "5a7feb07c0124b9ab00900eb25efa616",
      "455c651682fd42eda5a25ce06560893f",
      "a56f6c918e064278a0217c53e8cc2f6e"
     ]
    },
    "id": "nXBFaeQHfSxp",
    "outputId": "39a49b43-58c8-499a-a870-63f5c7e60189"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9FcosGvfdNr"
   },
   "source": [
    "Let's see the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "pb0hbEekfeqf",
    "outputId": "1f7d27a8-d2a4-4a41-8730-c92ddadb8adb"
   },
   "outputs": [],
   "source": [
    "non_reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_0L18QMfot4"
   },
   "source": [
    "Now let's see how long both datasets are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unDFuUq1foWj",
    "outputId": "ad1e986e-32ed-4f51-a7df-34499501efc4"
   },
   "outputs": [],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_conversations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgknnOf7fn3e"
   },
   "source": [
    "The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n",
    "\n",
    "Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n",
    "\n",
    "Let's select 75% reasoning and 25% chat based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_szfriCBgCkU"
   },
   "outputs": [],
   "source": [
    "chat_percentage = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DANuEJA7gL58"
   },
   "source": [
    "Let's sample the reasoning dataset by 75% (or whatever is 100% - chat_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-e0KO9GgFy3",
    "outputId": "62c4b67d-b6b8-4f9c-9934-2bd948f9f5be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    int(len(reasoning_conversations)*(chat_percentage/(1 - chat_percentage))),\n",
    "    random_state = 2407,\n",
    ")\n",
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_subset))\n",
    "print(len(non_reasoning_subset) / (len(non_reasoning_subset) + len(reasoning_conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR-4prS_gVel"
   },
   "source": [
    "Finally combine both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfV47_SXgXH4"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "109c736c8b99496e8721f9595c54eb6c",
      "f89daea726144ded892113a161649b64",
      "9f0108d9201f40599facfccf668a6e84",
      "927c60cd1f0d4a32a1ff9427a96a3246",
      "fb6f38d9dd1e49ec8fdfb7ca7ea363a2",
      "6303ec778b1d49dd980542a191261961",
      "5d66a72e82e54d9e8367ca8a2469dd0a",
      "e17016c458a14736bd56ba55d7168f32",
      "1d31954fe1804140bcac71e3d4102fdc",
      "a7077f3352b246ddbbca391c08c48b4a",
      "a1c374126fb144e68060e4fedab51046"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "4aee3090-cf58-4497-979b-cd41cbc5fd62"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "344c14cb-0138-4981-9d85-b1e8856093ec"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9fa371ShyhB"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "4b644b12-626b-45c7-fb11-89825ae13bd2"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "3e2fcdf8-501c-4707-fcbb-7c1b4700bb9d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
    "\n",
    "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "b813e560-8e4c-4491-c8be-18067bc07639"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j873RMcEi9uq",
    "outputId": "3b358da9-aedd-48e3-a345-1ed0ca0bd3fa"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "0a9c6608-d1f5-4779-8ad4-7e3a46e2258d"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOfJSxs_VJjz"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
