{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73abac13",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Meta Synthetic Data Llama3 on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Meta-Synthetic-Data-Llama3.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Meta Synthetic Data Llama3</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Meta-Synthetic-Data-Llama3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img src=\"https://raw.githubusercontent.com/unslothai/notebooks/refs/heads/main/assets/meta%20round%20logo.png\" width=\"137\"></a>\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--upgrade -qqq uv'])\n",
    "try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "except: is_t4 = False\n",
    "get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "subprocess.run(['uv pip install -qqq --upgrade     unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install -qqq {get_triton}'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install \"huggingface_hub>=0.34.0\" \"datasets>=3.4.1,<4.0.0'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install synthetic-data-kit==0.0.3'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install transformers==4.56.2'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install --no-deps trl==0.22.2'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtV7V3HkfLZN"
   },
   "source": [
    "### Synthetic-data-kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1hV9-YHp0FQ",
    "outputId": "834dd4db-df5f-408f-9744-ff07a34e0f46"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Load and run the model using vllm\n",
    "# we prepend \"nohup\" and postpend \"&\" to make the Colab cell run in background\n",
    "subprocess.run(['nohup python -m vllm.entrypoints.openai.api_server --model unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit --trust-remote-code --dtype half --quantization bitsandbytes --max-model-len 10000 --tensor-parallel-size 1 --gpu-memory-utilization 0.7 --enable-chunked-prefill --port 8000 > vllm.log &'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiJcnjILx5kA",
    "outputId": "47274e9c-8469-4556-cd82-6099369a651a"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# tail vllm logs. Check server has been started correctly\n",
    "subprocess.run(['while ! grep -q \"Application startup complete\" vllm.log; do tail -n 1 vllm.log; sleep 5; done'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDGMqFWCiwMT"
   },
   "source": [
    "Optional: Function to check if vllm server is running. Change False to True and run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RXVBeib0kon"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  def is_vllm_server_running(api_base_url=None):\n",
    "      \"\"\"Simply check if VLLM server is running and reachable.\"\"\"\n",
    "      print(api_base_url)\n",
    "      try:\n",
    "          response = requests.get(f\"{api_base_url}/models\", timeout=2)\n",
    "          return response.status_code == 200\n",
    "      except:\n",
    "          return False\n",
    "  is_running = is_vllm_server_running(\"http://localhost:8000/v1\")\n",
    "  if is_running:\n",
    "      print(f\"VLLM server is running.\")\n",
    "  else:\n",
    "      print(f\"VLLM server is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woFPIrnMfZgK"
   },
   "source": [
    "Create data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mni2qxzGRkSW"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['mkdir -p data/{pdf,html,youtube,docx,ppt,txt,output,generated,cleaned,final}'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go18XrL7fd2e"
   },
   "source": [
    "### Ingest source file\n",
    "\n",
    "Ingest source file \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\" . Can also use pdf, docx, ppt and youtube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9FCs0cq6Lwc",
    "outputId": "298e1c7b-811d-47fa-a9ec-935397b31acf"
   },
   "outputs": [],
   "source": [
    "from synthetic_data_kit.core.ingest import process_file\n",
    "import os\n",
    "\n",
    "# Set variables directly\n",
    "doc_source = \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\"\n",
    "output_dir=\"/workspace/outputs\"\n",
    "name = None  # Let the process determine the filename automatically\n",
    "config = ctx.config if 'ctx' in locals() else None  # Use ctx if available, otherwise None\n",
    "\n",
    "try:\n",
    "    # Call process_file directly\n",
    "    output_path = process_file(doc_source, output_dir, name, config)\n",
    "    print(f\"Text successfully extracted to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmBog9CNfzZC"
   },
   "source": [
    "### Generate QA pairs\n",
    "\n",
    "Generate QA pairs with the help of vllm and Llama-3.1-8B-Instruct-unsloth-bnb-4bit.\n",
    "set num_pairs to the number of required pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VL8q7aqTEbJ",
    "outputId": "b89cf4ad-9bcf-49b1-eca2-dae8ba036371"
   },
   "outputs": [],
   "source": [
    "from synthetic_data_kit.core.create import process_file\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set parameters\n",
    "input_file = \"data/output/ai_meta_com.txt\"\n",
    "output_dir=\"/workspace/outputs\"\n",
    "config_path = ctx.config_path if 'ctx' in locals() else None  # Use ctx if available\n",
    "api_base = \"http://localhost:8000/v1\"  # Default VLLM API endpoint\n",
    "model = \"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\"\n",
    "content_type = \"qa\"\n",
    "num_pairs = 10\n",
    "verbose = False\n",
    "\n",
    "# Read the content of the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "\n",
    "print(\"\\nGenerating QA pairs...\")\n",
    "try:\n",
    "    # Call process_file directly with all parameters\n",
    "    output_path = process_file(\n",
    "        input_file,\n",
    "        output_dir,\n",
    "        config_path,\n",
    "        api_base,\n",
    "        model,\n",
    "        content_type,\n",
    "        num_pairs,\n",
    "        verbose\n",
    "    )\n",
    "\n",
    "    if output_path:\n",
    "        print(f\"Content saved to {output_path}\")\n",
    "\n",
    "        # Additionally, print the content of the generated file\n",
    "        try:\n",
    "            with open(output_path, 'r') as f:\n",
    "                output_content = f.read()\n",
    "            print(\"\\nGenerated content (first 500 chars):\")\n",
    "            print(output_content[:500] + \"...\" if len(output_content) > 500 else output_content)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read generated file: {e}\")\n",
    "    else:\n",
    "        print(\"No output was generated\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkWS4c4hgQ89"
   },
   "source": [
    "### Curate Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iH2oDbeRUGkp",
    "outputId": "1c85499c-f5a1-4305-9adc-5d4cfd3dcd26"
   },
   "outputs": [],
   "source": [
    "from synthetic_data_kit.core.curate import curate_qa_pairs\n",
    "\n",
    "# Set all parameters directly\n",
    "input_file = \"data/generated/ai_meta_com_qa_pairs.json\"\n",
    "cleaned_dir = \"data/cleaned\"\n",
    "base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "output = os.path.join(cleaned_dir, f\"{base_name}_cleaned.json\")\n",
    "\n",
    "threshold = None  # Use default threshold\n",
    "config_path = ctx.config_path if 'ctx' in locals() else None  # Use ctx if available\n",
    "verbose = False\n",
    "\n",
    "print(\"\\nCurating generated pairs...\")\n",
    "\n",
    "try:\n",
    "    # Call curate_qa_pairs directly\n",
    "    result_path = curate_qa_pairs(\n",
    "        input_file,\n",
    "        output,\n",
    "        threshold,\n",
    "        api_base,\n",
    "        model,\n",
    "        config_path,\n",
    "        verbose\n",
    "    )\n",
    "\n",
    "    print(f\"Cleaned content saved to {result_path}\")\n",
    "\n",
    "    # Display the content of the cleaned file\n",
    "    try:\n",
    "        with open(result_path, 'r') as f:\n",
    "            output_content = f.read()\n",
    "        print(\"\\nGenerated content (first 500 chars):\")\n",
    "        print(output_content[:500] + \"...\" if len(output_content) > 500 else output_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read cleaned file: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGNU5woJgTwl"
   },
   "source": [
    "### Save to chatML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSOKEeQMaRCY",
    "outputId": "967e7979-2298-413d-8dd2-5fae81eb8871"
   },
   "outputs": [],
   "source": [
    "from synthetic_data_kit.core.save_as import convert_format\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set all parameters directly\n",
    "input_file = \"data/cleaned/ai_meta_com_qa_pairs_cleaned.json\"\n",
    "format_type = \"ft\"  # OpenAI fine-tuning format\n",
    "storage_format = \"json\"  # Default storage format\n",
    "\n",
    "# Set up output path\n",
    "final_dir = \"data/final\"\n",
    "#os.makedirs(final_dir, exist_ok=True)\n",
    "base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "# Determine output file path\n",
    "if storage_format == \"hf\":\n",
    "    output_path = os.path.join(final_dir, f\"{base_name}_{format_type}_hf\")\n",
    "else:\n",
    "    if format_type == \"jsonl\":\n",
    "        output_path = os.path.join(final_dir, f\"{base_name}.jsonl\")\n",
    "    else:\n",
    "        output_path = os.path.join(final_dir, f\"{base_name}_{format_type}.json\")\n",
    "\n",
    "# Load config if available\n",
    "config = ctx.config if 'ctx' in locals() else None\n",
    "\n",
    "try:\n",
    "    # Call convert_format directly\n",
    "    result_path = convert_format(\n",
    "        input_file,\n",
    "        output_path,\n",
    "        format_type,\n",
    "        config,\n",
    "        storage_format=storage_format\n",
    "    )\n",
    "\n",
    "    print(f\"Converted to {format_type} format and saved to {result_path}\")\n",
    "\n",
    "    # Display the content of the converted file\n",
    "    try:\n",
    "        if os.path.isfile(result_path):\n",
    "            with open(result_path, 'r') as f:\n",
    "                output_content = f.read()\n",
    "            print(\"\\nConverted content (first 500 chars):\")\n",
    "            print(output_content[:500] + \"...\" if len(output_content) > 500 else output_content)\n",
    "        else:\n",
    "            # For HF datasets, it's a directory\n",
    "            print(f\"\\nSaved as HF dataset directory at {result_path}\")\n",
    "            if os.path.exists(os.path.join(result_path, \"dataset_info.json\")):\n",
    "                with open(os.path.join(result_path, \"dataset_info.json\"), 'r') as f:\n",
    "                    info = json.load(f)\n",
    "                print(f\"Dataset info: {info}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read converted file: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwXNRq_Bbpxb",
    "outputId": "b388fe75-e50c-4a8d-9a56-707723a4d509"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# kill vllm server. Takes around 5 seconds.\n",
    "print(\"Attempting to terminate the VLLM server\")\n",
    "subprocess.run(['pkill -f \"vllm.entrypoints.openai.api_server\"'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4nXiXBRgoOf"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "ab6ba4f352de4f56abf4a6ff91d8fefa",
      "d319d99ce8e84c858be0fc5d0ee53a22",
      "f6c7ae8c66ea4323848e3da703d8c98d",
      "65441e44919b40248af806dbc37dcf78",
      "543c5052509f481f9922d851a5d03dad",
      "cc0876b62bd54c8fa02fc3148ced4f1b",
      "22ab3beb635a4fc4b9787d7857da9735",
      "42b76c2b475244709e06992cb1bc2b60",
      "079045628e3b4ed2b2ec06cc065d5de1",
      "71d3d14817f94ac9b69d0bca784fc171",
      "8b4e647f7b374d01bb4f019413f1d771",
      "23a43154454f4844b316614fbfc52879",
      "60f56871f426425bbeeb18cfe9856a8e",
      "9528c2e5e6c14019af14f165b241a75c",
      "34df1a077e10481aa7d0fe02df084a50",
      "c230bc1042f44b22aa33f5424719920a",
      "1e56214ea0654ea39dd2c36da1589109",
      "503a0aaec9964edd83eb9d55db83a425",
      "8eebd4e40b4643ce8c30c080af4de97d",
      "b846789249a34ab29594b442d7887972",
      "22ac359d555442b790c946d547e47413",
      "ca05ea6a911f47cd83be9b84c76cf476",
      "0c4b12071a864b248ecc1e8a59605324",
      "fd78c8249bb34c68bcee135f542d9ead",
      "6672d3af7dda4a9b81482729744ca8ec",
      "f214e13094854ed2a7c7f5da2fad6157",
      "c8b47800e7cc46d8839ec9658e498425",
      "b92edd7809b84a4db8288b8de3b0274c",
      "0e287a989a84403cbe8c16e298bfaf20",
      "b15238421c48467d8e74b7c76aed6c42",
      "0d7a249c7b0d408aa86037d4f0689e45",
      "069296ecd4d047808d0fdb6350c4e061",
      "d145f007f4d74467b71d7044e42eafe4",
      "40e360d77f1b48139460d1e4d0f7fb9f",
      "6be688d97f434348a453da3956f708a8",
      "301a948c88a0490897cf9cfb3c0125d1",
      "cf96a86f56ed44d19623c07190e3eb94",
      "803e1a09fe1f48bdbdf13683f29331e5",
      "7fedda50349342c0a2ef021f030cd82e",
      "3499305b74a94d91a1812c8545136e77",
      "ce9bbe4968714f9ba27c731a08f3aa36",
      "29e5efda305941be842d917391a13e42",
      "95c593c9249245b0816abd70e5c3377e",
      "3830709475f94c72842d1fdac9d7f253",
      "fae552d7f28e4a18b4a92288f2336d6d",
      "f4e093d3b96f472c9de2356b3d598484",
      "a8b76b225cba465c8a4c451cbb578065",
      "a322a3d796e24c47a29c7f78559dac23",
      "8590a4973e3c4dc4920d1d33ea979e14",
      "dbde86e0667341a697476e4c43ae0f15",
      "1fc11f3e552c4545bc8489c80862e535",
      "cd97acdfd05a4205b39634f5e0e4cd4d",
      "5c03f68bc3004e62b1fd9d779ddb63f7",
      "e7ac1b007eed4f288cb23c9f64bae637",
      "f473a9b930524635b73ea767d55060a3"
     ]
    },
    "id": "7NFJ3OaicA2P",
    "outputId": "630b8cb8-281a-4d4b-a8ac-313d963f2bcd"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agHJ-Ufecuhl"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdJ5NSg0cvLO",
    "outputId": "8738485b-f947-4afa-ea14-ee042c000915"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXuiq_Qjc7iC"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `ChatML` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style. ChatML renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "What's the capital of France?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Paris.\n",
    "```\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old` and our own optimized `unsloth` template.\n",
    "\n",
    "Normally one has to train `<|im_start|>` and `<|im_end|>`. We instead map `<|im_end|>` to be the EOS token, and leave `<|im_start|>` as is. This requires no additional training of additional tokens.\n",
    "\n",
    "Note ShareGPT uses `{\"from\": \"human\", \"value\" : \"Hi\"}` and not `{\"role\": \"user\", \"content\" : \"Hi\"}`, so we use `mapping` to map it.\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://github.com/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "267afe0c8fa34a86952f0f71e29594d5",
      "5d1138410605484bb760816513ba0d66",
      "0a845d380f334775bdeb75f0f678ebb7",
      "c12ad669eb6049e88f68383332be4a6f",
      "0f9c94795f084b9198f7a4646b266804",
      "ac29467028fb4a5fbf020e2adc629cd9",
      "4cdb1773a6344a97b36f305f8f0e5b81",
      "63e0016129124d8594c10e8b242d6e55",
      "a428fa2504b243dca5a0a8a31882bf4e",
      "22585ec7d5b544b28ccd4359ceaf40cf",
      "20804bce2f9f4faa9450c920ca92e96c",
      "f25e711fe26149aeb2806319f6ec50ba",
      "47135f7c1edb40659e4bd3a3f7ca767e",
      "9e34bc6269d64be8a198f6deeffe80f0",
      "0e05421251204969820a7b2462ab4942",
      "fced6c677e434bccaa2351fd8c09bf2f",
      "628738149eb2408b951f3fac42eb27e6",
      "9d8d72dcd36e483a81965682bb6c8dc3",
      "4e697867516d413da3078374f259e12b",
      "ce2d74f898a849da988bef9e0151da38",
      "f0f74a0cb99548c09e87dac3b768e8c4",
      "478bf5068443445b9343521a23a97977"
     ]
    },
    "id": "tQXQUtfmczE0",
    "outputId": "2cd0ae8e-1e85-4104-fe6d-ec830630cfdd"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "#     mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "#     map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    "# )\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "dataset = Dataset.from_json(\"/workspace/data/final/ai_meta_com_qa_pairs_cleaned_ft.json\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLJRdoTNdBmF",
    "outputId": "a3d42e6d-103c-46fd-9a38-09619f823454"
   },
   "outputs": [],
   "source": [
    "dataset[1][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZcDMk_hdFUO",
    "outputId": "37f8e17c-3ba6-4229-a324-e1081f7f088f"
   },
   "outputs": [],
   "source": [
    "print(dataset[1][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaJo2gVKdWpg"
   },
   "source": [
    "If you're looking to make your own chat template, that also is possible! You\n",
    "\n",
    "---\n",
    "\n",
    "must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.\n",
    "\n",
    "More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEx_zjZ2dI3P"
   },
   "outputs": [],
   "source": [
    "unsloth_template = \\\n",
    "    \"{{ bos_token }}\"\\\n",
    "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
    "    \"{% for message in messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}\"\\\n",
    "        \"{{ '>>> Assistant: ' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "unsloth_eos_token = \"eos_token\"\n",
    "\n",
    "if False:\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = (unsloth_template, unsloth_eos_token,), # You must provide a template and EOS token\n",
    "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "        map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VfnAwcAdfI8"
   },
   "source": [
    "[link text](https://)<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "1cf63a5709a3419ca9a25fc850e41ad9",
      "be62b64786fb45a6ad385afc3159695d",
      "306021728bfb4184b764980f4db086c7",
      "b5c4a874b09b4271926c5b775d144f43",
      "4762f640d04347f9b5d1828310917059",
      "8ce62450a4cc42b98d2e05ba830a09d4",
      "e0d1d7f2cb25401d971e2c88b40764f5",
      "3f66b3c3c3a040bdb70a1438f8c45513",
      "bce66a4b33b94e2aa9aa1b85d2f0fa5f",
      "c58c58a2f0354288963d90564d059dbf",
      "9d83f2442388443384ec9b7c58c4fb07"
     ]
    },
    "id": "sAhTN_47dbDx",
    "outputId": "909f9137-a041-4286-ba2f-78d27757542d"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5O7s1eldppw",
    "outputId": "ff284c05-397d-4b81-b341-0c462ea10a15"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gTbJG8PDdl49",
    "outputId": "2657923a-fe7b-4d8b-f07f-1e8f063b65be"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSy1iIZ4dxFS",
    "outputId": "7b643b59-cbbe-4d3d-a659-de0709e681a8"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exxEe_OqeGBz"
   },
   "source": [
    "*italicized text*<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Since we're using `ChatML`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GcqXN8ReGdm",
    "outputId": "3318490f-3ba8-45b2-9ff8-7b4163e70e1c"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A0o2bnEeTyn"
   },
   "source": [
    "You can also use a TextStreamer for continuous inference - so you can see the generation token by token, instead of waiting the whole time!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VkB8p3FeMwF",
    "outputId": "fdbf4d2c-9ba1-4475-bb77-9ef328e766e1"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXD2HbvqeY5P"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oq0uQvJleVzM",
    "outputId": "2d6818d0-1a74-440b-afa2-e2739a3aaa78"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-skt9DSCej7i",
    "outputId": "5187f9b8-0a06-4277-baaf-81c102ed2b57"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    device_map=\"auto\")\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psUxQUjVeqRH"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0EjFtYNenVp"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoModelForPeftCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
    "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8K1z6UXeu56"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH-zV3w6esgG"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k88rv58-e1vI"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMkq_nIGexe5"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
