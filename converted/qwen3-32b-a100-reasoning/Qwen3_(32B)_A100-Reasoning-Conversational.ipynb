{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a649b9bb",
   "metadata": {},
   "source": [
    "# ü§ô Qwen3 (32B) A100 Reasoning Conversational on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(32B)_A100-Reasoning-Conversational.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Qwen3 (32B) A100 Reasoning Conversational</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(32B)_A100-Reasoning-Conversational.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636,
     "referenced_widgets": [
      "684944b0a4a54c8ab092dc0355d1af0d",
      "54bde3f09003431b82112b50aa866dc5",
      "74c7d5c55a09439184c7afc0106edf62",
      "e5f557ff28794c56b32b128490dfb978",
      "b7b3ee4be162498ba1e5263fd9122307",
      "d45cc87b420f4963ae231de816db831f",
      "dd07225f24e54f60ae43f918125fd01b",
      "e56b400164b54ad98ab3b37206b16cd4",
      "061f77ee47c7496d9e2b602034e80e41",
      "9eb6c87859bd4362b57b3aad3962ce5c",
      "e3763bbfaebf46948a257f2908ca99b4",
      "5385d44fe341447685e7c8776afcd9d7",
      "14f6aa21605f496e83c952e941fecca3",
      "53b0dcd5f1824236b11801160f82c888",
      "d9544a25774d42b5a2c2ca16f607b7af",
      "04218a34afc043889b8a81a8bcbffedb",
      "f84cc8ef0dda46ab99eca3297f33e160",
      "24b300fa85d44ba298eeee9192289b6d",
      "2338444dbc414620852faadb7d87865c",
      "f6da3fae5fa24e51abcfb9f1ce1eba0f",
      "a975af7f2e7a4dd7b85a0892b9b497ca",
      "c3a08e0a0e2544b7b076a1d2e9040a47",
      "00fe766fd2eb489bb365713699ca7183",
      "1129df54f3284bc88decc097932ad106",
      "9e05a4401b0f469ca2d63be1fd3a0e84",
      "010198263e7b4fff907e825e0b2f6a2a",
      "4fd63b1598e14c5eae9865cc9930f9b2",
      "a22b1a1eadba45519f6d7430d1341680",
      "b8f8e4a10a9b4505819cd308d2e4e5c3",
      "81524275549b4ecaac8314d82498c905",
      "af423a98a1d249bfba0ff39cb51e01c4",
      "2932a2d8bb744e2d9bfa77f599bee67d",
      "87d6fd7d33b14ee79f0b2b0cf4b0b759",
      "fa3db98649b843e197f479a7667d9ce1",
      "1970c64590fd457a8f966f58ea6c6bbf",
      "85842b7ff00c47678d3ce707ee44e435",
      "7c2c14088a69406ab357efb0b67fb96e",
      "a878eba81bde4b8dbd3be5813a089392",
      "bf83b0a460f24830b3a6ec9386a11075",
      "f96e405107724968babef2def1b19d71",
      "3febed01e53847989c5a4d584b4a46de",
      "44ca5c000d4642dc8c17c1bdf3c6944c",
      "25d1b6415bef4b429663ab1e930244a3",
      "8d1f0315e5394e35b1231b7eeccee9cd",
      "f816f85e91f045a5a63fd5b66b0b885a",
      "a29e61688de1407bbb76ae03506270bb",
      "8cca2982911941e89faa1544cd7fc03a",
      "46cd589a98054441a4ad78208688ad64",
      "01821c10a86d4b479582af1630b4a778",
      "acee62fbddcd4cc5b67e90abb8538270",
      "1bfa02da760d49a181fb843794654082",
      "fa0da76216c6479ea3f2d6b07dadabb6",
      "82eacb68f4df43c791b642f16d6f9a36",
      "cd6dd342878949f8b2f2d2db65675a53",
      "1a07749cc4b8444eb2889c13af19e20e",
      "1cab0e8419e74029ad67b647629b5fc5",
      "7081749625cc44be8615632c7d0ea466",
      "01450d77f51e42ebb8cf09986a69fe37",
      "4f4d186ffd7d474eaf9cc393708e38f3",
      "91a5d90851cc4e538fb7ab89c564c20b",
      "5f455e26490c44d4b414a8cc40df26fd",
      "5608554efbf44eaaac91c7419af30d93",
      "761fb1e2e71948d69c5db1e2631b88e1",
      "37a8a76d52664de0a673be71afb15c28",
      "65f90af4fc7b4c889cd8917c00139e82",
      "ea41c0b57aa74cc2a857f6fdd41eab10",
      "2c1ca9c4a3b447d69641b364f7fd57b1",
      "e3cd8bc98c854f4e8a3cc9a11b3629d9",
      "7b6189d3881c4baab07a6290ca322434",
      "88b4cc8f61b1448783081c550ab8aee6",
      "fd21087eb1dc4117b801625ff58e72ea",
      "3273235d3c5b4556ace12b225105f9ab",
      "b06683d0c5ba438caa3da94ccdbebf9b",
      "220ee1eba5ae47c6857bc5e8ddec5bf1",
      "1a44b60f1911403cae75ecf931835800",
      "a94e6f0c59144104846341156910b3e7",
      "161bb2825d9b468c9778af9a804e8b2c",
      "120d4798b3d34b13b07bd85e5eea0337",
      "f8d649f2a2a34971976a23fe1f133123",
      "37bfce64cb2646a9a4d8aabcc8ce016a",
      "d2c477faf40a44dca58e7286ed9607b9",
      "410479b8fd3741e8bc7a36af18c59e0c",
      "393c2b3ce28f4338a21593d5f5197087",
      "1bc75f93480648eb8ccefca84556ffeb",
      "bf491e62f22b43c393503a029b542363",
      "8e860b9934174b3f9e177eafb0d54e1e",
      "6ddf6d67e9f74785a19e6c3bcd8ff4cc",
      "1b8025c63a22471da9bd6d1c9d4ca4be",
      "8f218a65022c46c8bc26e690ba3f5097",
      "a8b7fb45b44340199c762a3d856df9c3",
      "ce33b236ad6f42abbb5bf4b045fee3ef",
      "219699f074504f2cacb0160e1bff294b",
      "2f05acf187334e26b81225a11f376c23",
      "93cc18a2a1fc41a3a6c6b0121b94509c",
      "cc492c115366430e8ecf513d0730b29b",
      "61693908ddbf4e279a34ea0a9b6a4e7c",
      "646a4256ed1f423b855eb5db6ce34c4e",
      "64f8a1b502264485b44ad1cd68f22f4b",
      "405107859c1f4e66be25cfc822c2e874",
      "9af3af41dcd94d5783719471c54f1645",
      "21467b8c0117416da18dde2c784ac131",
      "7cc671bd4a2d4cb08d91113a08cbd80a",
      "5a3eec8ac7824d77880f762729f1445c",
      "3d0ff69a138f467aa28585208773cc2c",
      "e976949004b84b18985afa31f3908a45",
      "fd0efbf571ea4b20a529b569ab6002cc",
      "e46152bbd5c44d2e9fce6ac750db257a",
      "eaada5b5857a4c5da78ee82787eb7527",
      "ebe6badb9c9f46a1887be2ef37a7853c",
      "65b616547deb4848b2dd631e5d912e84",
      "dd225709661a42e28cb130e69a2eea8b",
      "1c4577284b724532a5c8610beb279d7e",
      "3248b0a405214c40add6bb881d2b1485",
      "0ad185e3ad6a4b0bb64316ba2e56aa85",
      "3a2245172dd04a0da3863b00ae20d99a",
      "f96ddd5c3c8942b9b4dd03653875d458",
      "3a2abfd3f05e4e60adab05430f697f4c",
      "ef3110e56bee485cb996544b0dd9c9e7",
      "5c95ab78b6a6425db9d98f75b9e942c8",
      "1caa80e4681749b0ab2cb8ca6427ffce",
      "2068d98f1bf346deb70079762d982f71",
      "f770f8ecd17b4044bf46fd5ee9d6ae35",
      "0a515ca49eef4cd1883d94a965a1d447",
      "3bd77338b9be43bdb9b3b1cdf7245c45",
      "acd13e8003d048efacf260bfe6caf3f2",
      "a596e58a14ac4aa382b08d3da56292b8",
      "155411f6b7fd497b8fe4d442740c75a0",
      "b73ab4264bc845b38ca161c0b1761046",
      "5733e5d0d2be49d9917f103f136782ff",
      "cfa3b6ff71464b8ebb366c0ec02ebdfb",
      "b3b1d87a8d224ba3a57117bb770c97af",
      "bb99f890bb764c79af63482ed0828f62",
      "07a8b59ea41544b2ba000bc98105c8b2",
      "3e8d8702ecee4f68aeb4771d6c59be94",
      "19e255dddbd04130bcc153c004662179",
      "9e275f2e8ca14b1bbff5c17762681ab4",
      "dec4abc4940548d591c0fa97ded514df",
      "0750ce7e888d4469b140474507cd3eec",
      "8d515bd569b34828bed3872ca1df267c",
      "505853093e474eab8ba152f3e7998b76",
      "25c2955f8f0048aa8f04db31929e5655",
      "3a8296d0adb34663a7b5a0337d991cf7",
      "aa3dde61d91e4fa6ba9108b3e22d9f75",
      "a7ddd868980d4d80b840dcb56b5ffa17",
      "9a2987bcac54473ebec48d126d9702c5",
      "bc9e9df56d974910b3cb240867e61edd",
      "e7009b4b0c2c40c386155a2430a31170",
      "977bfcd523cd4d8da683ae260c91c673",
      "be18f28e115649868e1f27e9fef6f692",
      "08b1fb66b6114a28940cda6cc75672e6",
      "7e316e3960ad418fb84a1480a87f268f",
      "af560caa252e48c588f99e1b268ed729",
      "b25b5f2bfe324e91a70cd77ff75dab91",
      "da6a273b833446448615bb200906510b",
      "1fdef942a22a4265bdc583a6e0c77f46",
      "aaa7c9ed676f4cdab3ec2b8202279c12",
      "128162ec464849549ce9eb437907b88c",
      "0d17afce62a94f019ceb3736d846d325",
      "70ba3b179ef64eae99826cbcf2148698",
      "b1704309479b4b63bf32ac98c5bfc76f",
      "0f82df91eda14ba09827c8c61052f541",
      "c2ac4db67754406d98f292045032bce0",
      "a530970d62594e209438ea5f5d42f992",
      "f3fb1a1406e447bda884b6774281bfb3",
      "331eeaab277144f4b7c0f8e26c83527e"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "7d8591f6-6514-461f-df17-b9530994cd17"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-32B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "15c234f5-fd25-49c5-afa3-8ecae2de5a4e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n",
    "\n",
    "1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n",
    "\n",
    "2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "5f43ca943a654622853bfd7940d53134",
      "a694639b5bf44a749de9df5c8e4330ff",
      "7a47e1f51e97472da5787001abee2d0f",
      "20c1fb8e27ca44d787d43f2e69e48e1c",
      "941d856b93cc4879b45b426c21bf2f18",
      "e56e8374d6a54996a38f30e390b1d847",
      "77991c6419f841b68acdac4a2c304985",
      "77c85bf568e647c4a7af6554e6ad6291",
      "c346f50693fa43d08d8365b41651b412",
      "4c19386e8db845ff97b41f2b79ba89dd",
      "10cab372a7eb4c9480c5ecfa8e823fde",
      "47c9a4d0e94646ba8e3ba9f286c3b464",
      "7c441dfc201c47cda4f00a5738785724",
      "e623fae1374b4be588463f42c2aef8c9",
      "cc25c80a48b5433296b7646953cd1686",
      "8b4c72e726194b8b96bbb0047b9f8b29",
      "0def7dea067e460db7293f133633074f",
      "e302c4243b8c4bb098989c89418d46ee",
      "ee020080397246b8a0d37d20ccb831e0",
      "6ac1834e5f404ca8a434ad2c9ea13a79",
      "3b1610bee47f4fe69eb9f4122a0f428a",
      "b5e75ddf174140778ae5765814be8a4f",
      "06bc172576314f479c41d70a8f71185f",
      "f792a2eb993c4a1cbfde810f1252daa9",
      "c08378aa67be4b71868866c0f4fcd506",
      "d53993c42c854445878d0e949f659c87",
      "b218095c92f74e1898f0bcdcc1942e37",
      "f1679562308e4b79811fcd47d4f86513",
      "95e0174a1f204d7db8baf63b8fa8cbf9",
      "63b8d1721a274ab286f221331b172f32",
      "11adc6815dc749cdabcd24ea4fa3f257",
      "d5ecf9c8d043452991935e04e3cda787",
      "8ab2d842ffe24d3a84bba1ab6f27d5c7",
      "ea9fc967c5944b0d93394613e4ef3c9c",
      "dab6dfc320ef471f800bc28859386135",
      "3bba7e3fee244d2fa3011cb05a936ca4",
      "a3b2f98f40c24e398afbbf828743533b",
      "945446ec806e4d898cf3bc233775de9d",
      "b324da5ce22344f989d327932473dc49",
      "b579ecf8567042c689aa47d18a25a32c",
      "c30a2a2f85fa472388cde06d2fbe149a",
      "32a4fa884eae4bc3846b8e191c93a867",
      "764bfbeb7dc24945bbd1e8980543f03b",
      "fdba2ef34e29407e935fa35df914ebf4",
      "68ed77d07d464168a76aa30c08373551",
      "88ee1737f3844145b6d10065dc766bce",
      "5d0a348c8b844156a93193eb18eafe2e",
      "644fd46218a246ab8dc278a947ed8cd3",
      "dcf58764799a47abb8b352e7365ec746",
      "5365a540303d42bd93140ca37794dd4a",
      "fa6b78b182f3405d931ea834015a4346",
      "011ce12068364b2b82bfe15446f4eb79",
      "067e2f9936fb4c9da15e742a3ed3df9e",
      "8b9b5628f8d9430d92982b340c80447d",
      "d5474af5a74748299b1e7dc819224f9c",
      "d6a428b616984cdd80d0bb085888012a",
      "0896c4e27e16408bb7d2d2fb74c0c816",
      "9622082622584d81abe79194f9f37eec",
      "6da7af32b9224562ae6444343f8f2fb8",
      "126191e7f297429996d726acc674ad28",
      "03c6a6b7ae2d4bec9ccd56450db95af3",
      "2ce310468fd0422082df2c07762084bc",
      "f888213d6dec479d841a6f606987c17c",
      "40c461ef5fc0437ba5f44b53b648906e",
      "b438c8ad34a24736bd7b413a08055ff2",
      "17304ff745d2499bafd903f0a377920c"
     ]
    },
    "id": "5kyTw2n1edte",
    "outputId": "e9f44575-1ae6-4d05-ca7b-c4f0d0f5bda7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "reasoning_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "non_reasoning_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTZICZtie3lQ"
   },
   "source": [
    "Let's see the structure of both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjgH3lt0e2Sz",
    "outputId": "c7eea1f5-e76f-4744-9468-3df2c3788512"
   },
   "outputs": [],
   "source": [
    "reasoning_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zoaygOAe3I2",
    "outputId": "7be91381-71cf-4a72-ce96-4c14d2db1c1f"
   },
   "outputs": [],
   "source": [
    "non_reasoning_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX8H3urDe00l"
   },
   "source": [
    "We now convert the reasoning dataset into conversational format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "def generate_conversation(examples):\n",
    "    problems  = examples[\"problem\"]\n",
    "    solutions = examples[\"generated_solution\"]\n",
    "    conversations = []\n",
    "    for problem, solution in zip(problems, solutions):\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : problem},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "83d4bdd33c0e404da42269b7be9da7be",
      "312ae797b4094ee7a30e35aa9f39d76a",
      "91f74518ffc24fb4924d564a39f94026",
      "181472bf32e4456d872811f446880fa3",
      "7667841295c74b0087f60ea0fdf6d5e6",
      "56b5fc2657c84969a70dc1de237be716",
      "2614d9994ad34f26b747d3b7f0869ebe",
      "bad9eda2ec3446a6b8871c9e2edc8cb2",
      "07790b943c554fa4b6ee48809fee66ce",
      "314b7e39dfbc4f47915ca54f127b882a",
      "d6e887555114460799bed5ed909e2a1d"
     ]
    },
    "id": "gbh19fTOfHDB",
    "outputId": "c46c8e17-f639-4b64-eb9d-8c9596a5f677"
   },
   "outputs": [],
   "source": [
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTexROzQfJn5"
   },
   "source": [
    "Let's see the first transformed row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "mkj4c6NrfIz3",
    "outputId": "10b4f682-e02d-4605-9dff-1aa5f9238461"
   },
   "outputs": [],
   "source": [
    "reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OMhyEXkfM5e"
   },
   "source": [
    "Next we take the non reasoning dataset and convert it to conversational format as well.\n",
    "\n",
    "We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8acff8cdf1a945329d11f171bf0ccb8a",
      "773ca99f39474537be5568674920e1a9",
      "650c5305a96043da8102687d11d37da0",
      "819126bfd6e9458baba40bc53ce4c1a6",
      "fa2e3427b2894bfc8361c15e97834f27",
      "a2e95f8b3dd548eeb56e2bb073451928",
      "415b748f30ff458a93e2a5d818d90b5d",
      "4b6ee0c86bc2437588b7b63dedaf7cb3",
      "fd3addf2a22c4219866692f31a65a6e1",
      "9624b4a5050848e9bbf223f8abd0607a",
      "1d5c83650dd7426b9836a4fa4d0a582b"
     ]
    },
    "id": "nXBFaeQHfSxp",
    "outputId": "d21b11ee-7150-484a-a345-a41466912b95"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(non_reasoning_dataset)\n",
    "\n",
    "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9FcosGvfdNr"
   },
   "source": [
    "Let's see the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "pb0hbEekfeqf",
    "outputId": "d391a93a-3b8c-40ac-9a9b-faef3767c78b"
   },
   "outputs": [],
   "source": [
    "non_reasoning_conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_0L18QMfot4"
   },
   "source": [
    "Now let's see how long both datasets are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unDFuUq1foWj",
    "outputId": "31b3b708-4d5e-4e17-8c07-6f6b482a35a2"
   },
   "outputs": [],
   "source": [
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_conversations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgknnOf7fn3e"
   },
   "source": [
    "The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n",
    "\n",
    "Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n",
    "\n",
    "Let's select 75% reasoning and 25% chat based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_szfriCBgCkU"
   },
   "outputs": [],
   "source": [
    "chat_percentage = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DANuEJA7gL58"
   },
   "source": [
    "Let's sample the reasoning dataset by 75% (or whatever is 100% - chat_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-e0KO9GgFy3",
    "outputId": "5d29ceea-2e4a-4b63-dbe5-9c56e82579c5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
    "non_reasoning_subset = non_reasoning_subset.sample(\n",
    "    int(len(reasoning_conversations)*(chat_percentage/(1 - chat_percentage))),\n",
    "    random_state = 2407,\n",
    ")\n",
    "print(len(reasoning_conversations))\n",
    "print(len(non_reasoning_subset))\n",
    "print(len(non_reasoning_subset) / (len(non_reasoning_subset) + len(reasoning_conversations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR-4prS_gVel"
   },
   "source": [
    "Finally combine both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfV47_SXgXH4"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(non_reasoning_subset)\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c8977608eb524f86a789525d2f0fbe96",
      "1055358fc407474dbdfe30833ec201fa",
      "91b7fd515e7f409f9bc97f0a250f300e",
      "e981c3a04e284b14b9ddc3b0fa3766fe",
      "f992db897a1e4dbda64b5349be39418f",
      "a0300876ab3b405393618796352c9dca",
      "e3e796e20eb449dda112e33c942ef788",
      "98add6c6b8ee4c65adc6f0cad93ba61b",
      "cbccc65b4a2146bcb799da35550301a6",
      "6808c39043234b83804fdcc4f992fd7c",
      "dade17c63bd34b8089ff231ca4a0f75c"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "c262f6d5-705d-4a91-e058-7d552c11f859"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "278b56f3-4fdc-4027-c5c2-90d74d2bf395"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9fa371ShyhB"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "8da671bc-2a7b-47c4-c1e9-bb0b996e260e"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "7a423537-b21e-4440-d695-85718c1640f4"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n",
    "\n",
    "For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "520acb0a-c156-4563-8ad8-5a90fac039ce"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j873RMcEi9uq",
    "outputId": "52c8e01f-9bd9-4b29-80bd-23eb608a28b8"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = True, # Disable thinking\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "90a23ec2-f66e-4b2a-b73e-64a5ece4b803"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
