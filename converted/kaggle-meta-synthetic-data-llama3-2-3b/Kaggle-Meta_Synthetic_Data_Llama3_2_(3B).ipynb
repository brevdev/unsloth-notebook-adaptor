{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be2d1ec",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Meta Synthetic Data Llama3 2 (3B) on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Meta_Synthetic_Data_Llama3_2_(3B).ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Meta Synthetic Data Llama3 2 (3B)</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Meta_Synthetic_Data_Llama3_2_(3B).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img src=\"https://raw.githubusercontent.com/unslothai/notebooks/refs/heads/main/assets/meta%20round%20logo.png\" width=\"137\"></a>\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om2qjxs5PSr0"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCa86oMuPSr0"
   },
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imwJhjjYPSr0"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--upgrade -qqq uv'])\n",
    "try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "except: is_t4 = False\n",
    "get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
    "subprocess.run(['uv pip install -qqq --upgrade     unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install -qqq {get_triton}'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install \"huggingface_hub>=0.34.0\" \"datasets>=3.4.1,<4.0.0'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install synthetic-data-kit==0.0.3'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install transformers==4.56.2'], check=True, shell=True)\n",
    "subprocess.run(['uv pip install --no-deps trl==0.22.2'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Goal\n",
    "Our goal is to make Llama 3.2 3B understand the \"Byte Latent Transformer: Patches Scale Better Than Tokens\" [research paper](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) that was published in December 2024.\n",
    "\n",
    "We'll use https://github.com/meta-llama/synthetic-data-kit to generate question and answer pairs **fully locally** which will be used for finetuning Llama 3.2 3B!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937,
     "referenced_widgets": [
      "7ba927b6ec4e41c39cb445443c301c57",
      "81c79f2262d04a8aba854e679f5e1dac",
      "734ba48df6c34b8fb20e92f99c4e4aa2",
      "fde296d28c5d4c61b384a4553b499d69",
      "442f7ccc30314a58b1aabc91cf4dbe48",
      "1f65e57e365e47eeb83cb259efbd698b",
      "772f1cc7afea484bbb3cb1cc6aa7576f",
      "0d66ab586e7f4f588b3e70bb0d6191a3",
      "d527203d8b414db08f40dd6be3235c5c",
      "50f52e4c2e5e4513b0d1fb23c39657f8",
      "73d0263833c147fc99cc5be1ce5357e9",
      "09b1ea46e7d4437bac1539aeb1a47fbe",
      "ecfbc357537f493e83f9a84606581f4c",
      "425f26b7325541569144cd46eaefcdc0",
      "1739bb52ba624c67804927dd372649e7",
      "33262ee83efa4523b434a4c456067d18",
      "8e20e88da0f34b6fadf118a0443674c6",
      "d5500717a6534c768be6a07913b6a339",
      "7f246d9fe6e9415596645f736027d246",
      "1d95662d1741434581d501e83b99c778",
      "ed0ba5d3437b492dbdb9980ae677c35c",
      "122c08b183f0425f881acef17f5daf08",
      "5454423e6291484bb63b310699f5d645",
      "7e23ee6479ee484e90b93c572848a2b8",
      "cea072caa7694eeb8b858ea0624e88fa",
      "812390f2da734ba9b516af73ddb8f452",
      "f83ddfe2510447d0a6c08b109cf42dbf",
      "cab7a4bf98cf42c69f81b9c4b7b4b21d",
      "1a155e85dd674f468d8b240481413722",
      "1b032fc19dfa46a5add673aa9c6aeda5",
      "df624458fe5a4a21b58602af80fa0a1c",
      "bf983c3e807d4e25ba8c2a079aea34bf",
      "315f16beade4418390ad511458013b81",
      "9a62a83412df4453840f06c97ebcc216",
      "6b9895bccc364862a39b1c70d9612ba5",
      "2eee87d6fc0445038e9ccf896cfa3d77",
      "5c022f8ebf4c44baa2195ca706b687e7",
      "834de6e5d5504ba794c43d186d6de8a7",
      "f4c156d84ae041339c5fef3c9ce6ba30",
      "6b9d7dd7465a4fbfbfbca7cc5dfc2c23",
      "2f5974e37f7847efa368f06b96ae9481",
      "97ff30809ee4484988b6d0de7acd5c04",
      "b0c12a5f8a9a4a7c8a5584b54acb6adb",
      "5738348f2e70424babbd91c44680b87a"
     ]
    },
    "id": "Ym8UA1PRiXsa",
    "outputId": "1dc83bf1-adf6-47d8-fdea-6a4c2f26ffb2"
   },
   "outputs": [],
   "source": [
    "from unsloth.dataprep import SyntheticDataKit\n",
    "\n",
    "generator = SyntheticDataKit.from_pretrained(\n",
    "    # Choose any model from https://huggingface.co/unsloth\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef_MnK575tr2"
   },
   "source": [
    "## Generate QA Pairs + Auto clean data\n",
    "We now use synthetic data kit for question answer pair generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q487TNby-nwT"
   },
   "outputs": [],
   "source": [
    "generator.prepare_qa_generation(\n",
    "    output_folder = \"data\", # Output location of synthetic data\n",
    "    temperature = 0.7, # Higher temp makes more diverse datases\n",
    "    top_p = 0.95,\n",
    "    overlap = 64, # Overlap portion during chunking\n",
    "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV7DyufR51IN"
   },
   "source": [
    "Check if it succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2gQZcr_Wp94",
    "outputId": "6c1fc4cb-de58-4573-a8c4-7c7d05f4f07f"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['synthetic-data-kit system-check'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdl7aPFK55M1"
   },
   "source": [
    "## Document Parsing (PDF, CSV, HTML etc.)\n",
    "Now, let's take the Byte Latent Transformer: Patches Scale Better Than Tokens research paper found at https://arxiv.org/abs/2412.09871 and covert it to Q&A pairs in order to finetune Llama 3.2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BN1yrPGmANA",
    "outputId": "18ad218b-7977-4daf-9662-ddfb32e9972e"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n",
    "subprocess.run(['synthetic-data-kit -c synthetic_data_kit_config.yaml ingest \"https://arxiv.org/html/2412.09871v1\"'], check=True, shell=True)\n",
    "\n",
    "# Truncate document\n",
    "filenames = generator.chunk_data(\"data/output/arxiv_org.txt\")\n",
    "print(len(filenames), filenames[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGdAXafV6S2M"
   },
   "source": [
    "We see around 37 chunks of data. We now call synthetic-data-kit to create some pairs of data for 3 of our chunks.\n",
    "\n",
    "You can process more chunks, but it'll be much slower!\n",
    "\n",
    "Using `--num-pairs` will generate **approximately** that many QA pairs. However it might be shorter or longer depending on the `max_seq_length` of the loaded up model. So if you specify 100, you might only get 10 since the model's max sequence length is capped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYYYlMJ7ZtT7",
    "outputId": "d83c7691-944c-45ab-9fc9-19434a34cf2a"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import time\n",
    "# Process 3 chunks for now -> can increase but slower!\n",
    "for filename in filenames[:3]:\n",
    "    subprocess.run(['synthetic-data-kit -c synthetic_data_kit_config.yaml create {filename} --num-pairs 25 --type \"qa\"'], check=True, shell=True)\n",
    "    time.sleep(2) # Sleep some time to leave some room for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNkxxvBx7Csp"
   },
   "source": [
    "Optionally, you can clean up the data via pruning \"bad\" or low quality examples and convert the rest to JSON format for finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMD-izj5OiAK"
   },
   "outputs": [],
   "source": [
    "# !synthetic-data-kit \\\n",
    "#     -c synthetic_data_kit_config.yaml \\\n",
    "#     curate --threshold 0.0 \\\n",
    "#     \"data/generated/arxiv_org_0_qa_pairs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AScJ5-vAOjYj"
   },
   "source": [
    "We now convert the generated datasets into QA formats so we can load it for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9Um4Z8SqUTB",
    "outputId": "d7880fb6-7548-45f0-9112-9837094285b9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "qa_pairs_filenames = [\n",
    "    f\"data/generated/arxiv_org_{i}_qa_pairs.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "for filename in qa_pairs_filenames:\n",
    "    subprocess.run(['synthetic-data-kit -c synthetic_data_kit_config.yaml save-as {filename} -f ft'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dVK-qza7rPB"
   },
   "source": [
    "Let's load up the data and see what the synthetic data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrBwG2KT7dam"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "final_filenames = [\n",
    "    f\"data/final/arxiv_org_{i}_qa_pairs_ft.json\"\n",
    "    for i in range(len(filenames[:3]))\n",
    "]\n",
    "conversations = pd.concat([\n",
    "    pd.read_json(name) for name in final_filenames\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "dataset = Dataset.from_pandas(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaZ3tRP8frSn",
    "outputId": "c1ec0e11-0a8e-4969-bb9d-c4496822ba9d"
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "504n46Sxfruu",
    "outputId": "815928be-e07f-4257-ddd7-751cf8008cdb"
   },
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BVBp9YXRw_o",
    "outputId": "6c7fcf69-7de3-4bf7-8037-cb0477bce3b7"
   },
   "outputs": [],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aO9qePmP7yaY"
   },
   "source": [
    "Finally free vLLM process to save memory and to allow for finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8qgTjywzgl6",
    "outputId": "f54b04cd-9586-42ff-8276-c4c54496560b"
   },
   "outputs": [],
   "source": [
    "generator.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQo2PR7oqDQE"
   },
   "source": [
    "### Fine-tuning Synthetic Dataset with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282,
     "referenced_widgets": [
      "fc9c4d66a2fa41a393525a04ced011f3",
      "0365c9b5e9a44318a70109069135b7fe",
      "5759ecdb02714a9f82892e12c6b9ae46",
      "85ff0a11606e466ebd7f7b3b6ed3048e",
      "cae045b72aca476a9c16c39778be53a0",
      "c0cf2ea32d7e42d2b6cb00ba5b128e6d",
      "5856dc5484e54deaae2c0ea0703885cb",
      "f229d574614945029666403a10bb761e",
      "b6da3300ba7e42888add16741bafd614",
      "5d6cb85f619b439db3544414b732607c",
      "9f083cc2ddb64b6196cc2d170affabae",
      "3ca561741e5f4b59b796b074cac64d80",
      "15c3082a57e04b0198a8232f135290ef",
      "7cd6c8ef7ec9451c90560208277975c5",
      "f631738fd3014d13a7e6a50a66eaf92a",
      "f8b4855c3f0146dd8d3d0876f3a12c08",
      "a87d29f844c449119a95f5548c0af0c7",
      "abe93b4cbe524be08d08ff75f22d0c08",
      "9f60d2b4aa054b82bcef0b49cd4014f3",
      "27be088be2de4517964a8e5217641fd9",
      "7f616bf83de54cfeaabda52b3f592e4c",
      "2f822636140c45e8a9713b711d057005",
      "d944a4b11b614896bdb6af583faed134",
      "83657ceb17f1454ca7e33a5fcc5ad5f7",
      "c6f86936f86347819002d6bbf50ec484",
      "cf6e3f6267cb49d5ac4575d1d1e41d10",
      "2d0a363ce9c54665811b8cd0a8fdb42b",
      "bbc163a35ec54087bf0c56cc332df0b4",
      "c901f4517b474511b6dd880e63f25b58",
      "74806db2db6e4085a28dceca4fbeee28",
      "e2f989df5f494351a5b242d2430c2917",
      "ef715f56f3824a64949be1f2bb05296b",
      "cd3f2a8158124c6b87be72e959a455d5",
      "62e6190f733844319221ce6eb7a7928c",
      "acd616b0a9574855a3d4525e72a6513a",
      "9b27fcca9ec64b81952560f38080abc6",
      "2f88451af2cd48aa93da32431e5de139",
      "55e82538888940c1b754fa02d29e965c",
      "2b15c1d19cf84911bee54a6d113eed1a",
      "cfdfb65ccad04899881976d93945bf0d",
      "e3ab144489504058b7c9009d867bef95",
      "943c7280c19948999aa663ca6736d698",
      "47d0de835e7d48e39161a7f038167653",
      "6956008e9def426b82617536b015dd38",
      "4952048f852a4593af48573a163a052f",
      "a6e9f1533f83416c8b512f31b099254c",
      "937a90e4c4dc41b195d24fa1694c9157",
      "b82aae8041fa462d88467a02fa252af7",
      "e7d6be17a7bb4d5e8b248d7f5dee9647",
      "fc915d911ee144bea96b1885f5eb7191",
      "caed1189f10140549fe2b445dd7dbd16",
      "8c6845e9bd81418e8c02d66c32046129",
      "2fb3b417c2404e8a9ad83b1a4114174c",
      "e22b473dd97446298b42987e36d945ec",
      "9a18c25e8d694fff905917e7ce2931dd"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "b094477b-ebf4-4980-934b-686f135e1347"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "a8a510bf-226e-4baf-b333-0cbc58cf93a7"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 01 May 2025\n",
    "\n",
    "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "2<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c7787082fecf4df0ae58e67f46e42589",
      "60c5e826831c4afbb869a1429d3a372e",
      "3fdc25451ed543d288d89df799883b3c",
      "079705095aa14fa0bd5af53459ad5bfb",
      "1082d64463a94f6282c1754720c702cd",
      "137ca939b43b4bc7b41da9005e28b646",
      "390ed406aa524d0c91e11ef74d01d700",
      "21ba911bfeb2439aa0ca135c299de1ba",
      "91ae4592c2d545bf9df31983257b6907",
      "1e5ffbbd4d954281af45f8d0f2488de7",
      "6751a852357447259e610430fdbb444b"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "a8b657b0-e534-44a1-89a7-c7962e764ad2"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "# Get our previous dataset and format it:\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKA0VEF4CfCB"
   },
   "source": [
    "See result of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0usAI0M40hpT",
    "outputId": "d9debc89-361b-4fb8-cf56-77acf55195cd"
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cfd9f69db23f4b86a42179ac0ac91a8c",
      "e624589b592d43239e812e02fd3ab244",
      "e0311985f098497e821f7c8720457384",
      "7c05acca6aed46d49663afe10ebf6d9b",
      "8d38463677324bd79c193798157a1568",
      "b554950c61ae436dbb7772e7de173a81",
      "ab2bdd97242243d7aadfe932c72c39a2",
      "7be62d010b564972924fea1d8343398c",
      "14b6b0568a074f8eb20cb57ffbcbb0f3",
      "644e720f44d24d159133bfa515482054",
      "50066adf2c2d4f6abe16456cb3cee737"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "6ee189f7-91bf-4064-faf5-c6a8a96823f5"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "11f7102e-4aed-4389-aab4-35aa346c9df6"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "1965bdc3-3caa-4d5d-8dab-aa93fc64fa39"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "7d2e126d-42d9-4175-d909-31fe15a4193e"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "76e23150-16b3-4ef9-9cf8-8c494bc0f1ad"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the Byte Latent Transformer?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRrr--20Udm9"
   },
   "source": [
    "The model learns about the research paper!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2strt31SUc5W",
    "outputId": "3359dc6a-f533-4960-a875-400af205e20d"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are some benefits of the BLT?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "b74c98e2-3909-46ff-f4c2-9ad838c7021b"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "93cafd03-c9f7-4fdf-d031-f81e3497a839"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    device_map=\"auto\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is so special about BLT's tokenization?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
    "                   max_new_tokens = 256, temperature = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Change to True to upload finetune\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
