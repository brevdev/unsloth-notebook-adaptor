{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8011f0",
   "metadata": {},
   "source": [
    "# ü§ô Granite4 on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Granite4.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Granite4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Granite4.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLQyJ3EDvvZk"
   },
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGNxOXnrvvZ1"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiqrFOJsvvZ5"
   },
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_2LddcgvvZ7"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mKZRiQBvvZ9"
   },
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0GIyDEnG7GS"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# These are mamba kernels and we must have these for faster training\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-build-isolation mamba_ssm==2.2.5'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-build-isolation causal_conv1d==1.5.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "b29e5305f3ee496c8c9a879d85782ac0",
      "5f8a5f77e8b64df98d3552b2dad8a8d1",
      "658f00b01b4b4d94ae2840c011c94326",
      "0d902f1694904abeb5253bc5fe06a9fd",
      "e5310b1920af4733858812fa5c2feffc",
      "7ccba4be8c7549a4853f2bac2fa37b32",
      "d4e858508f634fc98b3f2de47d25ecf8",
      "c564de2c25194b72a82f259b39e5e604",
      "293c5cc6364e47ec89a24614dbdd4dbe",
      "11ed49c6876544d8a0ba57105d9e3711",
      "c557b250871044b5b2af217679ae9863",
      "0f4602c282c44cd3ab174769e2e1b85f",
      "cbb22961f83841c4a1f70cf2817498bd",
      "2677cc5b22064d179867627b75eb22f3",
      "199231efcfeb4f0296561fe38455118d",
      "74c20e75cc944aada1f80b43e84511d4",
      "c374d81f27444157b3d19964c5d4ede5",
      "2484ad6378854463b9b82949a2350882",
      "a6ef5dad92a8405ead885552e228d2a8",
      "6b9f1a821d21466ca16284d5d614d2e2",
      "e75b690548ef41e8827de5728f564fdd",
      "cbb3bba588c14ab3b9c2b7dcf854d88d",
      "b1100dbcbd9746f89240bbe3311bb0d0",
      "7af98c3210624f96b1895341ed4a67e2",
      "f89d47cc98ad4b19bf40da093f96f4b0",
      "c91445b6aa8e4ee29f4ddb9e3092ca45",
      "47c68b58cc8a4fe29d908c17cebb0686",
      "d290257b46f54ddca57ba15c1cb32340",
      "ea2f5667709d4b50b79ad53b68a08197",
      "2f3333c7e63e48659f18d834f12298b0",
      "36317a3053364f0c97f6b1a4a1abcb30",
      "62ee0b4d4a194bd9980ff130e870dd8f",
      "22e862906c144a10a27c1e2dd731b176",
      "34e1589d79734f0e8ea31d106ef6ef28",
      "cf9128ab5f374be8818ea4e9e3c52e27",
      "9f74e0551a34486f957f40dd23be58eb",
      "3f6219a26d844070b3ec62797d3a1caa",
      "5beaaa086167498381700d46cc8ac8b8",
      "a8cc1af3efea46a5aa8f0b7195e8eded",
      "94515227efa348e9986de39335547676",
      "333cd30464a048df903e22ca97f57072",
      "52ce0a733a8c489ab4f520a434e607ca",
      "5bb4a0a07c224b9ea777001ec9333528",
      "a7ed1aad41ff4dd7ad84415c13e12355",
      "47977d72268a48a784e932611bccfe9b",
      "db576dbe825545718f0801756a6ee073",
      "d77798f7922b4810a75cac6e6a73a3f8",
      "329157c9d33f4315bad7e0e568ab022b",
      "95b55cbc68a340c9b1989b13ae8dfe24",
      "b6545d2637794d94b62b9b4cffbe54d4",
      "7c1de13dba39448093dc70aa904d8392",
      "fccfa0c4f18f426093f5c4e6286e972a",
      "d58a042ca1a244a798f530ea39afdf30",
      "ddd0888de1b443de8ff71850b725bb22",
      "945a390235074e2c99bfd54b3258f581",
      "63bc3e92bc9a42f99c6c61975c683f1d",
      "5ddeed106e864428a3604f154b70d74e",
      "f535b7ed30bb4f20b7be26d7f2fb4fa4",
      "30e40fcff49d4e569d17b7af90e17d82",
      "e033a6e78fbf471e91d9b232e198d4f8",
      "ec9a1bb867094cdbaf4f960c66db48a5",
      "6ed8b89858464775893e759d767e24ac",
      "ea0eceb3dfc24565802ad51f6bc8e285",
      "014ca604316342c198ecfb443cf5fa89",
      "587e809acda345b38b24bc2f91cf44fd",
      "91d260d2cd424eac85143b23a557b585",
      "23a355a7060d47fbb945b1c448da77fe",
      "40aeec86d7a5437ba4fe31cdb10b4b71",
      "89505cfab0554ad3a0c4c999ce9b2438",
      "5b29b65a7505457490799d80cb61d38d",
      "4e64e41a0e85484282f297bbde06d6cc",
      "73f99fa50c3b4d1b98336043b0c345d1",
      "d6b252d594e442729fbdc6f412f308c5",
      "95abb90af66d46f681ec0fece453d5c9",
      "6058e3448cbe41f3ad6cbaa2cfd170e6",
      "99dcfd0bcdfb40fbb2b4e1a74044d717",
      "e2228038fa38473084b5b3f69eaa327c",
      "76a2c8eddc3440b7b5ec8be7a7941d23",
      "d463ee86e8e749de8f8e58b86a6ba1c5",
      "0a2a10558213440aa9ab3594ea62d7c0",
      "75a2352d5dee4f559a0607e5fb8616d6",
      "53f46ebe1cd04d2db3d6eb74b8e9b323",
      "96d10357d7334c6b96acf3251456a5c6",
      "7aaf9d1de3c842b38d705b74edb83441",
      "f8a46ba4fac84c6a89d290b7de380f4d",
      "bc3103332e2549108e113f274ba2e498",
      "eb206e3d9e754ad4a15f8a5fe6f31507",
      "e4b02b1eee0c415c851ee2d494e1c593"
     ]
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "52e77b60-a371-4b7f-fa1c-0c13ca757f52"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/granite-4.0-micro\",\n",
    "    \"unsloth/granite-4.0-h-micro\",\n",
    "    \"unsloth/granite-4.0-h-tiny\",\n",
    "    \"unsloth/granite-4.0-h-small\",\n",
    "\n",
    "    # Base pretrained Granite 4 models\n",
    "    \"unsloth/granite-4.0-micro-base\",\n",
    "    \"unsloth/granite-4.0-h-micro-base\",\n",
    "    \"unsloth/granite-4.0-h-tiny-base\",\n",
    "    \"unsloth/granite-4.0-h-small-base\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/granite-4.0-350m-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,   # Choose any for long context!\n",
    "    load_in_4bit = False,    # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False,    # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "138ef88f-efd6-4eef-d0ad-4ad772834eb4"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"shared_mlp.input_linear\", \"shared_mlp.output_linear\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "#### üìÑ Using Google Sheets as Training Data\n",
    "Our goal is to create a customer support bot that proactively helps and solves issues.\n",
    "\n",
    "We‚Äôre storing examples in a Google Sheet with two columns:\n",
    "\n",
    "- **Snippet**: A short customer support interaction\n",
    "- **Recommendation**: A suggestion for how the agent should respond\n",
    "\n",
    "This keeps things simple and collaborative. Anyone can edit the sheet, no database setup required.  \n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "#### üîç Why This Format?\n",
    "\n",
    "This setup works well for tasks like:\n",
    "\n",
    "- `Input snippet ‚Üí Suggested reply`\n",
    "- `Prompt ‚Üí Rewrite`\n",
    "- `Bug report ‚Üí Diagnosis`\n",
    "- `Text ‚Üí Label or Category`\n",
    "\n",
    "Just collect examples in a spreadsheet, and you‚Äôve got usable training data.  \n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "#### ‚úÖ What You'll Learn\n",
    "\n",
    "We‚Äôll show how to:\n",
    "\n",
    "1. Load the Google Sheet into your notebook\n",
    "2. Format it into a dataset\n",
    "3. Use it to train or prompt an LLM\n",
    "\n",
    "\n",
    "The chat template for granite-4 look like this:\n",
    "```\n",
    "<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\n",
    "Today's Date: June 24, 2025.\n",
    "You are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>user<|end_of_role|>How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|end_of_text|>\n",
    "\n",
    "<|start_of_role|>assistant<|end_of_role|>Astronomers make use of the unique spectral fingerprints of elements found in stars...<|end_of_text|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3fa698da54b44110851e375a4776b9cf",
      "ad4ef84883f046678d12f86e19cd6522",
      "a5fef1f43f754475a7a28c745c6cef11",
      "08438adcd889463b97752f567e0cdf66",
      "a303da9f6ab04063aeefb944842e2468",
      "bf4a2842bf80493e8a1e302bd37eadd0",
      "f66d3a937f6e41d3b73fa3c18d07b109",
      "7ce72dd2edb148d3bb4abac5a9434278",
      "4cfd962be8814fea84dc3072bf3c0780",
      "94fee79235e74eb0b4f44c770341f217",
      "b1c689f8338b407994076f3fafd5d5ed",
      "20df73d4b9664f6aa4368eceed791dcd",
      "622fc5748fcf4362af80591942eb5fa5",
      "25ae5d06a87d4180b1ac50273af44706",
      "452922814d8c455da22904615575e112",
      "1f33f7d575504a748c31f91f37d81747",
      "117dd4da6e4f4d359eba9c648c8d8469",
      "7170c9ec44744056b53499c000616b8a",
      "8d7bb61120dc408faf77b26322839b5f",
      "7aeaff8ed3f343d987af85c57b669e44",
      "e6c152d1d50c4832877989a682328c41",
      "c77c7a6ffa2c4697817a22eb86961794"
     ]
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "ab255509-02a5-4c21-f394-12f0e819808d"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Use the below shared sheet\n",
    "# sheet_url = 'https://docs.google.com/spreadsheets/d/1NrjI5AGNIwRtKTAse5TW_hWq2CwAS03qCHif6vaaRh0/export?format=csv&gid=0'\n",
    "\n",
    "# Or unsloth/Support-Bot-Recommendation\n",
    "sheet_url = \"https://huggingface.co/datasets/unsloth/Support-Bot-Recommendation/raw/main/support_recs.csv\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"train\": sheet_url},\n",
    "    column_names=[\"snippet\", \"recommendation\"], # Replace with the actual column names of your sheet\n",
    "    skiprows=1  # skip header rows\n",
    ")[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We've just loaded the Google Sheet as a csv style Dataset, but we still need to format it into conversational style like below and then apply the chat template.\n",
    "\n",
    "```\n",
    "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
    "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
    "```\n",
    "\n",
    "We'll use a helper function `formatting_prompts_func` to do both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ce3f0cdce1d8498bb56bc08e6488bb09",
      "eefe235fac474eb2a6cd8beb6c117e6e",
      "10093d60538a4c4c88934ef18e50b83a",
      "d33fa7f870714b978bcdad129f491539",
      "97a3b51c01c54ee7b194e662849c2cfa",
      "fcb2e5be8b954cb68c43b59c9322ea72",
      "3ea59cc40bad40feb2a7d9a431527f7b",
      "d6651b484e384165b805946432517c37",
      "14b6d36089724a53a11d9233965f645a",
      "992e358daeb24357aef638840f511e1b",
      "a39cf55355fc44a08e3aeb1d7fe451fd"
     ]
    },
    "id": "reoBXmAn7HlN",
    "outputId": "68f958fb-a779-4aea-e8fa-a06e3d7d4a4f"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    user_texts = examples['snippet']\n",
    "    response_texts = examples['recommendation']\n",
    "    messages = [\n",
    "        [{\"role\": \"user\", \"content\": user_text},\n",
    "        {\"role\": \"assistant\", \"content\": response_text}] for user_text, response_text in zip(user_texts, response_texts)\n",
    "    ]\n",
    "    texts = [tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = False) for message in messages]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "We now look at the raw input data before formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "bf4dfb13-417a-4c88-db64-cc85cd23f5ca"
   },
   "outputs": [],
   "source": [
    "dataset[5][\"snippet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "LA-aC7w-x72-",
    "outputId": "dbae9908-d9dd-4fa6-8d9e-34f491390f42"
   },
   "outputs": [],
   "source": [
    "dataset[5]['recommendation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q76PqXv9yKa0"
   },
   "source": [
    "And we see how the chat template transformed these conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "du1MB2NqyGW4",
    "outputId": "9c4875fb-c5e2-4fc6-e1d1-b9ca057deeb8"
   },
   "outputs": [],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5549476a33e64d1586186e56dfbe2dc3",
      "989e1fe05ae6479aad1c1b60b48def9d",
      "c959e8ff2bf44b42a908259ba0f8baa5",
      "685a54e7424b4cdda074496ccb409543",
      "a7d7ad29c8294c28b2f359926fa3d50c",
      "a447e0938c0d4571a9d78571a3247499",
      "5137eb8fecb648c38c6ccf1258d23861",
      "20bb1067f6ea4ea4b82003c250c2a315",
      "ec2a964e3f9e499cb71b14b5cf76422e",
      "5d9e2ca5a2064510b153d17a7681240b",
      "640993dccbbf48008e5b356c29620cb7"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "0b1beeba-e3c6-49ab-cdd3-764324eabd56"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7ae9a574815546e6b736e61402eda4c7",
      "28e15588d1924dd0ae630e54cbac2e3f",
      "8770266042fe44de9a895fb52b2b785a",
      "f4b8967e582048188172775f14310446",
      "9feaee4d2d784131a06c77c444589118",
      "77aae93d7b234153ada66f280d1e6642",
      "fe349820038740bb8d5cfdb420a936a4",
      "60f1a21db7b54347b0d5d726b30e6278",
      "889fd38cb33e4dd9bffae5f2818a3c94",
      "4a999f8d59564f45b1be3c800293aa36",
      "74938bc3b8ab4820939fab208780ce42"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "df85d9a6-658a-4146-e545-33b57a850461"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_of_role|>user|end_of_role|>\",\n",
    "    response_part = \"<|start_of_role|>assistant<|end_of_role|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "LtsMVtlkUhja",
    "outputId": "203e30a8-84a9-46d5-ac9b-0e3c58b14878"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "f0eb4d77-1402-4775-92bd-afd301f7bff4"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "5519aaec-b79a-42e1-acd0-b10e697dd62c"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`\n",
    "\n",
    "```\n",
    "Notice you might have to wait ~10 minutes for the Mamba kernels to compile! Please be patient!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "1def9272-54d7-4cf4-fba9-4cee553f7503"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "1603364b-c7ee-4e06-d48f-bd2b01bfa6e4"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! We'll use some example snippets not contained in our training data to get a sense of what was learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gk4A3U4l-SjD"
   },
   "outputs": [],
   "source": [
    "# @title Test Scenarios\n",
    "# --- Scenario 1: Video-Conferencing Screen-Share Bug (11 turns) ---\n",
    "scenario_1 = \"\"\"\n",
    "User: Everyone in my meeting just sees a black screen when I share.\n",
    "Agent: Sorry about that‚Äîare you sharing a window or your entire screen?\n",
    "User: Entire screen on macOS Sonoma.\n",
    "Agent: Thanks. Do you have ‚ÄúEnable hardware acceleration‚Äù toggled on in Settings ‚Üí Video?\n",
    "User: Yeah, that switch is on.\n",
    "Agent: Could you try toggling it off and start a quick test share?\n",
    "User: Did that‚Äîstill black for attendees.\n",
    "Agent: Understood. Are you on the desktop app v5.4.2 or the browser client?\n",
    "User: Desktop v5.4.2‚Äîjust updated this morning.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 2: Smart-Lock Low-Battery Loop (9 turns) ---\n",
    "scenario_2 = \"\"\"\n",
    "User: I changed the batteries, but the lock app still says 5 % and won‚Äôt auto-lock.\n",
    "Agent: Let‚Äôs check firmware. In the app, go to Settings ‚Üí Device Info‚Äîwhat version shows?\n",
    "User: 3.18.0-alpha.\n",
    "Agent: Latest stable is 3.17.5. Did you enroll in the beta program?\n",
    "User: I might have months ago.\n",
    "Agent: Beta builds sometimes misreport battery. Remove one battery, wait ten seconds, reinsert, and watch the LED pattern.\n",
    "User: LED blinks blue twice, then red once.\n",
    "Agent: That blink code means ‚Äúconfig mismatch.‚Äù Do you still have the old batteries handy?\n",
    "User: Tossed them already.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 3: Accounting SaaS ‚Äî Corrupted Invoice Export (10 turns) ---\n",
    "scenario_3 = \"\"\"\n",
    "User: Every invoice I download today opens as a blank PDF.\n",
    "Agent: Is this happening to historic invoices, new ones, or both?\n",
    "User: Both. Anything I export is 0 bytes.\n",
    "Agent: Are you exporting through ‚ÄúBulk Actions‚Äù or individual invoice pages?\n",
    "User: Individual pages.\n",
    "Agent: Which browser/OS combo?\n",
    "User: Chrome on Windows 11, latest update.\n",
    "Agent: We released a new PDF renderer at 10 a.m. UTC. Could you try Edge quickly, just to rule out a caching quirk?\n",
    "User: Tried Edge‚Äîsame zero-byte file.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 4: Fitness-Tracker App ‚Äî Stuck Step Count (8 turns) ---\n",
    "scenario_4 = \"\"\"\n",
    "User: My step count has been frozen at 4,237 since last night.\n",
    "Agent: Which phone are you syncing with?\n",
    "User: iPhone 15, iOS 17.5.\n",
    "Agent: In the Health Permissions screen, does ‚ÄúMotion & Fitness‚Äù show as ON?\n",
    "User: Yes, it‚Äôs toggled on.\n",
    "Agent: When you pull down to refresh the dashboard, does the sync spinner appear?\n",
    "User: Spinner flashes for a second, then nothing changes.\n",
    "\"\"\"\n",
    "\n",
    "# --- Scenario 5: Online-Course Platform ‚Äî Quiz Submission Error (12 turns) ---\n",
    "scenario_5 = \"\"\"\n",
    "User: My quiz submits but then shows ‚ÄúUnknown grading error‚Äù and resets the answers.\n",
    "Agent: Which course and quiz name?\n",
    "User: History 301, Unit 2 Quiz.\n",
    "Agent: Do you notice a red banner or any code like GR-### in the corner?\n",
    "User: Banner says ‚ÄúGR-412‚Äù.\n",
    "Agent: That code points to answer-payload size. Were you pasting images or long text into any answers?\n",
    "User: Maybe a long essay‚Äîabout 800 words in Question 5.\n",
    "Agent: Are you on a laptop or mobile?\n",
    "User: Laptop, Safari on macOS.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "c3b9b781-246c-4773-ad42-c102c7e77e39"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": scenario_1},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
    "\n",
    "_ = model.generate(**inputs,\n",
    "                   streamer = text_streamer,\n",
    "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
    "                   use_cache = True,\n",
    "                   # Adjust the sampling params to your preference\n",
    "                   do_sample=True,\n",
    "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGSmEnAd-sOP",
    "outputId": "eaf60a1e-db15-451e-b84f-e56f898a29ac"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": scenario_2},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    padding = True,\n",
    "    return_tensors = \"pt\",\n",
    "    return_dict=True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = False)\n",
    "\n",
    "_ = model.generate(**inputs,\n",
    "                   streamer = text_streamer,\n",
    "                   max_new_tokens = 512, # Increase if tokens are getting cut off\n",
    "                   use_cache = True,\n",
    "                   # Adjust the sampling params to your preference\n",
    "                   do_sample=False,\n",
    "                   temperature = 0.7, top_p = 0.8, top_k = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "3c31451d-33c2-4a6f-d4c3-455de8f45157"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgcJIhJ0I_es"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: # Pushing to HF Hub\n",
    "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzmjlc3gzJVs"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
