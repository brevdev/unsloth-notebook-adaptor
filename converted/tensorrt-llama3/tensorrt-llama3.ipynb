{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1de996",
   "metadata": {},
   "source": [
    "# ðŸ¤™ Tensorrt Llama3 on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>âš¡ Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/tensorrt-llama3.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## ðŸ“‹ Configuration\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "\n",
    "| Parameter | Value |\n",
    "|:----------|:------|\n",
    "| **Model** | Tensorrt Llama3 |\n",
    "| **Recommended GPU** | L4 |\n",
    "| **Min VRAM** | 16 GB |\n",
    "| **Batch Size** | 2 |\n",
    "| **Categories** | fine-tuning |\n",
    "\n",
    "</div>\n",
    "\n",
    "## ðŸ”§ Key Adaptations for Brev\n",
    "\n",
    "- âœ… Replaced Colab-specific installation with conda-based Unsloth\n",
    "- âœ… Converted magic commands to subprocess calls\n",
    "- âœ… Removed Google Drive dependencies\n",
    "- âœ… Updated paths from `/workspace/` to `/workspace/`\n",
    "- âœ… Added `device_map=\"auto\"` for multi-GPU support\n",
    "- âœ… Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## ðŸ“š Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/tensorrt-llama3.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d1de5",
   "metadata": {},
   "source": [
    "# Deploy Llama3 with TensorRT-LLM\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook, we will walk through on converting Mistral into the TensorRT format. TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.\n",
    "\n",
    "Once the TensorRT engine is build, you can use the run.py script provided at the end of this notebook or use this engine as in input to the Triton Inference Server. \n",
    "\n",
    "See the [Github repo](https://github.com/NVIDIA/TensorRT-LLM) for more examples and documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b67443",
   "metadata": {},
   "source": [
    "### Step 1 - Install TensorRT-LLM\n",
    "\n",
    "We first install TensorRT-LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39eb57b-2e6a-448c-b0ac-f9c065e486ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bc778-169c-4d3d-8836-2d1afdce48f6",
   "metadata": {},
   "source": [
    "### Step 2 - Download Llama3 model weights\n",
    "\n",
    "Llama3 is a gated model which means you'll need to request approval on their respository and generate a HF token. This usually takes about 20 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e7c94-6acf-4562-bb8f-c2b6bbb52dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b31d9-d6cf-40ec-8132-569b498ba34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(\"<ENTER TOKEN HERE>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d824340-b183-4cea-bc95-d2d7592ce349",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.snapshot_download(\"meta-llama/Meta-Llama-3-8B-Instruct\", local_dir=\"llama3-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333ef85-a14e-485d-aea0-2f7f055f3dc3",
   "metadata": {},
   "source": [
    "### Step 3 - Convert checkpoints into safetensors and build the TRT engine\n",
    "\n",
    "There are 2 substeps here. The first is converting the raw huggingface model into safetensors which is a safe and fast format for storing tensors. \n",
    "\n",
    "Next we build the TensorRT engine. This is where the magic happens. We take the converted safetensors model and convert it into a `TensorRT engine`. Engines are optimized versions of models built to run lightening fast on the current machine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f5d76b-87a1-4275-9205-ec811676a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['wget -L https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/llama/convert_checkpoint.py'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e8d92-c303-4cff-b33e-da9e51eac3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['python convert_checkpoint.py --model_dir llama3-hf --output_dir ./llama3-safetensors --dtype bfloat16'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fe149-a330-4d8b-a2c7-af915c8cb1b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['trtllm-build --checkpoint_dir llama3-safetensors --output_dir ./llama3engine_bf16_1gpu --gpt_attention_plugin bfloat16 --gemm_plugin bfloat16'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582f71d-5d83-4599-a78a-fc160ef9b47e",
   "metadata": {},
   "source": [
    "### Step 4 - Run the model using the example script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8fb1d-addf-48bb-aac3-bd09cc1ffa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['git clone https://github.com/NVIDIA/TensorRT-LLM.git'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c03ec-aca1-40fe-8532-3d903314cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.run(['python ./TensorRT-LLM/examples/run.py --engine_dir=llama3engine_bf16_1gpu --max_output_len 100 --tokenizer_dir llama3-hf --input_text \"How do I count to nine in French?\"'], check=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
