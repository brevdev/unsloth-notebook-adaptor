{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef9a036",
   "metadata": {},
   "source": [
    "# ü§ô Codeforces Cot Finetune For Reasoning On Codeforces on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Codeforces Cot Finetune For Reasoning On Codeforces</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "\n",
    "# Create cache directories with proper permissions\n",
    "for cache_dir in [cache_base, triton_cache, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o755, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 32768 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-Coder-0.5B-Instruct\",\n",
    "    \"unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "    \"unsloth/Qwen2.5-Coder-3B-Instruct\",\n",
    "    \"unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-Coder-14B-Instruct\",\n",
    "    \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit\",\n",
    "\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "<a name=\"Data\"></a>\n",
    "### Dataset\n",
    "\n",
    "We are using the [`open-r1/codeforces-cots`](https://huggingface.co/datasets/open-r1/codeforces-cots/viewer/solutions_decontaminated?views%5B%5D=solutions_decontaminated) by the Hugging Face Open R1 project. This is a dataset of problems for a competitive coding challenge.\n",
    "\n",
    "The Open R1 team have found that models trained to reason on competive coding tasks like this outperform other models on coding benchmarks.\n",
    "\n",
    "\n",
    "subprocess.run(['[image](https://huggingface.co/open-r1/OlympicCoder-7B/resolve/main/lcb-evals.png)'], check=True, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "837117c4d55042b7bb5628c498391d7a",
      "2fe72d46e4294379914e04a7ec489aea",
      "474bc14fff814858ae1b1dcc58856437",
      "0ac3b42ccc5b47cead2f6f0ee2081a5b",
      "88b2132dbcc54346b38651b8454b970a",
      "b3994668c32b484789857b65d0d4546a",
      "ac062ea3fa7741049523ea7f3c4ab8f7",
      "61f66b04e02b4c80b47bc37df00d45a6",
      "4b5e3b9379524304af5f1500539f8023",
      "ae8b3a02550741e09549d03dc7ebd73a",
      "12df1a2ad8f4434d9c1e5f53c05b428e",
      "d2e649f7dbba4308b69af0c6bf3dc723",
      "548b3877dd8f45b5a289ef19c2277b9b",
      "953e18a621ae4d0dbca4920b23442985",
      "f964ae08ae0e43f0b9afa9d8ae03f059",
      "886c66379d834d398ee16c07556b4d13",
      "cea2982abe34432b8b36608ba169f37e",
      "5da83529046a4a89885323fb100c24d9",
      "a9c6a6dae2b344199c4f8b9e2be64c1c",
      "2d790ca616ac4265a8309d2caca02ca8",
      "35c3e7878e58458480492b2f7cda6ec2",
      "5ef9f87e551e4254b211c443425aa128",
      "972db6b994d04e218f3b102b8a702f9b",
      "c1654690256b422595ce23df725d7f58",
      "e270f4765e3f4fcaa74180c1252a3f45",
      "7d510433d4ee46eb8b3a19839102d2a4",
      "14dd294ad2bf43ee8a1463cad677d65e",
      "3634ca6a3e764f0aa5d4e18ce5299ba8",
      "9f5ab63f67934814bacf2a350d55e742",
      "71506af4519146c0a1162f6e51c79e88",
      "7a5328bd126a4b1bb830806ee3d7bae5",
      "57dc5ca77cdc475b80f8e1840b0c3971",
      "96b9876b8a014b5d938867637be6cc33",
      "dd2e2b3b1a764ebc858622bf1aa272ac",
      "18b4fd083f7846a7b6042eda4b6f1693",
      "5667ea8628e34d73ba2e1a67cedddd5f",
      "ebd4ad0def3048758ec4da453264ea5a",
      "b748be1a912d4fa98d8ef86dd4dcc980",
      "8a0781f90677458ab2fdddf128974ef0",
      "e559f9d4331745fe84f15a1471819030",
      "18b345ab4fce499a8065426277ec895e",
      "5f52106a4abc43518730bcd0a113eda0",
      "cf263dd799b04683a06d8538bb8376ba",
      "9df5437edc3b49b592be205211f4abc9",
      "cf945ca040294d5f9b1fd7b54f770f23",
      "32803e92ca0f409186eede2a5e4b137e",
      "aef38e1009624b1c87ff634af746ea21",
      "b12d52957016467d82095f6bd06cc60e",
      "49aa3ffbc69a4e509ad422b61ad46395",
      "5ea702649d0545359fefe213f04c6b88",
      "2d720698354e4cba8f74f7e6c8de2750",
      "25ed6259da5a48918a5bbe7a4e7fda7e",
      "a5728486457a4a4aaee8524cf53b8872",
      "9b0130c0801241cfa0ccc9aef052ff3f",
      "2aaafa1cec6f4e94b03e5970b68b5f8c",
      "748d04269417415aa7725653b6957017",
      "ab10f167d2444580a14d686530fda9db",
      "dbfd46e764ec4f16af62826b52198e17",
      "f46d7ccfb290463c9a9c0cc03d91f769",
      "14d9c4fbe9094138a8b54ccfb1bfcce4",
      "f177e0648cf64e9fbfd80cf42a0b1208",
      "441e4d603fae44059dec3530e4ca176a",
      "33fddcbbcd29453dbf33e383decc7e58",
      "abf13e2e63144ec39f8a5fda5da12a74",
      "69e9f0e1801d49428e2d526d75fe6d3a",
      "77f9b1cf33494e74a88a041fa5955f7b",
      "c3073f6b3df64cfaa4d9d495e131bb59",
      "78efe5ffdcac4e88a4760f62bb3c4bd1",
      "5e9a0c855c2c4f52a6d290f073109622",
      "439e53221d1146e1b43b80ffe6d32cfd",
      "7fd5021b05fe47038adcde4b928fc285",
      "00747555e4dd4948ba0ea78a1b984b05",
      "7b515bedd0cf44cc93ea1d7a1b725115",
      "0390ff4e81fe4ba9a232a176ba0ccb20",
      "b6c6bf3a058b4555b28bb00d899be56a",
      "310f07982c1d4eddb7c31aa982de23eb",
      "2e8f4b9e8f3b4e9b8ee141f9e15ad74b",
      "8a4d647f2b2d45cc8ab6281c77442171",
      "cb1621c3f1c14f79aa91f2a5aa6dce6d",
      "ae61b5ec923344069e42c07d6f5a3fb0",
      "56c050a514864f7a93370649192c06d1",
      "3ee680f5f7b546e6b8f478e76c51e85c",
      "65e3634af18d407eaf75cb2120beb89b",
      "49d8dcce735e427087bf294b026d1dd9",
      "27405e7ea1764ac7ad550ed8efffd7cd",
      "849a6266b89543f197d2f7b28f4b657f",
      "7ffc77c350f841d08c4a7cd5c7dfe4bc",
      "8e1ff1c109dd46dc880b66d446fcd9fe",
      "0addc708a8164de9bcbdcff4d31520fb",
      "009e62115d41493fb9892ddf6df9653c",
      "40e39add40364fed9a64d08608c4735d",
      "e80860dcb858450cb1b557e1b33d90c3",
      "549f33e29aff423084ed02a3cf7f93d6",
      "e53a1d9364054bd29d1746119744d34f",
      "79e4ead9667e43a095e292923de319ca",
      "71672032d686442388ed7bdab1ae2adb",
      "44131d1a7c774ae39dcb368cf8856915",
      "572bada9f31f4a06b4ee3952d1a6dbf7",
      "7bc46c5b9b974c5e84dc7ee4c9377cec",
      "41ab0e68f02b4afc8e7877bae7ae0e13",
      "e7e7de3073b647fba7ca6050cdc459f1",
      "8c1853363e9442aea51d0d42aa699c94",
      "26e850b9da9e410daa7e4f5111afe560",
      "5c7a4c5f03c14696a160d5a00db1775e",
      "4e8bccc43dba4142a03b65e5fe4f25f5",
      "a57561753b7845a3ba19ac1f8d28cdd8",
      "d5a4ead3c49f49059ed18e6e28784223",
      "910f2e363e344508b07f32d561cce2e6",
      "f91c3ba0d4c449e6a9d31d3fc2c8c8bd",
      "ffac2d758bc24144bb66067abe535b0e",
      "3632f1ab7caf41cd914754ce54c641b0",
      "12a7a5d583a1471b92b5cbb242c59dce",
      "7f34b428baf545cd88f6c9d655bbec4d",
      "a2841fd86e634f7aaba734d2116b71b4",
      "cbdeec319f55424a826cf8f922b96113",
      "949326fd05304021a3db146625510ad2",
      "5c0a8e18831840c5a7c34e126171c8a3",
      "09d2565d0afa4491959f5196e768fd9c",
      "e6da0151c68b472198597373bf2f0b42",
      "d4c41e99f09b46d7bc7b199424d4f317",
      "2ea61f6acc7d49d4864b3516083aed77",
      "9fcd2c84f6ca4ed88b1b4cf2a09422de",
      "5fb1c7ce789540f7afec509d36e73738",
      "89815984566c44d1a110085c398adbdb",
      "46c546e8a3b649cb81319b4ef5bc9361",
      "0b809aa10ef4402bb19f29c3568068c4",
      "45cf62b7a5db4a07a06056302dea3ef7",
      "7f0a41037c8f4b3982d0729c64d23a3a",
      "a65ee57bb8584a8f8f37fb8c681d4bbd",
      "d91fc50d3b9f44778d2af4ac8de8b683",
      "cc7bf941468a45059efdd9a98c2561d9",
      "b1ba1a3a87fb4bf99da96fe003fd8359",
      "44f1681ed0114877bd2d7bcdcc9e61a8",
      "40c3f2cdd26e499dad3b3e86ea0e391e",
      "d60a12d175584f1abaf3af6a3d4f0673",
      "6ed18d3b8efc40288058246987fd1ee9",
      "9dba7d0b9d8543e184a6c900103234b7",
      "619b515d37564e368253c14203be8e0a",
      "1daba34c063c4871b33a55e61bae0df0",
      "96f4eaefa6f14ff08301f1bcda5fbc35",
      "02896b919c1148b79f4ec6cc19434166",
      "4b8f5bd55a9c4c4c9aae2a330cfb9f05",
      "dbf66405cba24a23a3710a65992504f6",
      "f409527019b7460296ecd2c7ff144f31",
      "526f42fe23f44a98b15cde5283a4ddd5",
      "04598b5093334d33b411d31214f339fc",
      "6779e1487d2449508a9e4d32a4ddcd64",
      "93f3b3eaafa6482da175716bcda96965",
      "48126f45448446d08a21304e4e2057a7",
      "f989c5379ee04ab6bcb49c0d4d74f37e",
      "1e6da79b4f1b4f369bf590d544958ab1",
      "b6217192519b4d7eaf1aec265199e5c2",
      "59c6d35c053c41ab9d256de5478eaf44",
      "09ec7846e88b4e3987ca006e02055d0a",
      "b6f5fe04ad0647cf80dc6710086edd0d",
      "216c746011a244678035ae3243acbd75",
      "6883bc3c7b7d467ba49f08d3ab21fabc",
      "5a008701fd024af0aea09a3e900560fc",
      "1dcf723ccfa6482c89aa4096052c78b7",
      "f7db9dafeff149639f2cf07883d9cbda",
      "e6953cd8b9aa42c18b55a4e0f44e245d",
      "5e35d3ca0db64653bdbf3e4666eb5dc5",
      "d204f8ce961042d2af30f78e750a0317",
      "21638526a970492ebcf71a4b020ba5a7",
      "632aae02ca284c319da9793f6d00f31f",
      "30d40d6199d446bb9a447ec9974df732",
      "1f6ba042a3654639a39190688f45666c",
      "4fbb6b0af1964fb680d43db0d4857244",
      "6df1c87e80be4a6485a1f867b97f31b3",
      "d30c74a7b1ed4be5b658b53977688c28",
      "2d9778680beb4bbca3ef72d6dd4ed4f4",
      "557ada4598664af3b62acd9969eaa1c4",
      "81939aaf8ada47249e5ea3b292964dc4",
      "e15cbca3bc6d48fab2de2f5502362205",
      "11b026d360c94549b5b16b861f2e611c",
      "bd28187cbc8a42688625900efa5f6729"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "b7ababe1-80b3-4e15-99f0-6720d7dde3fd"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"open-r1/codeforces-cots\", \"solutions_decontaminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "pim3Grca-GIa",
    "outputId": "1e3aac57-352c-451d-d050-70bd7912d54e"
   },
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    ds[\"train\"][\"messages\"][0], tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPLgr6sxBmOY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['problem'])):\n",
    "        text = f\"### Problem: {example['problem'][i]}\\n ### Response: {example['generation'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e92eb62e284743b6af284638a8da591b",
      "0360fb88da70455f8dee3448741da9f6",
      "507ba8e73c8c451db278848ee054e130",
      "c6f3eb06ddb945e78916ae91482ff0ce",
      "1470b5ec997b42aba820870ae023e169",
      "85dc6838069f40b4a8db958765eb413c",
      "c43a088652a34343bdc6899cb7ee48f1",
      "51794cec056440b4ab0fd9215c2004b5",
      "271e98a654ba4c02b06083d00bf664d1",
      "a1525ae5572549e7bb6272d912ba554d",
      "590c10a5c3da4db6bd1dfb767632fe82"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "5e67e3b1-efef-44dd-b027-895c02903b1c"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds[\"train\"],\n",
    "    max_seq_length = max_seq_length,\n",
    "    formatting_func = formatting_prompts_func,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "83919595-69a9-42c4-8ef3-130533a68257"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "2e4fa5ba-c9ba-4b59-d5c4-ba723c799992"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "3005b362-2b86-468a-8f28-8effd9208566"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "ef815e2b-2095-4c27-d2af-a6d3007a8f68"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "example_problem = \"\"\"A tech company, \"DataGoSlow,\" produces high-performance memory modules. They sell these modules in 'n' different retail outlets. Each outlet sells the modules at a unique price, represented by 'xi' dollars.\n",
    "\n",
    "A large data center plans to purchase these memory modules over 'q' consecutive procurement cycles. For each cycle, the data center has a budget constraint 'mi' dollars. They want to know how many outlets sell the memory modules at a price *strictly less than* their budget.\n",
    "\n",
    "**Constraints:**\n",
    "\n",
    "* Time limit per test: 2.0 seconds\n",
    "* Memory limit per test: 256.0 megabytes\n",
    "\n",
    "**Input Format:**\n",
    "\n",
    "* The first line of the input contains a single integer 'n' (1 ‚â§ n ‚â§ 100 000) ‚Äî the number of retail outlets.\n",
    "* The second line contains 'n' integers 'xi' (1 ‚â§ xi ‚â§ 100 000) ‚Äî prices of the memory modules in each outlet.\n",
    "* The third line contains a single integer 'q' (1 ‚â§ q ‚â§ 100 000) ‚Äî the number of procurement cycles.\n",
    "* Then follow 'q' lines, each containing one integer 'mi' (1 ‚â§ mi ‚â§ 109) ‚Äî the budget for the i-th procurement cycle.\n",
    "\n",
    "**Output Format:**\n",
    "\n",
    "* Print 'q' integers. The i-th of them should be equal to the number of outlets where the memory modules can be purchased at a price *strictly less than* the budget for the i-th cycle.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```input\n",
    "5\n",
    "3 10 8 6 11\n",
    "4\n",
    "1\n",
    "10\n",
    "3\n",
    "11\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer([example_problem], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "187a7d22-bde7-439a-b1b0-a3e0dc308843"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "0995f3eb-3eb9-423f-ede2-3b7f2f9f10c9"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "0412171f-a9b4-4018-a4aa-d9725d2b8201"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    device_map=\"auto\")\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"What is a famous tall tower in Paris?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKl34TVyjv-"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
