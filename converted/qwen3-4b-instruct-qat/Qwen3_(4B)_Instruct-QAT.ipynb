{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c8e58f",
   "metadata": {},
   "source": [
    "# ü§ô Qwen3 (4B) Instruct Qat on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(4B)_Instruct-QAT.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Qwen3 (4B) Instruct Qat</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(4B)_Instruct-QAT.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577,
     "referenced_widgets": [
      "16d9751a90b7435d9c900ad6754ea02a",
      "12960b8629ef4d43ac58770b3ad2cb8e",
      "d5ac21ecf53146b9a91e77c1365a8087",
      "affc3e6798984ab7bee3b100e9bf07f3",
      "091ea70a8a4c43a58ba0e804fb35231a",
      "d027fe1c1c874b00b35d1aa728e2f64b",
      "c050f260067d4a8cb0a48b8789853e42",
      "b284c29f86754ee191d4c8c9270a581a",
      "c85614534d6c48ff9646cbd2508e1eb1",
      "79ac14919f7c47de914f634481c7c47a",
      "611831076b59469fbcfcacdec4ac4bea",
      "f5279e94f100416e858be5122226ef53",
      "eeec35c7bba7474ea1c52cdf2ea1c24a",
      "b021cc01ce3748d48aa7fc32843c51f9",
      "a8064d412b224deb8c97ad384402baa7",
      "cded2220150c4bde98c63ccaba8e6137",
      "c5be281628bd49f0b184cd899143c2c4",
      "4d4f8726c88d40bcabdb664a663cbcc8",
      "d951846ecc3d4de4820a09c8d414000f",
      "5826d07c16f949bf9839a11abea99827",
      "61c268a7d13a44afa56bedade90e4f34",
      "272afad7361c4eeab631c7bd41e6fa14",
      "d38043a7e0ce4eeabd900a7ad01fb733",
      "455ac00a911b4a6eb66b320b111d2cc5",
      "0efab72ae1a747a19d8952d2e756eccc",
      "5396691d92a64a91b94e8eb3d0f3fcd4",
      "9c2cdd2b76fb4395a2722c1d3f5e1fdd",
      "4b78a2ab29ba40bb98e18bf3beb1181e",
      "5d6d1d2d76dd45d4a6ec9d1cc067968c",
      "d97f83b016ed4a3291d8a56162b46ee9",
      "64b11ea2321a49e8b71e9cde9ec63ac1",
      "c88063243cb0423db943d5c14b53b799",
      "af75f290b4e1431681085b82b894129c",
      "2bbac7b919944102a78bfee0edd30956",
      "33955961c17b466fb4ea25b1977a794d",
      "93df10c2f9a843389925e07b4b88f333",
      "408db6b200ed4f79a54794b97ed3b738",
      "708aef9e376047dd98a436d0c97f1b2b",
      "c8de0d52dd214fd283bf3d2ff9c50bd7",
      "a0f7475651e84e02afd306a2f6d8cb51",
      "939d69d9c8b946c2aed1a7af665093bf",
      "5e9835c8c90a45f2ba8d552a3515a11c",
      "b7f2a83d93994f83b4163627a6619c72",
      "47586e1fc8cb454e84d447d3852fb8fc",
      "e03cb045a6a940c4b1ad2a01b7de41a4",
      "daa62a989c234b7e9e7d5dea7f52eac1",
      "f81c5ed93c684019b289c7a01ec0f177",
      "3e167abc6f7443ea87acb63044214ecc",
      "1e47ee7466d94c56a1ed2501a0acbdc5",
      "a8f346bd2311473bb41f9c832d38fde3",
      "27080c04e0f44a21986825766793e12a",
      "8e0aa0bf9fdf456bae3ff6dae9825eac",
      "252c528b8d684258a095c14b0edb39e8",
      "36986d45f3f342f494287555aab6d79c",
      "7ad06c11c12e4d12bb8567f46e005940",
      "ad7f6b4ab99e4f0a96724495d08c1fdf",
      "89039d6732af48a5a303967bedf5ecb0",
      "0a15f05948e248cc8dc3051c17b1232e",
      "0592ffbb979f429fbb1bb1693b3dfd21",
      "a9eb249e5731485a84404784206c580f",
      "598cdec8e8dd49a0bec21b74394ad47d",
      "da70610720a14a1ca0e689850a081809",
      "60b372b2ef6945bfbcfa6e19027f85a2",
      "c295a07668554a949d190085067bbdde",
      "c168576851f240a9a6f0c9de5045ea76",
      "6f4d50b5f5db4d27a3219b6a83560542",
      "ff46988e19d747f38365c774431c9c84",
      "7c3e8fede34b44abaee97ba21f806c02",
      "c1e1fc4c5a0544659679637ba7d6a8a3",
      "ab4e08d51c3948c0a7c546560e7c36a7",
      "427fd3e792df4317b4e4499ec5a965c6",
      "a1cba3ed46c34f2ebfaf2b1dbb72d0d3",
      "573ad1a363b746699575ad8d77592312",
      "1df5ba9bde4d48bbbe370d923bae9a92",
      "fb50a4a2e7554af0b2952c871e9307a6",
      "c993d2d141094cf0b7151a8dbffd9a45",
      "3d9701a29b684fb0ac32535accf25ccb",
      "36c3916de5f24a2ca57bfcf70d7e2d0e",
      "667cdabc8a2b47baaac62ef642b6275d",
      "f9ec6d0aa0ac4f98a6b5992334defc6f",
      "f81b734bbe184ad7a4fa8e05862f878b",
      "52fac0f7c65343b59f258e94352656f1",
      "f72b1f631cc6458dbbdbc4fc3642133f",
      "c0a0d7c3181442c69fd330ed1452dd84",
      "447dafc3293b436899a6c3d51345943a",
      "50dc3b245d9c44d7b3ca81d961f3cd1a",
      "577decf0054b409a95ff0a0bf23b73ed",
      "cce32a3c7860476690a0e5762f22b1ab",
      "b50a13dc8d0d499eb54edbfaf72b8f73",
      "e63c4b5d40ef448b80c12b7d7cd011d7",
      "3946fc91b02448c9b5d5839f70986ba1",
      "97bacfb1a1ad4fed9345f4861f91d72e",
      "9a89009072e647b2b760688428c76e06",
      "6eecd84361254cf6973f16e70265707b",
      "0fa3c3feb88042afbd921a51c91353ee",
      "f86bfb3c2db34d8c8cf382998ac32ae8",
      "3d30d8ba6c874eaa92c1159daef2ce2b",
      "40b609ebe3c2437a9ec895e48de6d1ca",
      "6e7a0eb4f9e340bd9883bc975c6a489c",
      "6f80c13524174d26a65df025bf58b5b7",
      "c5b9d4b9635049c2a6ad03268955a033",
      "1d0eea905a954cd98b15bb606fb60ccd",
      "940a8ab0aa8a41318b836aee99bfb19d",
      "cb844e0da31c47368c128258456df717",
      "1ea6c960f5d3449eb445ba26b226ab80",
      "b7a3d157a73644c899f1e4d60678d9c3",
      "1c052efed432458ca1480a983818638a",
      "8fd82f125e74446880c3a64cff794aa8",
      "6226ba2708f54d3682c2257422299b61",
      "5b790390fed14b339b58be310eaf5b7b",
      "0658d6cd875e4ddfbcaec38acfc6ab8e",
      "56443ad69c2a495084c90d0d9ad84408",
      "e096fecd3fad4bd4aa72a5e7fdcb9bd2",
      "afb3595a56ad4a9980fabd546695d3f3",
      "81089768f7d74390b7f1733940fa4b62",
      "63a2c8a60c494acd87b00b4cfdff5c68",
      "adbddacbb5414d24a37c1a4a0f036547",
      "23182fb1edd248259238b5eae55811e2",
      "b055daa576e84beabbc92afbd313f709",
      "cc8af4f890a54538ba6aae19866dc7c0",
      "803af90c303241edbbca3338dd4f043d",
      "02c6f324e66b49d18ae7c96791778703",
      "564ec00e94eb477785335399eb492556",
      "a556562d0d7248f1b5096d4534020a00",
      "4668e8fbb9b14bb7acf9a2f312b97b10",
      "67c3227617ea4f72a5a3a9e822bb8abb",
      "1aba00c5303e49d88c4bc8987c8e2b03",
      "bf150441841847489593ab9cffc385ac",
      "31013a451a4d497d8c6e04ed2304870b",
      "48e757aacca14eb78c257ea7bf6e1620",
      "54bb3cb7e89843e881366b277107f10e",
      "b48febda135143208ab19ddbc54db83f"
     ]
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "fbd90eda-42a6-4856-922f-5d03b6e99d06"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "    \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Phi-4\",\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Instruct-2507\",\n",
    "    max_seq_length = 2048,   # Choose any for long context!\n",
    "    load_in_4bit = False,    # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False,    # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "0934a7b7-613f-4a2a-9b81-38ca0f0e0f8e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    qat_scheme = \"int4\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bwp4hfqQ2PWO"
   },
   "source": [
    "Lets check if QAT is applied!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnbMXON92Ma-",
    "outputId": "6e299710-98a8-428d-c61f-5912ca9a68fa"
   },
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "    if \"FakeQuantized\" in module.__class__.__name__:\n",
    "        print(\"QAT is applied!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Qwen-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Qwen-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|im_start|>user\n",
    "Hello!<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Hey there!<|im_end|>\n",
    "\n",
    "```\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTXnFWbg_OIZ"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen3-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "15493837f6ed4859a856749581e4bc94",
      "e897e07ca0124dbe86f1cc107a42c2ea",
      "ec4ba57d62ff4ea6b03140c32d0342c2",
      "28f8f4afb2a04b88a2bcbac5a78d7a45",
      "64e215feaf43462486a4300b13c9bd2d",
      "3ff4a0687fdf44fa95afe9ec0ab781ea",
      "a461d5efcb48465eb6c1b39a7d67fa07",
      "ee8278abfd7745908827b52aaf57729b",
      "63099867ebb1450c83dcd2881026cc6f",
      "74499d604e454b009f07f9900f010e4e",
      "3312b27851c64b50a3e52013b4285c13",
      "ea888f9b76b448208a11d6eee0c1eb81",
      "837e7cffc374417bb7496e430d9d521f",
      "155a4bb58c5b4b2d87403573c8007fd0",
      "dfe5475d5906460e9a771e0eeefdcd46",
      "ac2ceaa6d5fe41169557e673ccc83ec5",
      "613103c611fd412c815a37de1007dd3d",
      "78a08d6e960f43ad9b74f46c125b5aea",
      "0c864ffd6c37476db75bb6eeab65a4b0",
      "6b8055c09a6e4d95aa60682c847149e2",
      "7648f19accea41f5abec2f3d8296ac19",
      "a34d8312550e4c89a37dafb88a906dcd",
      "a6bb16ddc562422f9cbeff4108cbb6c3",
      "193efd1e6e1d4f9784c6b26e729e2690",
      "697bd3b6b0e64b80bcf194e16ae1284a",
      "a9593efe92f64505a6081737409a9d1d",
      "0fdfa95c965748848191416311f3ecde",
      "eed8825725c8493aa839ee1f662981a8",
      "873775127eff456e8418e310c10f68c6",
      "e01669badd9043e998951c212c8ff174",
      "a98adcf1d23547f599e01ab83bec8b48",
      "0e31ade8b833471a81309de8c821bce9",
      "88b7cea7ba0e42b7866bbfa4dc2df056"
     ]
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "1ef1d38f-19f9-4272-cc14-4a7b828440d9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e86016cd0c124ca78fe3119e59c94de8",
      "638b4f79fa53444d81d05b482e4a4253",
      "5538b91ba5654c3888b6d6831980cecb",
      "24c428f608764d20825ddda625388835",
      "b3c652b38c7a40418629e237bc28909d",
      "fc3356580e6744ca83b8649368480b3e",
      "5809cbdec1844cbebc0db710185d8072",
      "0297fab8f99f4de6a905c5e16ecf6d45",
      "0b52823d40ad420d86780cd0962c1cf7",
      "c2986f10f6a64a2fa7bd4fdb70e08e04",
      "9efb3a9c377c4ce3bf13038cd8cb9c63"
     ]
    },
    "id": "reoBXmAn7HlN",
    "outputId": "7ce2ae10-2ab7-4584-896a-3874e9ddbcb2"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "Let's see how row 100 looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "3417426f-9e55-45a8-a8d8-139cfb0feb47"
   },
   "outputs": [],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xs0LXio7rfd"
   },
   "source": [
    "We now have to apply the chat template for `Qwen-3` onto the conversations, and save it to `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "69daa83911fe4c80b28a22172fc3bcef",
      "d2b066684c56424e9a1194e2c88f331c",
      "e084e0336159458d8347347a60f7ddaa",
      "99c4b5e446c04bb4b0dd9d44d8565e9a",
      "aa62cf376c234ba4a15c877c563c7259",
      "592370835995436e8ce573f9f121b410",
      "24d4ad43783e4715b89a1bbd0a84b90c",
      "0bc64dea96664af7ae0e45ef4626c736",
      "a893ae8f4f954f78926d91a2a2b019de",
      "ba99aaf582914c83b18910ac713a9b6f",
      "94561bbb46cc4636ab21428a8a47f107"
     ]
    },
    "id": "1ahE8Ys37JDJ",
    "outputId": "c7ff1416-c287-480d-a69b-6fc0ff13991f"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "Let's see how the chat template did!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "AK86LIyn_OIa",
    "outputId": "d1e9ed3a-a63c-4ef8-cdf7-b19063f7b762"
   },
   "outputs": [],
   "source": [
    "dataset[100]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "499965024ab04a68a8734b9e4de72441",
      "a95fca80392b4af3aae5d189059dace6",
      "ed920d7f09c142ffbd5fedae28fda860",
      "4c00285f3e5f4034aeb2b92011d5148f",
      "998b761858c0484cbcd51d4c2ba7c075",
      "4641a1c322e94997a1d4e99ba4d29db4",
      "e2e1b4e39b7c44caaf698082cef96f20",
      "e0d414cf958d4fc29369453e4b60a7af",
      "20bcd302a7a64154badd8de51395fa97",
      "ea93bb20bbc44092ab0b1394cbd28d4e",
      "06d793c0fe3f46e29e279674de222b8f"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "08d056d3-4ec6-4fa1-8af3-e18b114d5bc4"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "854ab752c85449ad80b68ad7677df198",
      "9dece93213f04c58ad60be6f73e43506",
      "9c0f7c95e64c48cfbdcf5cd7ff21c798",
      "01b7afe5c6a648cab3c1708c4c4e811c",
      "5cdb5b4edefd4bd18b02016cc7057240",
      "fa846c3ef9124e6090530fd0ad72851e",
      "3ab7781489bc4bbe9b072fd49f7c12f8",
      "b4f10a3d68a44899af3576bad218b9bb",
      "d8551981e7fe4b789a4656ee4c0a4a6b",
      "5471cb70b9fc476e924a79bae6fbe973",
      "10ccee1030c94c05a299e4dbb646f26c"
     ]
    },
    "id": "juQiExuBG5Bt",
    "outputId": "0d85b12b-e4e4-4e21-f52f-129124a0c79e"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "LtsMVtlkUhja",
    "outputId": "eaa73230-ffaf-40fe-9ab2-9ea8a155f5a8"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "51314016-f230-442a-e744-7db14fb9e415"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "7f5e118c-81f7-4e73-f04c-79d9fca79d60"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "7fabec8c-117e-4e37-fe89-e072718a6b4e"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "c537d33b-641d-4cb8-e391-d4e4a13dcdb9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5AKpD2d2dBO"
   },
   "source": [
    "Now that training is complete, let's convert the FakeQuantizedLinear layers back to standard nn.Linear layers. This removes the fake quantization overhead and prepares the model for its final conversion step or for merging LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zrBZgnV2boS"
   },
   "outputs": [],
   "source": [
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.qat import QATConfig\n",
    "\n",
    "quantize_(model, QATConfig(step = \"convert\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for instruct inference are `temperature = 0.7, top_p = 0.8, top_k = 20`\n",
    "\n",
    "For reasoning chat based inference, `temperature = 0.6, top_p = 0.95, top_k = 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "eb19d999-d39c-4c7e-82cd-f365f791e29f"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1000, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "2a510d0c-c294-4205-c42a-ebb113a7ed9a"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q6VOEjBnzQo"
   },
   "source": [
    "We can now save and quantize the final model using TorchAO, applying the same configuration used during QAT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "a71eddcb7ef34c2686b58ec94af2ed40",
      "d4a54df75b8143afb3020abb4d3238cf",
      "6a7d02f76bb840a59e8d42a674ae1856",
      "210d2db911984c5ba9531653f74871fd",
      "6dfcb9745b2741b09fb6a919e298b71b",
      "a6c5b152ce8a4845aab8f4bd42bbc081",
      "2a86e8d4b9d64fc990de342af2359327",
      "54c24e7e44f84fb79803328635925d70",
      "984b8bee3e6e493e8bebaa177e9a8aa3",
      "1c977d30200d437abcc3bab5459ac3a0",
      "edc64ecdba034ac5afc978bf8c844016"
     ]
    },
    "id": "scH_8M9JnuTO",
    "outputId": "bb1d78b3-ac0a-4070-c38f-0ddfa335347b"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_torchao(\n",
    "    \"model\",\n",
    "    tokenizer,\n",
    "    torchao_config = model._torchao_config.base_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEOYNQfo3Lpy"
   },
   "source": [
    "### TorchAO Exporting and Conversion\n",
    "\n",
    "We also support exporting to TorchAO-quantized checkpoints with custom configs to allow inference in vLLM or other inference engines.\n",
    "\n",
    "For a deeper dive into TorchAO configuration, you can refer to Hugging Face Transformers official documentation: https://huggingface.co/docs/transformers/main/quantization/torchao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJIPK-Ik3NjO"
   },
   "outputs": [],
   "source": [
    "# Save to TorchAO int4:\n",
    "if False:\n",
    "    from torchao.quantization import Int4WeightOnlyConfig\n",
    "    model.save_pretrained_torchao(\"model\", tokenizer, torchao_config = Int4WeightOnlyConfig())\n",
    "\n",
    "if False: # Pushing to HF Hub\n",
    "    from torchao.quantization import Int4WeightOnlyConfig\n",
    "    model.save_pretrained_torchao(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        torchao_config = Int4WeightOnlyConfig(),\n",
    "        push_to_hub = True,\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )\n",
    "\n",
    "# Save to TorchAO int8:\n",
    "if False:\n",
    "    from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n",
    "    model.save_pretrained_torchao(\"model\", tokenizer, torchao_config = Int8DynamicActivationInt8WeightConfig(),)\n",
    "\n",
    "if False: # Pushing to HF Hub\n",
    "    from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n",
    "    model.save_pretrained_torchao(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        torchao_config = Int8DynamicActivationInt8WeightConfig(),\n",
    "        push_to_hub = True,\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
