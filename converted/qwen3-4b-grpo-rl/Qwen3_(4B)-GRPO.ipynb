{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22e06bc",
   "metadata": {},
   "source": [
    "# ü§ô Qwen3 (4B) GRPO on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(4B)_grpo.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Qwen3 (4B) GRPO</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">reinforcement-learning, grpo, reasoning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(4B)_grpo.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkH_y8UC9lvv"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN75nmdx9lvw"
   },
   "source": [
    "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
    "\n",
    "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0c50de29d31643f4b393e3cf33fe9305",
      "cf7b495d876c4a5fa6a2232f33b88645",
      "d061929d5b564fe9b935ddd2794fde60",
      "e586513f31904963b0cc636ba38b1f34",
      "c7dacceac8734e6a8bd84051b2606fbd",
      "672a496c5f1a4e4d91dffa603819c734",
      "dce31da5eeb546debffb63ca73095607",
      "c228ad7433a043c0a8a3e8c9d163e8a7",
      "87521bd756d04a3fb74128a4197d7495",
      "bc0fed6255fb4bd8bb8cfc056070fe3c",
      "61acce29966b4127ae3eb36eda2de845",
      "3205c37996aa43f1994f833f524a6635",
      "33a467f5a70d4505b766da816d2e7ec3",
      "f6348f60f4b448de91eb28d373d38382",
      "2c0480d9e3034e689a4cb980f7ba3185",
      "555c5e4d968948aab8b916f062396dff",
      "364829393cbe4ec7b9bad9376e6d577b",
      "64d1bc7a3efb47d7953935cdcb5c0ab8",
      "f68946c6aef84031b9281616a46e4c0b",
      "6afdc5ce3ef0405aaefbe4c7df8e6ff2",
      "ef79e95f5bd14099959adf1d97d0b2f9",
      "5ce28aa7cbee46ebbf6b6048c16be279",
      "21e5437cbdb14b6791c1cf2743b17035",
      "d601ca50f4f74b1e9504f06f570d3581",
      "3c0e156163e5446e8eacd22c8d13f5ce",
      "c824eb9631ba411b83378ced08fa8e32",
      "5f81bff16f1c4112969ea0905348a5e9",
      "acd44f8439d24fc3963bdbef97605dcb",
      "d1e3f7e8c55445979ee7fe39dcc4a13d",
      "5c0709a4e46d445fb230f2c8db8a32fc",
      "c3dd75b6a77f4c68a47f6b08671e0bda",
      "8dc2e9a5bc8c4be19c9c5abbcc0b177c",
      "8847ab5fb88d4f8eb9bfbcf37ab2a726",
      "684ae6a5f9c44f6bb5119267601e00d5",
      "84f307f9a2e449a487f0f01fc485988f",
      "edbd03a6e9ce480981835491a5729dff",
      "c02327b34517480e97b385f7cbdd0c65",
      "f9df6965d3554779a536cd8b8351c2d1",
      "be29847b1f0d4d6486c4b11b1d911575",
      "148c0996516f43e89e5f1db8fd9ae940",
      "efeba004a8b342a3b39d9cece6341d1d",
      "1b3bfcbda4924c41a9ea1015f17d083e",
      "7eb2a72d36d34873933333f3b20178ad",
      "aa6e3f2214734c62ac3d1afd6daae113",
      "70aae21006b74b3d8853b0b0ded6764d",
      "3343700439584c62b90c873c2de296c3",
      "79dfbcad56e54c1dbe6623b91226ffc9",
      "c9e7299c50ca4bdbbec8f2d924a2ee47",
      "b6090914c62f4dd3b6d255ea157937e6",
      "1ff5260b805444cc9f84f736daa2fe33",
      "5dac73d276b9409cbf6e36d9968b1329",
      "8256936d64a14cbab7fd6ebbad829b8e",
      "943af5bc990b42da943288e32a9fdc38",
      "1e12e905e0734cf3a3830d7a2a657849",
      "ecba05c351e44779ac874b0f89a8bd52",
      "9da4bc555055409ba92389a01bc8355b",
      "1e92baf068ae47d8a865dd3e88f7897e",
      "d96e7930c38f4651af3afa5a8e9e8a38",
      "ddf13acbbdea45aaa76ca9b0e338bcce",
      "34481948ffcd48d5b8eb452f28997df1",
      "825aae008d7d470dbc749e062496cd57",
      "880d9ae8da4543848af20721d1536f5c",
      "7fe08f2fa9b84a0094438724b64fc29e",
      "acf929fa9d9d44d48aa1366fc4c71e04",
      "ff5def5c858c487b8934f6f0760ceb90",
      "428ec48b94174097b79efbb965be81c1",
      "812549042d8c4e42bbbff0489385b6b5",
      "7341b4e4618843f9af54666c3ea57b06",
      "47417b44ea17495c9ca432bc7c9631b5",
      "a28b13f9076641759a7f2eec7bbb9314",
      "76ccd421962c4409992af9a786b3a214",
      "11b7ccc7b90f43a4b7ae3c11e242f58c",
      "376152519eba4b1095f81b4c350cc29b",
      "5f9087e750334d778977e6a5639f207b",
      "b9958beb3fbd4c3e997cbb488f221014",
      "2613d48b70ea4466ae03d0527de8adc0",
      "77053c0fe7584a0aa99c6b52e6bb01b7",
      "da7f0c3bdbb84be98e76d4c103f1d7b0",
      "dfa560da058c44229f214abcd7eea3a3",
      "4c150d2dc5cb49e2a97fc4c0d3a65d33",
      "bad797134b9047ebab079f7484cea9ae",
      "de006f4baf3f4a6fa0b80204c32e6630",
      "bea07474f55a419ea86ceae4e31cfdb4",
      "b6467d1e10884f84940fbd480f8e2e25",
      "ba45ee92016b44ea970e4bded906867f",
      "349f5465f09a428ebd95bbdd9c6fd345",
      "a0a2930e04584eeda51d797ce3e1e796",
      "b1996d5c9d804a51963f376659fe8a53",
      "4b7dbd914e9c4153921892e22c9a6a75",
      "11e6606ebe8743ccab9b5accb34c975b",
      "673d380ab14d4efa8d58e19564aa42d9",
      "e767cb14771443d08a7d28cfb17f66fc",
      "d43acdfb6bc64950ae43e60cfdc5e016",
      "b1c329e41a3740e0af78e3e5db460175",
      "2b6dca7094154e5387484ca3c99ad27e",
      "3a9e43c89ae44234b110c10a0ce59706",
      "42d415f99bdc4c9e92490828794461bf",
      "9ee8af4a9216496fb9291c0f7b75ae9b",
      "967297b0411642bd9b49ccd8af324712",
      "1d8f16e89a6c49e0a83dd9bd950b9f2e",
      "d63ce203ec9040248bfb7392a470ba35",
      "f39b44f90b534e53b0149303bd3b8c35",
      "0d379295a65040c48d29c970c01073af",
      "628755bdb70544dc87fa0dc9c50c1a0c",
      "7eff27b4a7d04a04ab24299aec26af8e",
      "6dbcaccb8abc4c24bd282b47475cc18b",
      "b494999ae9074148bf44bf3c3ac2027c",
      "bb2da51f85104128a3571291482417d9",
      "cbb7966a9d32446ebcba328a597312c7",
      "a3db41e1322746f7ad9451a04cb2375b",
      "f0ec9b2ee1d34b36b63ca497026e0fe2",
      "f737f6f521ea4a22b8b9c2a9d7c0e66d",
      "b7008e37a9df4af681033ebac31d89ef",
      "793e384f2a06439ea791bf586f50822f",
      "3bba54e71b384b219902bd90b90952ff",
      "a0440e6f7ddc4b67a63d68414ffced82",
      "f4ae9ed8000848d98eaf5a448cae247a",
      "1c18ce46279144c0a088bf17a4c643ee",
      "57af1ff86abd4314be1129aaaf58feb7",
      "5e14499a3c084181bedbdf7deda7785c",
      "8e348c0f2cd84b68909ca6b135f91de7",
      "9095fbffbc2d4dfb9e87e3a6b8d52f7f",
      "2b379da56ee546518c2a1a973887c9c9",
      "8f7434b457b9426fae760511386d2f99",
      "f70c425adeab445f8ede3dffcf17628c",
      "16625ccaa8af4bdfae39c402e46d8931",
      "c6f8f3f9ad314976b172b958919ced61",
      "366278b0181e44f59212d8c3ce3f02bb",
      "86168f4fee4245c4ab5e61a707b12bc7",
      "4abf998bd1b249baaa01a3ffbb7fe8cd",
      "489a62327986402c821fcccfd64a1d8e",
      "0e904825e9bc43afa4a64ce639ede567",
      "5bfd5632a7a94aecb50b56c5c691d370",
      "0ea49ce02598418c98a22f89cbe1739a",
      "440a24a1c63040479e8fed28cde13f6f",
      "9844334c27d441dd8513fcb5f4601354",
      "604ef9959ca24661b9c87c4a9603499a",
      "8de3773fdea04b5a915efb8f4caa774c",
      "80f5247b70464b37886ecae0b32de24e",
      "21227e457b9b4c838404e634aa27a260",
      "91bbd7eb05564f909f5c3b68b0d8806c",
      "d1978d5ef63b4afd8aaefd96603d25c2",
      "a6487b7b1f214b36a865c9bfda0fbb06",
      "50959bcebe4c4317acdb8fa3deed0a34",
      "d9cc66f1c2d943c18af16403b45c74e7",
      "3a0ed850287442f3a15a2fca9510d9ed",
      "cd43ae439128449e99cacd078ed27f15",
      "6840fb3f70fc4563b48e937bd4188fdd",
      "a6b8d18e89ec4ad883261206fcca51b5",
      "df93ec750950471090a678a7c4d30368",
      "1b71541742224c74947aa1048cbc5f5d",
      "e23fd44a5e6a4a0e9a9704d096deb93a",
      "0271e219acc34fe8a84be99c44228ea3",
      "64eeabd68ea04df595164d74065ee12d",
      "4486f96e74c54ed3afd8cb3bae9d5c66",
      "0642e760ad7c48c49a1a3379cba7635c",
      "11e3b7912e2247d7a0a3d25a3ac84078",
      "4de06cf823ec49819c023277df59e03e",
      "bfbeb8af6bd447f78de6792b1c743f36",
      "4399c7f86f464244beaaaabcc7a5f6a8",
      "b8cea323a71e489984572e8aedef4db9",
      "dcaa1f5d29274e8f99491e216880fa1b",
      "1475e165462b41138efd43ad8466c1d0",
      "8934f472c8c24aca8dcf404067902152",
      "348c639c169e4808a46f080d2183f947",
      "e47ca12917434a3eb19b2922a9992989",
      "0a731d9d8dbc47eaa01987d261345b3a",
      "3db91858524a49729d4e5db7ce536e76",
      "d96a27ba6f354b54a353ece3643cb0d6",
      "4058a59b210a471d8b30a4c935d6b4a5",
      "0eae2ca191b7481aba7715b6d47e446a",
      "5332d0e87d844a1d947ce2ea7f3ebe86",
      "c32ea5fbc97343218e9fe7b8a2a88e15",
      "fb8a25d2039d4cad884c6bc43da5ef9f",
      "94adc0bfe6ba4553b894941cf7907b86",
      "b023e634d8af4b86a26faa27eed31d52",
      "2601a137306e4cd9b60652032f4022ea",
      "6c331869a8314509bb386e67f95458b2",
      "59a014cecd6b41aabeb9d0a49b0ff314",
      "27a3e24e1a6649118e0d93f71140bc2b",
      "05bdc2b2b47f4035bdeb8b2c392bbb09",
      "967768c57bd94e50b5d5d060146b90f0",
      "7060bd2d51484eae98b9a7a0e2e13d09",
      "cac0fe41bb37439a9f49fdefb9cef3b9",
      "87c4f4c7e20c443b87a191cae97b7bf4",
      "a50955b6c970409fae2381e4afe51e43",
      "603069e6eb7e4379a8d87086a78b143a",
      "1e2b7f7fa864493a87e3e068e00b1077",
      "e54741a3f72d4bd29b0a32506699620f",
      "e4e9475a24644804b13b5f9ceec0ee1e",
      "5a96013b3b354989a7a1a56d6b6c7632",
      "f55c4237cee24f4786692ba55d552741",
      "673b5fe288e6476fbb2396c8d3f6f7c7",
      "89b2b8ced23c452aaa40969a5583e3a3",
      "3b6106daad044d6b8dd3142c3034afa6",
      "288aed779ba848499ae71336def6d327",
      "d470de5f2ee042f08fd874b71f368028",
      "a80c247a0e98453189c6260d664bc49b"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "b20d048a-450d-4c0a-c889-cee88da7d090"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9DuiVRLhMco"
   },
   "source": [
    "### GRPO chat template\n",
    "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
    "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
    "2. A `system_prompt` is recommended to at least guide the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "6UjowCbT-cFz",
    "outputId": "9390bd61-7864-4f23-c40c-4f69c8752cd3"
   },
   "outputs": [],
   "source": [
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGgs0MJkDkYL"
   },
   "source": [
    "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3fF9gMujY02"
   },
   "outputs": [],
   "source": [
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with out specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcLdymBEHdk"
   },
   "source": [
    "Let's see how our chat template behaves on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "BciEDYSSYFNj",
    "outputId": "730f6a22-ebdc-4c25-ef57-b80d92d9c516"
   },
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mdsuGjxHrjT"
   },
   "source": [
    "### Pre fine-tuning for formatting\n",
    "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
    "\n",
    "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693,
     "referenced_widgets": [
      "b57476ddcf71441a9be7a7adb5344c97",
      "cab74710156848319ca1bd9f49956341",
      "6eb3e586a7944970b38740e5f66ac216",
      "9f4b810c28484866aee40c2178b54d94",
      "fd3c03604a4446c59d9b9512a5ddbec6",
      "cc44e1389c764206a2cda64169a94db3",
      "548c259a8531415986f411afc5dd8490",
      "7d60f812cfda4ff89b771d904f9c6f10",
      "69f897b60822454b9654975b38bcd6ff",
      "e3d8f87bb3464aaf865e15bfd17d0823",
      "28bd56771de34bedb0760bf7a317bf7c",
      "b6af2e74a5854886a0954f604dd32247",
      "4618e733a27a4403ba026791fc917d9e",
      "11b8b96b56be4b2c95b926683856de45",
      "46645fcc03154741b7f1c84a9e9cb20f",
      "2d682b3b68b144d3b24088f630bc781a",
      "d7b149842d5b4d9e95485215e2063c81",
      "df64f2521d1142c3a2086ecde4703879",
      "43e85b2a1d2b4c77bf7eb6d010968d31",
      "385e47cc7d6f448d84c7b75cf837ee2e",
      "de3f998257f84dbe9db71c632cda6885",
      "a5092fb7bc2448e49bf2480b895ae4d1",
      "51d8dc61f8814edaada0866be9ad5ab7",
      "99bf39c9ea9a4b8cbf61296e2ada173c",
      "0412222cc6c146c0b44a2f71f3b5a319",
      "252d523b8b6e4c1d945af82948c8afd4",
      "cde621f1846045c7b72f8c54bd7c313c",
      "53fc0a37829e4ef9b08146bea56d61f7",
      "c0aa7c54f5c749cea0afa569732baf58",
      "6fd8b6aefb2c4ba3af00c374d907a35e",
      "ef0c1b5686d94f6482b4cd88cf78460f",
      "8e97170085b74887bd040ff70ac01114",
      "56e6cd78af5c4017ac1084ff339288df"
     ]
    },
    "id": "AXxM2lStVIkd",
    "outputId": "027252b5-8466-4bed-fe17-60a47c2b0629"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# Try converting to number - if not, replace with NaN\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVRFqoSdIEVK"
   },
   "source": [
    "We have to format the dataset to follow our GRPO style formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9ydcV_Abfi6"
   },
   "outputs": [],
   "source": [
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    return [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5NI47rOIRP2"
   },
   "source": [
    "Check to see if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "LTdXBKcslhRH",
    "outputId": "7985177a-63b7-4af8-b5f1-dfb66470d643"
   },
   "outputs": [],
   "source": [
    "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHV9BXYiIYaq"
   },
   "source": [
    "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
    "\n",
    "Note this might take 2 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBHFlRbae9_s",
    "outputId": "f0d89109-b397-4e1d-9a52-098167508609"
   },
   "outputs": [],
   "source": [
    "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
    "\n",
    "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6NkUCAGIj8N"
   },
   "source": [
    "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rgdtiV_f5hx",
    "outputId": "764c505a-52da-4512-8e25-f9ac7261786e"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQJjQrYKzOk"
   },
   "source": [
    "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "referenced_widgets": [
      "3e62f48b4aad425fafc115e6c57de6a4",
      "c614ef0e015740759da02d55a5cb80ac",
      "08b378b542234293b57ae8fbd63e4613",
      "902823b5472e4f54bd5573328f06021e",
      "b2b0a3e1e994479abaf8ac4bfd263004",
      "b30e4c9f78134ed0b52bb4d34bc62af1",
      "4d3e6fd0123b4262946145c39f2a7601",
      "146309965bf04eeabfb6c02d821bf321",
      "45841c30e99c430799346e2546106ea7",
      "2bd4b8a7151c4d108b422fff4d99fb3e",
      "c36c2fa5594e4fabb4651cb018a91804"
     ]
    },
    "id": "woYi0SSygpqp",
    "outputId": "faef19e7-228a-40b1-aa03-77122b4a2dc4"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "l4-2v_bLhZuE",
    "outputId": "82af442c-124f-4bae-e179-0b7c9d67f39b"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRMBNUBgLC8T"
   },
   "source": [
    "Let's check if the model has learnt to follow the custom format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HJxrS76h3Ds",
    "outputId": "312e41cd-cb12-4786-eb32-cfebf8d1ca59"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"Messages\"][:2],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtZ3qGOALF95"
   },
   "source": [
    "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWSZ0DET7bob",
    "outputId": "b02a0eff-3910-49ca-f900-a09a520770a2"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201,
     "referenced_widgets": [
      "c811b38ac4b84edca4f8992795364254",
      "c2b418b8cf5247a2a5da08299df0917f",
      "51761db5c13b4d418614165d033ac6f5",
      "e489590b36ff4d2386ada63aade4a79c",
      "0775473857a643c093764a2ca588d5c7",
      "db544ad9ccd3495882b1a3d0f7644ded",
      "a1eadf3bb73c4a9fb7ded669df942e29",
      "f40d34dd33144818a896ce31cc95e222",
      "1e301887f50942d28ea1c9b7b2478331",
      "19978cc961894e63bc4d1a914a277234",
      "9443539f42e54bf79847f679c7d1d2f3",
      "28e48534b89a403da0c07339682fa74e",
      "05ae7521c459456a80369a22e1ee88a8",
      "c1c1c813346947f298261d558576cd30",
      "c0fb4a9d32214edebbc9443287ef08f0",
      "21547cb7f692473c8166e88154915dff",
      "26983a12f6384fa6bbe345651db6e79e",
      "02deca4036cc480f9ed6c2ef3d29fb73",
      "068a889330bd4e039714ea86d8484bf6",
      "70f8efe0424b4ba7bae3aeb7591df22a",
      "68229c85da5b43698abdeb474bfe80c8",
      "041067884fb540fca3c0f29f5bb50914",
      "9e85547dc0f8443cb16bfbda4a5cf463",
      "2720be9bf09146179f5d0e48a90632a5",
      "99f0d155bdc741678854e082378a866b",
      "62e6423319c3493a8a6788918522b890",
      "6cc43eb82b2e4ebb9bcc13a7591da5b2",
      "c04bb6548e494830b3bbe145b1c61c37",
      "53d45528cf5c43ee9f5781e5cdecde97",
      "d9bb7dd467804742ba2cb93a14b7bde0",
      "1f1665a6bb1b46c091e6ffc08c21dbfc",
      "739707b035094350859d1143f5ae1b0f",
      "7241eafe1fb34ca89437566845dd5d22"
     ]
    },
    "id": "o7-eUrQn-OzE",
    "outputId": "5a096f76-ed3e-46b4-a599-360c857fa46a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b00gUsS-ROW"
   },
   "source": [
    "Let's look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "siopxjG8-ReF",
    "outputId": "35fc68d9-f4fd-45e8-a265-5de46bfeeac3"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KGupRQqD-Wcf",
    "outputId": "cd5d25dc-3e63-4692-ee5e-cea88c4ae3a9"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmnXj6hn-Ydi"
   },
   "source": [
    "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8JJGXKdJ-Zl_",
    "outputId": "74eb22f0-72c1-42a5-bbb1-6d1863c13b72"
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    return text\n",
    "extract_hash_answer(dataset[0][\"solution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K30CygaU-dir"
   },
   "source": [
    "Let's map the dataset! and see the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "0a87ca8327df48099b3691bfa446ef3a",
      "9abf2f4547c9483a99f983d7ba26b1cf",
      "ed21acc8e597481fb1b5dfda4adacaa7",
      "cfdb8549eed24409b2aa09271863c06e",
      "db8c351189b44767a9aca3534e5cc93e",
      "4886339fcdee4cb3990e458061c33a74",
      "20f7043b723746519cef74899dfa4153",
      "97119d7743e34110af6c48fbd14104ac",
      "81eaf78c3c294383b43c2efdfa5e2225",
      "b66ab6459f9b4d15b364a9e94d161534",
      "a7c1f428da3448688b6336aa14c3859d"
     ]
    },
    "id": "qyEVI972-d3n",
    "outputId": "8efe505b-fd40-4940-a55e-519fa4260d65"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9m8eR9T-gMh"
   },
   "source": [
    "We create a regex format to match the reasoning sections and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQwjTjNz-gY_",
    "outputId": "7364b9dc-668d-42ef-fc3a-df768285591f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OycMneOq-iNC"
   },
   "source": [
    "We verify it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndzHnQ_6-jHt",
    "outputId": "6b211d52-6e49-4a44-82b6-3a34c643da4d"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRMDAzDk2x6t",
    "outputId": "c4f3c11c-7c8d-4ea6-8df3-ce06180dfac8"
   },
   "outputs": [],
   "source": [
    "match_format.findall(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOjmO5l-kl3"
   },
   "source": [
    "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgFNXORy-lpO"
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf69i2WT-m4K"
   },
   "source": [
    "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUfHzCVx-nGK"
   },
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAUWwtE-s6n"
   },
   "source": [
    "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmtI_8gg-uIE"
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        # Correct answer gets 5 points!\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        # Match if spaces are seen, but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
    "                else: score -= 2.5 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 4.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atMyfhXh-v3R"
   },
   "source": [
    "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
    "\n",
    "We also remove possible commas for example as in 123,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVW0kL8q-wL5",
    "outputId": "a972c5f9-4cf4-46f9-bd0b-c12d9e01de65"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbfaaAywNHHh"
   },
   "source": [
    "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjBFrttr-y1_"
   },
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOR3wJ_AyLr"
   },
   "source": [
    "Get the top 90% prompt length so we don't accidentally truncate them!\n",
    "\n",
    "Ie we'll remove the top 10% long prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189,
     "referenced_widgets": [
      "45529456ca08414fb6941d5d9c0620ba",
      "73ef0bf5f1fe4617a05c0dcea97d2694",
      "d16e3bbdf6f4487c8ce52a0045c388e7",
      "f950c74d41464996b06a84d15a3ad726",
      "bb70890c0f0f495a9b92a3ee6b25981d",
      "f10f9ba304fb4f3fb16078c17afc4732",
      "f62e720173534370a3c136e5741b764c",
      "84250230c06e436cbc7b7ad27714d467",
      "3bff794ce935432cacc12ff1e59895ab",
      "192818ea28e9415ebb28a77c763af419",
      "f8e295340e3d4babaac21e38c7adf8c7",
      "5a6e164f1bbd40bf874c94142f8d0912",
      "866fca106ba140eb9d9909c6d76dcc4a",
      "ca8137bdf68a45689c06942e8d72fde8",
      "affdb144cf7d4250abba63fb73749e79",
      "a2c1209c193c408e80a6a49c24f09edc",
      "0daefb9c3dd44f149b5e5265c721f0e0",
      "d0747440cf3243aab108a974878ab6e4",
      "762b33136da34e3cb201cff3a142da09",
      "642154039dd940bd85225057ecbad572",
      "c5cee8d441bf48c9add4b6537f6440e5",
      "8cb3184ee7f6498b81ab3a3fafbffbeb"
     ]
    },
    "id": "6EgAi4Q5fGE-",
    "outputId": "5f660416-82a2-4cf9-933a-b4c8e9335411"
   },
   "outputs": [],
   "source": [
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-IOMhVg-2AM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "a79c06e7-afb3-45b5-bde3-77dbf20a7686"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir=\"/workspace/outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vzOuSVCL_GA9",
    "outputId": "58bf64fe-2e53-4d28-a517-a1bf68ecc517"
   },
   "outputs": [],
   "source": [
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "a0959e1bca5a4c349cf0a960cac5a409",
      "a2a0258791614ce5b2fa6df769e13dfd",
      "417e93b26e894f9a95f042e143a656e8",
      "d1279e91e27b458c900d3643732485df",
      "57ce3d8d27a84a528b982b89140deeea",
      "59840055328441b38fbdad2592548b57",
      "ffcab2e96bd442c08336f871c27269ce",
      "ee8689f732384799be79eac6fb99ea91",
      "e7eb41f3bf4b4a068b6dc97cb067b43b",
      "d35d45abf5fa46beb8d940ea6d9dc1e1",
      "0146226f52b147119f45ea8da473cb22"
     ]
    },
    "id": "qtcz_lpbVC92",
    "outputId": "09698cd0-9005-428f-975b-97c8e1348fba"
   },
   "outputs": [],
   "source": [
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL-BcuB1VLIv"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SfdI-ERbpiw"
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwpbwnDBVRLg"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "4da56ce2fac243808bfba15d02cdffe9",
      "8cb3786311b943869e29d7e9d1c4d0e6",
      "701f2deb751f426891cc601ce1ca80b8",
      "65f6ba1b6cfc4b2891984d32d7902618",
      "3271a4acd52e465aa6370045e40bff20",
      "e73145b3b4b04c19a7ad3061936153d7",
      "8b0c1ff7a53541a7b0a1a99bd2485b9c",
      "635e5142d133454cb85ffd26991ca8c8",
      "631095864293470e8205edef4e93da30",
      "d3b545cee66c4ea1accbd506811b18a2",
      "7d2acd645b8e4d0e841d9ca8eda1fbf4"
     ]
    },
    "id": "zf_OY5WMVOxF",
    "outputId": "b05f52c1-44e3-469a-b037-9ee544620495"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aDgFfhFYIAS"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjXGTkp7YNtB"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V15Yhj1V9lwG"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
