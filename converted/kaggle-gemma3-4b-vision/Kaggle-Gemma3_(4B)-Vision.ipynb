{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d77c977",
   "metadata": {},
   "source": [
    "# ü§ô Kaggle Gemma3 (4B) Vision on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3_(4B)-Vision.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 1px solid rgba(128, 128, 128, 0.5);\">\n",
    "    <tr style=\"border-bottom: 2px solid rgba(128, 128, 128, 0.5); background-color: rgba(128, 128, 128, 0.1);\">\n",
    "        <th style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5); font-weight: bold;\">Parameter</th>\n",
    "        <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Model</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">Kaggle Gemma3 (4B) Vision</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Recommended GPU</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Min VRAM</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "    </tr>\n",
    "    <tr style=\"border-bottom: 1px solid rgba(128, 128, 128, 0.3);\">\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Batch Size</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid rgba(128, 128, 128, 0.5);\"><strong>Categories</strong></td>\n",
    "        <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Kaggle-Gemma3_(4B)-Vision.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pip3-autoremove'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'unsloth'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'transformers==4.56.2'])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", '--no-deps trl==0.22.2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFOEZbP7ONMs"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499,
     "referenced_widgets": [
      "e93d59a8ed334a64b5edee70de950de8",
      "e4d819242d564a8aa86ef79361a34664",
      "72c120c082a7488b8115f2582f47b21b",
      "218d331c3b7d44a7bee6bafb0b62078a",
      "ebbf8cf7bc6e4a79995886b005a0cd60",
      "7eb2d3f623894eceb4b30b3ca57f2438",
      "b62d55c966364a8d975a8787dc543c85",
      "f597becb5b1c4cfcb918b037734b5b64",
      "4aaacfee85564c9fb148e218229bbfc0",
      "de4e6780ed3f447b8b2f5a83b71332d0",
      "b6a7940401db4394a62edca9e7237d51",
      "f8373cae8fa94224b9702cb5b54fe3a1",
      "8cc19f11bfae43458cd883d42862f60d",
      "8b31b20120524d8cb873acc128d8f408",
      "9667bf6554e44126a8ba7d2a39d79193",
      "d6609f816f854bf3a95064579029cbc8",
      "2fea9250179b418f976df1cfe37ff952",
      "66e614a331ac4810a6593c49e38e5696",
      "47287270e700481ca5bd655fbfb2cc1b",
      "9e39e41782a64b079f0d8a35600941e5",
      "1421934a7306431283f482ddefd3b3d3",
      "fa9d9e41b65349af9869a0ff309a15b5",
      "4616a5e2f05a431a9fb8356418c9ffcd",
      "610e280ee7d64b9eb087e510bcee1b20",
      "454c4ded53f043b492d6da8db7dd7f88",
      "6f8f1e6dbdd84c05b5406b37ec298d5d",
      "0cdaaf635d4448ec9ab6093a7896537a",
      "e50155a32bbb4dc8bff38c2ea0d77757",
      "77719d312cf8458889c71ccf250ac3ff",
      "60602e9dcdf64394a1aea635635c49d9",
      "81a94676738e45efb5329be1015139f4",
      "da0ba3b117124b70af7af4dd93b4d7da",
      "3ea68d71ddc0425b9a85fbb828c18608",
      "4792a4e4307845319deab88da594fec4",
      "a9a0f8b31c944fc79524d09d62f91d74",
      "5621fd48f6044347b6fd7e0128177d9b",
      "40df524c71b7463485121fdbf1f0720f",
      "4f67de3df70f4eaba278058876a51ad0",
      "a5187cd5db14469b8dc8b7237c8bca0a",
      "fd76e5690710464db9988a4306049a8c",
      "c098757a031141e2a38289f5dfac7cbc",
      "3fd3cee164fe4f33a48855c8be2170fa",
      "65218ef9c77048968cdc7b2552f349b8",
      "813d3f0de4584213b53f0316354348bf",
      "9b86177225724164b769dad570297a41",
      "621b0db238754b0d8b1a39ebec02df56",
      "3461255d4a534b459337408a95d79ef2",
      "519dd95a841549688d559cfb9a7c3af1",
      "ca5ffbc49bd3456a85cd450a23d62d60",
      "9d758df985f14cc3a98b17f293687b35",
      "7f3b15bf2dd04601bd6d52f1c14ca6bb",
      "78501884dce140bbba910da94c062b47",
      "9eef998bdc96463abb79fe365dc9255a",
      "af522bc8df774456a4c66ced8cf2220f",
      "e35bf4c1bed84877a5f5f5d326ecfa35",
      "465733e15d624191beed6c32ecdf5fe7",
      "4a750bb6bdb84fae8dee0126bc7fb6fe",
      "b8cb44086ad443b88f1bf5f46ae9768e",
      "c0a79a7f3262451fbeaea71f621bc61c",
      "9504d479a2f741739650484c78d3ce4f",
      "babed59a2984404ca5e201afe0c5703f",
      "01f7f9e1d9334a6c8d8ae1b31f250760",
      "f3740927bbc94890b0765dd70bac6942",
      "1b6e541f9f0f4638897bf3da592da51f",
      "a306316e6381402ab85574927bcc8713",
      "fdcc0570d2224c61a7e1329e990adeac",
      "b328ab1712344e0b8dfa918a81e083d3",
      "fb3e7eef51834b4a89a86bf1d0e58210",
      "b02af876a32b4438a04da76cac00fa65",
      "b780011b444841cfac13a5c3065cb17b",
      "51e350fe34a04519a00d1a3b2e4d03bb",
      "c01de1d410ab4486b466d69877d3dd80",
      "372c27a58d6e4abf9c6dca9c4289772d",
      "bfb8b853e0594505951f826d90fc038a",
      "772aad5d5b8e4d289945290129dfa6e3",
      "a96bd4cf276149f5a33dc6f77590ec4b",
      "5e42db60cdb8419fb71bea3521d752e0",
      "fb45f16302e743eda2f769d93cfe0aff",
      "915b83a63f2d4056941a05975c0391bf",
      "c243e55080ee4e8ca5fdeb445fb4d37c",
      "3ee08311c4024e5ca2688ba684c71e9a",
      "7c12c3fc352a40c1b93288f6f7c4accd",
      "2ba3cec0bb304713b11b4d32842f9a86",
      "9ea1e065a877408f9880459034d73749",
      "7a39bd99818a49999abcc275e804d9e8",
      "8f5b4ca0e1064319ae6e031e6f8e7df3",
      "fe281c7d7a3c47fab2e603d727c50a41",
      "f9e8955c473b493a9bc405cfd4954708",
      "ed3f7041e2ab4cae9d3f6a22351aba14",
      "956e11e1d9b44c74a8ef4c0bdd94b899",
      "9e22d80465194b768ea7b986781a72d1",
      "5e54ba3e30d2469782f44aeed45f266e",
      "62dc6de132b34cccae644f8019d925d8",
      "769300a1c3f447a99b6126411886d967",
      "09f115e397d3462186e2e244497bf97e",
      "145416a3630b46dab13480b7daad3f5a",
      "239d5951b4914057b4c0aefeeac19250",
      "b561de0cf64b449b8d4d084a6f3c6e6c",
      "be9c02d8c6524f22bc06b523436aa9c1"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "7b2cc094-d9e1-4dc3-da42-6c22eef471da"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/gemma-3-4b-pt\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient fine-tuning, allowing us to train only 1% of all model parameters efficiently.\n",
    "\n",
    "**[NEW]** We also support fine-tuning only the vision component, only the language component, or both. Additionally, you can choose to fine-tune the attention modules, the MLP layers, or both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "2f629d76-5953-4946-b6a9-9dd1e8ca6939"
   },
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,                           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,                  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,               # We support rank stabilized LoRA\n",
    "    loftq_config = None,               # And LoftQ\n",
    "    target_modules = \"all-linear\",    # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We'll use a sampled dataset of handwritten math formulas. The objective is to convert these images into a computer-readable format‚Äîspecifically LaTeX‚Äîso they can be rendered. This is particularly useful for complex expressions.\n",
    "\n",
    "You can access the dataset [here](https://huggingface.co/datasets/unsloth/LaTeX_OCR). The full dataset is [here](https://huggingface.co/datasets/linxy/LaTeX_OCR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "900e2c0047b54a429c4f93c311037380",
      "92ab4fff707048e38693c506a14c1148",
      "2dd8c23af1c5412284f7d4045213ef85",
      "6fe6c9661a4a4ddfb5c978b60ba424f5",
      "23e8a66de3f8496d848e970e353a00c6",
      "61d17537ab6347b3ab8478a08d647350",
      "2fa71a8f55a3410994b00310ded4c485",
      "d7fc654c2e434f838d003f1515533978",
      "2a87f5abc13049639b0ebc7e9f87d1bd",
      "65851535dc7f40e3b30afbdab64f8e6a",
      "23f12adb8d86447f872b7fb739397d4e",
      "0fb0ae113ae448108d6a3ea54da95d62",
      "5d110eb3c42c425d9bed6f6da9ac099d",
      "e4e7070ba0804047bb9085aca22730df",
      "149cd5c8e67d4890b6c3e7d4731d511b",
      "fdc31dd898c94e998edcd6293e0f261b",
      "97ec37a6aab143f999e00ce0e9649b37",
      "705779632e724ad3a73d43ea52034638",
      "4946378f44eb47a884d95e2247918385",
      "fb17b9d5ace84165a6cadede34697010",
      "cecc564cb4394509a45168ca008b1565",
      "a99bf379df8d416fb343fc2bdbe9a63f",
      "7ceaf4e478f5474a806323116e1a3779",
      "fb698e65899c4d14ae6511a8080ecfb3",
      "1b8bf617f50b4ded90a888f5c0bc6cf7",
      "5ea4aa3e4cf84ff0bf1a94eaecde889a",
      "28fdb9a345ef453c97ef22c3c6a55ded",
      "73b5aa07865248a8bac0537c1e173f4b",
      "de5339d306144213b84a3ef323720fe7",
      "38caa5ec7dc142eaaa8f20f3909704f1",
      "c1c37538834a4740acac85badd767cf7",
      "3e7e3bafc4684bd3a4cf2e757e7bcfcc",
      "4b12b2e675a84647ab8b6518226d2597",
      "c631fb112cb64f6abacc18ae5e24ee8b",
      "9483c5c4a7da4797b0c1852314ff4df9",
      "959cf4d1d38a4eafa30725d09b8a2d30",
      "d3c2beef80c84d5bbc2314c796b0292e",
      "f99ca5ac2ca54ab4b35b378e48a10d53",
      "2cbc9b5bc5f84ad592f3e383812e85be",
      "4a0f9746813e47128f165979e7d8d7db",
      "47450bbcd27f48f7a13e747474de0f7f",
      "57faa7d69c674b05a63aacffcd73bbe0",
      "c93d87fc37b94fa3af4210cf68b60231",
      "09e14702010742fe8927d63c37ea7522",
      "2ebdae870ee94f0eac6113cccffff29e",
      "7750d304e4dd4b81a2b71e1a21934d7d",
      "4f675f12e44b4531a0e9b107a01c51c4",
      "eec50ca3fef5476bacfc066f47a536e2",
      "f57e983b895c4939872e0e35aceaa1ad",
      "aef9037557774aeda772ec354fd69783",
      "fd7000baed8346e98de4abfbca851689",
      "ff1c1da4242e47f3b2a5eb6f16f90fa5",
      "cf4733939e8848feb3fc7c3c6709cfd6",
      "eab194e82f854ffb82adcd5d0705c4cc",
      "858fecbe71514cd983ee0358a3dadfbc"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "651a35d0-7bd8-4094-858d-17cec302e330"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"unsloth/LaTeX_OCR\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1W2Qhsz6rUT"
   },
   "source": [
    "Let's take an overview of the dataset. We'll examine the second image and its corresponding caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfcSGwIb6p_R",
    "outputId": "5973c7b6-3189-41c4-d65f-18f1622e5d00"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "uOLWY2936t1n",
    "outputId": "4ee679c7-9b1f-4454-a3fd-aa4d74e00294"
   },
   "outputs": [],
   "source": [
    "dataset[2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lXjfJr4W6z8P",
    "outputId": "ddb56f54-110c-41f8-e324-f9ba60039794"
   },
   "outputs": [],
   "source": [
    "dataset[2][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKHxfZua1CrS"
   },
   "source": [
    "We can also render LaTeX directly in the browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "nPopsxAC1CrS",
    "outputId": "ecd52dfb-2ffe-4064-9bb1-8f86664684bd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "latex = dataset[3][\"text\"]\n",
    "display(Math(latex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "To format the dataset, all vision fine-tuning tasks should follow this format:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPXzJZzHEgXe"
   },
   "outputs": [],
   "source": [
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"text\"]}]},\n",
    "    ]\n",
    "    return {\"messages\": conversation}\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY-9u-OD6_gE"
   },
   "source": [
    "Let's convert the dataset into the \"correct\" format for finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFW2qXIr7Ezy"
   },
   "outputs": [],
   "source": [
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "The first example is now structured like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGFzmplrEy9I",
    "outputId": "da82ca66-a9c0-44d7-a7c0-8d64a5b56612"
   },
   "outputs": [],
   "source": [
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS55FC1f1CrS"
   },
   "source": [
    "Lets take the Gemma 3 instruction chat template and use it in our base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEzvL7Sm1CrS"
   },
   "outputs": [],
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FecKS-dA82f5"
   },
   "source": [
    "Before fine-tuning, let us evaluate the base model's performance. We do not expect strong results, as it has not encountered this chat template before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcat4UxA81vr",
    "outputId": "00dc0ddb-b981-4d90-c114-338ca6b69b2c"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "image = dataset[2][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
    "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeAiMlQ71CrS"
   },
   "source": [
    "You can see it's absolutely terrible! It doesn't follow instructions at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
    "\n",
    "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "78c74c51-f328-485d-fdb7-3bab581e8768"
   },
   "outputs": [],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=converted_dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=UnslothVisionDataCollator(model, processor),\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        gradient_checkpointing = True,\n",
    "\n",
    "        # use reentrant checkpointing\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n",
    "        max_grad_norm = 0.3,              # max gradient norm based on QLoRA paper\n",
    "        warmup_ratio = 0.03,\n",
    "        max_steps = 30,\n",
    "        #num_train_epochs = 2,          # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        save_strategy=\"steps\",\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\",             # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        max_length = 2048,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "e9119560-5845-42ac-a05e-d5dfd067472b"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "fde31212-3649-4fbd-82b5-c8f355335333"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "c07d7c52-f5cd-4f9d-fb43-96c20bca110d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can modify the instruction and input‚Äîjust leave the output blank.\n",
    "\n",
    "We'll use the best hyperparameters for inference on Gemma: `top_p=0.95`, `top_k=64`, and `temperature=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "4007fc41-1c37-4f9c-a36b-bb49470f3fd5"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "image = dataset[10][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]\n",
    "\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
    "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, use Hugging Face‚Äôs `push_to_hub` for online saving, or `save_pretrained` for local storage.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "5d6d63e2-0e87-4abb-c5ff-dd183aeff3c1"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "processor.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "57e825f2-ce09-4322-e48a-3c82ef92dfda"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Set PyTorch to use this directory\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "\n",
    "if False:\n",
    "    from unsloth import FastVisionModel\n",
    "\n",
    "    model, processor = FastVisionModel.from_pretrained(\n",
    "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=True,  # Set to False for 16bit LoRA\n",
    "    )\n",
    "    FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "sample = dataset[1]\n",
    "image = sample[\"image\"].convert(\"RGB\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": sample[\"text\"],\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(processor.tokenizer, skip_prompt=True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Select ONLY 1 to save! (Both not needed!)\n",
    "\n",
    "# Save locally to 16bit\n",
    "if False: model.save_pretrained_merged(\"unsloth_finetune\", processor,)\n",
    "\n",
    "# To export and save to your Hugging Face account\n",
    "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", processor, token = \"PUT_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
