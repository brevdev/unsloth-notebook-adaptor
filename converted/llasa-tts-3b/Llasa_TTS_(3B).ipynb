{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59304aef",
   "metadata": {},
   "source": [
    "# ü§ô Llasa Tts (3B) on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/Llasa_TTS_(3B).ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Llasa Tts (3B)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llasa_TTS_(3B).ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkWYsztAs9Ky"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
    "\n",
    "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212,
     "referenced_widgets": [
      "8a2659e604fa4b099bc1ae6834967704",
      "8e22db40969249a59b10c3f762e7049f",
      "5fda0b8297474063b07fe15c90e211de",
      "4008ec319fec4cdeb5a946b1787b0acf",
      "ecd2612e3de54eb3a85be34f9c70e01e",
      "f72b78744a25478d8db069cb536b74fe",
      "a3688b729eaa4f0b81a59b13bd78644e",
      "b6b6a75140bf454c8168600f03827f2d",
      "e94f99e69e6842e489879240be46c060",
      "b06ac18e87d944049fadde7a085e8fd4",
      "c565223736354c85b636f0cbe01b5eac"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "6f94c990-9b42-4524-e150-9ca6d8f2cfbe"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any for long context!\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "    # Qwen3 new models\n",
    "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "    # Other very popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llasa-3B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # Select None for auto detection\n",
    "    load_in_4bit = False, # Choose True for 4bit which reduces memory\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "bf64d842-5a0a-41a1-d96f-8c68ea0c2586"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_alpha = 128,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "\n",
    "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format:\n",
    "**text, audio**. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjY75GoYUCB8"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\n",
    "OUTPUT_DIR = 'processed_data_memmap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zK94B-Pfioto",
    "outputId": "19d2be12-7f04-416c-a1a5-0e1af69c6048"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "#@title Tokenization Function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoTokenizer\n",
    "from xcodec2.modeling_xcodec2 import XCodec2Model\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "XCODEC2_MODEL_NAME = \"HKUST-Audio/xcodec2\"\n",
    "SAMPLE_RATE = 16000\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "def preprocess_and_save(\n",
    "    dataset,\n",
    "    output_dir: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    codec_model: XCodec2Model,\n",
    "    sample_rate: int = 16000,\n",
    "    max_length: int = 2048,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    codec_model = codec_model.to(device).eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    memmap_path = os.path.join(output_dir, f\"input_ids.memmap\")\n",
    "    shape_path = os.path.join(output_dir, f\"input_ids_shape.npy\")\n",
    "    num_samples = len(dataset)\n",
    "    shape = (num_samples, max_length)\n",
    "    try:\n",
    "        arr = np.memmap(memmap_path, dtype=np.int32, mode='w+', shape=shape)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    valid_sequences_count = 0\n",
    "    skipped_count = 0\n",
    "    for idx, example in tqdm(enumerate(dataset), total=num_samples, desc=f\"Processing\"):\n",
    "        try:\n",
    "            if 'text' not in example or example['text'] is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            text = f\"<|TEXT_UNDERSTANDING_START|>{example['text']}<|TEXT_UNDERSTANDING_END|>\"\n",
    "            text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            if \"audio\" not in example or \"array\" not in example[\"audio\"] or \"sampling_rate\" not in example[\"audio\"] or example[\"audio\"][\"array\"] is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            waveform = torch.tensor(example[\"audio\"][\"array\"]).float()\n",
    "            original_sr = example[\"audio\"][\"sampling_rate\"]\n",
    "            if original_sr != sample_rate:\n",
    "              waveform = torchaudio.functional.resample(waveform, original_sr, sample_rate)\n",
    "\n",
    "            original_shape_after_resample = waveform.shape\n",
    "            waveform = waveform.squeeze()\n",
    "            if waveform.dim() == 0:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            elif waveform.dim() > 1:\n",
    "                waveform = waveform[0]\n",
    "                if waveform.dim() != 1:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "            final_waveform = waveform.unsqueeze(0).to(device)\n",
    "            speech_codes = None\n",
    "            with torch.inference_mode():\n",
    "                speech_codes_raw = codec_model.encode_code(final_waveform)\n",
    "                speech_codes = speech_codes_raw[0][0]\n",
    "\n",
    "            codes_np = speech_codes.cpu().numpy()\n",
    "            speech_token_ids = [f\"<|s_{code}|>\" for code in codes_np]\n",
    "            speech_token_ids = tokenizer.convert_tokens_to_ids(speech_token_ids)\n",
    "            speech_ids = (\n",
    "                [tokenizer.convert_tokens_to_ids(\"<|SPEECH_GENERATION_START|>\")]\n",
    "                + speech_token_ids\n",
    "                + [tokenizer.convert_tokens_to_ids(\"<|SPEECH_GENERATION_END|>\")]\n",
    "            )\n",
    "            max_text_space = max_length - len(speech_ids)\n",
    "            if max_text_space < 0:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            truncated_text_ids = text_ids[:max_text_space]\n",
    "            combined_sequence = truncated_text_ids + speech_ids\n",
    "            padding_length = max_length - len(combined_sequence)\n",
    "            final_sequence = combined_sequence + [tokenizer.pad_token_id] * padding_length\n",
    "            final_sequence = final_sequence[:max_length]\n",
    "            arr[valid_sequences_count] = np.array(final_sequence, dtype=np.int32)\n",
    "            valid_sequences_count += 1\n",
    "        except Exception as e:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        arr.flush()\n",
    "        actual_shape = (valid_sequences_count, max_length)\n",
    "        np.save(shape_path, np.array(actual_shape))\n",
    "\n",
    "\n",
    "class MemmapTTSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 2048,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "             if self.tokenizer.eos_token_id is not None:\n",
    "                 self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "             else:\n",
    "                 raise ValueError(\"Tokenizer passed to Dataset must have pad_token_id set.\")\n",
    "\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = -100\n",
    "\n",
    "        memmap_file = os.path.join(data_path, f'input_ids.memmap')\n",
    "        shape_file = os.path.join(data_path, f'input_ids_shape.npy')\n",
    "\n",
    "        if not os.path.exists(memmap_file) or not os.path.exists(shape_file):\n",
    "             raise FileNotFoundError(f\"Required files not found  in {data_path}\")\n",
    "\n",
    "        self.shape = tuple(np.load(shape_file))\n",
    "        if not self.shape or len(self.shape) != 2 or self.shape[0] == 0:\n",
    "             self.length = 0\n",
    "             self.memmap_data = None\n",
    "        else:\n",
    "             self.memmap_data = np.memmap(memmap_file, dtype='int32', mode='r', shape=self.shape)\n",
    "             self.length = self.shape[0]\n",
    "\n",
    "        try:\n",
    "            self.speech_generation_start_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_START|>')\n",
    "            self.speech_generation_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n",
    "            self.text_understanding_start_id = tokenizer.convert_tokens_to_ids('<|TEXT_UNDERSTANDING_START|>')\n",
    "            self.text_understanding_end_id = tokenizer.convert_tokens_to_ids('<|TEXT_UNDERSTANDING_END|>')\n",
    "            assert isinstance(self.pad_token_id, int)\n",
    "        except Exception as token_err:\n",
    "            raise ValueError(f\"Tokenizer is missing required special tokens or pad_token_id. Error: {token_err}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def replace_tagged_token(self, token_list, target_token_id, new_sequence_ids):\n",
    "        if isinstance(new_sequence_ids, torch.Tensor):\n",
    "            new_sequence_ids = new_sequence_ids.tolist()\n",
    "        try:\n",
    "            idx = token_list.index(target_token_id)\n",
    "            return token_list[:idx] + new_sequence_ids + token_list[idx+1:]\n",
    "        except ValueError:\n",
    "            return token_list\n",
    "\n",
    "    def pad_sequence_torch(self, sequence, max_length, value=0):\n",
    "        current_len = len(sequence)\n",
    "        if current_len >= max_length:\n",
    "            return sequence[:max_length]\n",
    "        else:\n",
    "            padding_size = max_length - current_len\n",
    "            padding = torch.full((padding_size,), value, dtype=sequence.dtype, device=sequence.device)\n",
    "            return torch.cat([sequence, padding], dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.memmap_data is None or idx >= self.length:\n",
    "            raise IndexError(f\"Index out of bounds (length={self.length}).\")\n",
    "\n",
    "        raw_input_ids_np = self.memmap_data[idx]\n",
    "        input_ids_tensor_raw = torch.tensor(raw_input_ids_np, dtype=torch.long)\n",
    "        input_ids = None\n",
    "        speech_gen_idx_in_final = -1\n",
    "\n",
    "        try:\n",
    "            text_start_idx = (input_ids_tensor_raw == self.text_understanding_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            text_end_idx = (input_ids_tensor_raw == self.text_understanding_end_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            speech_start_idx = (input_ids_tensor_raw == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "\n",
    "            speech_end_marker_indices = (input_ids_tensor_raw == self.speech_generation_end_id).nonzero(as_tuple=True)[0]\n",
    "            pad_start_indices = (input_ids_tensor_raw == self.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if len(speech_end_marker_indices) > 0:\n",
    "                 speech_end_idx = speech_end_marker_indices[0].item()\n",
    "            elif len(pad_start_indices) > 0:\n",
    "                 speech_end_idx = pad_start_indices[0].item() - 1\n",
    "            else:\n",
    "                 speech_end_idx = len(input_ids_tensor_raw) - 1\n",
    "\n",
    "            if not (text_start_idx < text_end_idx < speech_start_idx < speech_end_idx < len(input_ids_tensor_raw)):\n",
    "                 raise ValueError(\"Parsed indices are invalid or out of order\")\n",
    "\n",
    "            original_text_sequence = input_ids_tensor_raw[:speech_start_idx]\n",
    "            original_speech_sequence_with_markers = input_ids_tensor_raw[speech_start_idx : speech_end_idx +1]\n",
    "            chat = [\n",
    "                {\"role\": \"user\", \"content\": f\"Convert the text to speech:<|TEXT_UNDERSTANDING_START|>\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                 import inspect\n",
    "                 sig = inspect.signature(self.tokenizer.apply_chat_template)\n",
    "                 if 'add_generation_prompt' in sig.parameters:\n",
    "                     templated_ids_list = self.tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=False)\n",
    "                 else:\n",
    "                     templated_ids_list = self.tokenizer.apply_chat_template(chat, tokenize=True)\n",
    "\n",
    "                 final_ids_list = self.replace_tagged_token(templated_ids_list, self.text_understanding_start_id, original_text_sequence.tolist())\n",
    "                 final_ids_list = self.replace_tagged_token(final_ids_list, self.speech_generation_start_id, original_speech_sequence_with_markers.tolist())\n",
    "                 input_ids = torch.tensor(final_ids_list, dtype=torch.long)\n",
    "\n",
    "                 try:\n",
    "                     speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "                 except IndexError:\n",
    "                      speech_gen_idx_in_final = -1\n",
    "            except Exception:\n",
    "                 input_ids = input_ids_tensor_raw\n",
    "                 try:\n",
    "                     speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "                 except IndexError:\n",
    "                     speech_gen_idx_in_final = -1\n",
    "\n",
    "        except Exception:\n",
    "            input_ids = input_ids_tensor_raw\n",
    "            try:\n",
    "                speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            except IndexError:\n",
    "                speech_gen_idx_in_final = -1\n",
    "\n",
    "        if input_ids is None:\n",
    "            input_ids = input_ids_tensor_raw\n",
    "            try:\n",
    "                speech_gen_idx_in_final = (input_ids == self.speech_generation_start_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            except IndexError:\n",
    "                speech_gen_idx_in_final = -1\n",
    "\n",
    "        labels = torch.full_like(input_ids, self.ignore_index)\n",
    "        if speech_gen_idx_in_final != -1 and speech_gen_idx_in_final < len(input_ids):\n",
    "             labels[speech_gen_idx_in_final:] = input_ids[speech_gen_idx_in_final:].clone()\n",
    "\n",
    "        attention_mask = (input_ids != self.pad_token_id).long()\n",
    "        labels[input_ids == self.pad_token_id] = self.ignore_index\n",
    "\n",
    "        input_ids = self.pad_sequence_torch(input_ids, self.max_length, value=self.pad_token_id)\n",
    "        attention_mask = self.pad_sequence_torch(attention_mask, self.max_length, value=0)\n",
    "        labels = self.pad_sequence_torch(labels, self.max_length, value=self.ignore_index)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    codec_model = XCodec2Model.from_pretrained(XCODEC2_MODEL_NAME)\n",
    "\n",
    "except Exception as e:\n",
    "    raise f\"ERROR loading XCodec2 model: {e}.\"\n",
    "\n",
    "preprocess_and_save(\n",
    "        dataset=dataset,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        codec_model=codec_model,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        max_length=max_seq_length,\n",
    "        device=DEVICE\n",
    "    )\n",
    "try:\n",
    "    train_dataset = MemmapTTSDataset(\n",
    "        data_path=OUTPUT_DIR,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_seq_length\n",
    "     )\n",
    "    print(f\"Dataset loaded for split 'train'. Number of samples: {len(train_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dataset: {e}\")\n",
    "print(\"Moving XCodec2 model to cpu\")\n",
    "codec_model.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95_Nn-89DhsL"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 5e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "29905f3f-2821-4a42-ea31-9c9b0a20a2a9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "49df827f-a803-4a1c-f04c-e4127b758cc5"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Enhanced GPU check for NVIDIA Brev\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run nvidia-smi\n",
    "subprocess.run(['nvidia-smi'], check=False)\n",
    "\n",
    "# PyTorch CUDA info\n",
    "import torch\n",
    "print(f\"\\nPyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgP6HUMgeTqP"
   },
   "outputs": [],
   "source": [
    "input_text = \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "apUdB40Ep6Ki",
    "outputId": "ff84382f-66ca-4677-dc89-d4b8fc64e6fc"
   },
   "outputs": [],
   "source": [
    "# Fix torch compilation cache permissions\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test if /ephemeral is writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_dir = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "else:\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "\n",
    "# Create directories with full write permissions\n",
    "for d in [cache_dir, triton_cache, tmpdir]:\n",
    "    os.makedirs(d, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_dir\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_dir\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Clean up any old compiled caches\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ Torch cache: {cache_dir}\")\n",
    "print(f\"‚úÖ Temp dir: {tmpdir}\")\n",
    "\n",
    "#@title Run Inference\n",
    "import soundfile as sf\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def ids_to_speech_tokens(speech_ids):\n",
    "\n",
    "    speech_tokens_str = []\n",
    "    for speech_id in speech_ids:\n",
    "        speech_tokens_str.append(f\"<|s_{speech_id}|>\")\n",
    "    return speech_tokens_str\n",
    "\n",
    "def extract_speech_ids(speech_tokens_str):\n",
    "\n",
    "    speech_ids = []\n",
    "    for token_str in speech_tokens_str:\n",
    "        if token_str.startswith('<|s_') and token_str.endswith('|>'):\n",
    "            num_str = token_str[4:-2]\n",
    "\n",
    "            num = int(num_str)\n",
    "            speech_ids.append(num)\n",
    "        else:\n",
    "            print(f\"Unexpected token: {token_str}\")\n",
    "    return speech_ids\n",
    "\n",
    "#TTS start!\n",
    "with torch.inference_mode():\n",
    "    with torch.amp.autocast('cuda',dtype=model.dtype):\n",
    "        formatted_text = f\"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>\"\n",
    "\n",
    "        # Tokenize the text\n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": \"Convert the text to speech:\" + formatted_text},\n",
    "            {\"role\": \"assistant\", \"content\": \"<|SPEECH_GENERATION_START|>\"}\n",
    "        ]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=True,\n",
    "            return_tensors='pt',\n",
    "            continue_final_message=True\n",
    "        )\n",
    "        input_ids = input_ids.to('cuda')\n",
    "\n",
    "        speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')\n",
    "\n",
    "        # Generate the speech autoregressively\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=2048,  # We trained our model with a max length of 2048\n",
    "            eos_token_id= speech_end_id ,\n",
    "            do_sample=True,\n",
    "            top_p=1.2,           #  Adjusts the diversity of generated content\n",
    "            temperature=1.2,   #  Controls randomness in output\n",
    "        )\n",
    "    # Extract the speech tokens\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:-1]\n",
    "\n",
    "    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Convert  token <|s_23456|> to int 23456\n",
    "    speech_tokens = extract_speech_ids(speech_tokens)\n",
    "\n",
    "    speech_tokens = torch.tensor(speech_tokens).cpu().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Decode the speech tokens to speech waveform\n",
    "    gen_wav = codec_model.decode_code(speech_tokens)\n",
    "\n",
    "sf.write(\"output.wav\", gen_wav[0, 0, :].cpu().numpy(), 16000)\n",
    "\n",
    "display(Audio(gen_wav[0, 0, :].cpu().numpy(), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "b7b41fe1-b0d9-466b-fb81-e97b7e9416c1"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHjt_SMYsd3P",
    "outputId": "bd8cccb7-6b95-45bf-80da-de120988447e"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
