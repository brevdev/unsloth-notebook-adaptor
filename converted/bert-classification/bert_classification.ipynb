{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da43419",
   "metadata": {},
   "source": [
    "# ü§ô Bert Classification on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/bert_classification.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Bert Classification</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/bert_classification.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Unsloth: {FastLanguageModel.__module__}\")\n",
    "    print(f\"   Transformers: {transformers.__version__}\")\n",
    "    \n",
    "    # Check if we need to upgrade/downgrade transformers\n",
    "    import pkg_resources\n",
    "    try:\n",
    "        current_transformers = pkg_resources.get_distribution(\"transformers\").version\n",
    "        if current_transformers != \"4.56.2\":\n",
    "            print(f\"   ‚ö†Ô∏è  Transformers {current_transformers} != 4.56.2, may need adjustment\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"   ‚úÖ All packages OK, skipping installation\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - installing required packages...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Find uv in common locations\n",
    "    uv_paths = [\n",
    "        \"uv\",  # In PATH\n",
    "        os.path.expanduser(\"~/.venv/bin/uv\"),\n",
    "        os.path.expanduser(\"~/.cargo/bin/uv\"),\n",
    "        \"/usr/local/bin/uv\"\n",
    "    ]\n",
    "    \n",
    "    uv_cmd = None\n",
    "    for path in uv_paths:\n",
    "        try:\n",
    "            result = subprocess.run([path, \"--version\"], capture_output=True, timeout=2)\n",
    "            if result.returncode == 0:\n",
    "                uv_cmd = path\n",
    "                print(f\"   Found uv at: {path}\")\n",
    "                break\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "    \n",
    "    if uv_cmd:\n",
    "        print(\"Using uv package manager...\\n\")\n",
    "        try:\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"unsloth\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([uv_cmd, \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ö†Ô∏è  uv install failed: {e}\")\n",
    "            uv_cmd = None  # Fall back to pip\n",
    "    \n",
    "    if not uv_cmd:\n",
    "        print(\"Using pip package manager...\\n\")\n",
    "        try:\n",
    "            # Ensure pip is available\n",
    "            subprocess.run([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"], \n",
    "                         capture_output=True, timeout=30)\n",
    "            # Install packages\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformers==4.56.2\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "            print(\"\\n‚úÖ Installation complete\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Installation failed: {e}\")\n",
    "            print(\"   This may be due to permission issues.\")\n",
    "            print(\"   Packages may already be installed - attempting to continue...\")\n",
    "    \n",
    "    # Verify installation\n",
    "    try:\n",
    "        from unsloth import FastLanguageModel\n",
    "        print(\"‚úÖ Unsloth is now available\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Unsloth still not available: {e}\")\n",
    "        print(\"‚ö†Ô∏è  Please check setup script ran successfully or restart instance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRVPfEMf6CR8"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOy_0RwSkN-r",
    "outputId": "08410834-5120-4735-dbf0-555e508e018b"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Disable fast generation for bert!\n",
    "%env UNSLOTH_DISABLE_FAST_GENERATION = 1\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "id2label = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\",4: \"fear\",5: \"surprise\"}\n",
    "\n",
    "label2id = {\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"answerdotai/ModernBERT-large\",\n",
    "    auto_model = AutoModelForSequenceClassification,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    num_labels  = 6,\n",
    "    full_finetuning = True,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB70uYYu6Jcm"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3gfwKIll8YO",
    "outputId": "6b956603-12ee-4c9a-ed28-8503e366f5c4"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "      target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqGqhTHDkN-s"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep  \n",
    "We now use the [Emotion dataset](https://huggingface.co/datasets/dair-ai/emotion) from `dair-ai`, which contains text labeled by emotion. In this example, we load the **unsplit** version and use only the first 30,000 samples.  \n",
    "\n",
    "We then split the dataset into training (80%) and validation (20%), and apply tokenization to prepare the text for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3913099c91b44dd68664721207725bda",
      "84caddabc29142a791b6aa7b6d07880c",
      "c936d4395a674fd7a3622ebd19e6df74",
      "87ea3acc08014b96890d9179224bb13a",
      "8d84aed419014e468a7ef9b43f58bdd0",
      "3f6c8d3399ac4fbb8eb2d3a613e8b233",
      "6b9f9baa64a14e8ab5b36e5d9d5c7588",
      "e96fcd287a2b46208aa5de21a6a8d8af",
      "20c701cfac604fd68d8276da3c7d57f3",
      "f4bf05423c4446ad88d1404d09b02e5c",
      "5c6259da9b6f43ac8b353f0e1ccd65af",
      "1b74a2a274454ec7bd37fb1ad54d0091",
      "187b99fbc9184d7c9582852f3f4df2dc",
      "64871bff18824c2bb6b518d9efab6baa",
      "d473cbb57b70434dbf9ff91a6aeb2b58",
      "03e3ad560874455489c7ab69f89642a9",
      "8dbb7880282a46bca975831af4938a7a",
      "2825026f34ba4a2587a666f7506e2d9d",
      "dfe9107c3aa04957842084431d371bec",
      "157803c9a9ad44b1a1eee666996b5098",
      "0775ba4f85ea4be18fa887a44493c940",
      "6d430404163346bf81d9994eceade43f"
     ]
    },
    "id": "lvJ4AUp-kN-t",
    "outputId": "5e8242ec-a440-46d7-b0bb-516d83baf31c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\",\"unsplit\",split='train[:30000]')\n",
    "\n",
    "# Split into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "train_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "val_dataset = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MogHCKe0JNw"
   },
   "source": [
    "\n",
    "We compute **class weights** using scikit-learn‚Äôs ```compute_class_weight```.  \n",
    "This is useful when training on datasets where certain classes are underrepresented, ensuring the model does not become biased towards majority labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NAbnE4ZlpOz"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels = train_dataset[\"label\"]\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h0AmObGUfFR"
   },
   "outputs": [],
   "source": [
    "# We rename the dataset column from **`label`** to **`labels`**, since this is the expected field name for Hugging Face `Trainer`.\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBMqliVdTRfZ",
    "outputId": "bb6b0496-c458-41e8-bd9d-dca90749a9f5"
   },
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4xPELDq0vum"
   },
   "source": [
    "\n",
    "We define a `compute_metrics` function to evaluate the model during training.  \n",
    "Here we use **accuracy** from scikit-learn, which compares predicted labels with the ground truth.  \n",
    "\n",
    "**[NOTE]** Accuracy is a good baseline, but for imbalanced datasets you may also want to track metrics like **F1-score**, **precision**, or **recall**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79PoG6LhX_4H"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "271uxbcf1Jii"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We train for one full epoch (num_train_epochs=1) to get a meaningful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zn9nvs9kN-u"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    eval_dataset = val_dataset,\n",
    "    train_dataset = train_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # bert-style models usually need more than 1 epoch\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 5e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.10,  # Evaluate every 10% of total training steps\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir=\"/workspace/outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGQB87Ph4Gvd"
   },
   "source": [
    "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "-tNmAvAHkN-u",
    "outputId": "bd4fffbb-fdb1-40a6-dcf0-6391a74dcd8e"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pR_Rqb31mj5"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmNqG_48kN-x",
    "outputId": "a3ed6e90-1a69-4d68-89ad-8148a1b35383"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentence1 = \"We just finished training ModernBERT with Unsloth and its amazing!\"\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model,tokenizer=tokenizer)\n",
    "\n",
    "classifier(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsjjiJ9K1vJx"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving finetuned models\n",
    "To save the final model, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqgqEaLe1zM6",
    "outputId": "e7d44a08-cade-4d07-cde1-48dab4c6fbe7"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"model\")\n",
    "# model.push_to_hub(\"your_name/model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkIWIXm-2Q0B"
   },
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "1be15a159d9874788f7b7854451912393d9e82d0d2bc47d83a870bda7fd9bc22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
