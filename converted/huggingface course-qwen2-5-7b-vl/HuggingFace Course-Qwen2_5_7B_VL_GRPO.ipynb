{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d00d94",
   "metadata": {},
   "source": [
    "# ü§ô Huggingface Course Qwen2 5 7B Vl Grpo on NVIDIA Brev\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #00ff87 0%, #60efff 100%); padding: 1px; border-radius: 8px; margin: 20px 0;\">\n",
    "    <div style=\"background: #0a0a0a; padding: 20px; border-radius: 7px;\">\n",
    "        <p style=\"color: #60efff; margin: 0;\"><strong>‚ö° Powered by Brev</strong> | Converted from <a href=\"https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-Qwen2_5_7B_VL_GRPO.ipynb\" style=\"color: #00ff87;\">Unsloth Notebook</a></p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "## üìã Configuration\n",
    "\n",
    "<table style=\"width: auto; margin-left: 0; border-collapse: collapse; border: 2px solid #808080;\">\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 2px solid #808080;\">\n",
    "            <th style=\"text-align: left; padding: 8px 12px; border-right: 2px solid #808080; font-weight: bold;\">Parameter</th>\n",
    "            <th style=\"text-align: left; padding: 8px 12px; font-weight: bold;\">Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Model</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">Huggingface Course Qwen2 5 7B Vl Grpo</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Recommended GPU</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">L4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Min VRAM</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">16 GB</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Batch Size</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: left; padding: 8px 12px; border-right: 1px solid #808080;\"><strong>Categories</strong></td>\n",
    "            <td style=\"text-align: left; padding: 8px 12px;\">fine-tuning</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "## üîß Key Adaptations for Brev\n",
    "\n",
    "- ‚úÖ Replaced Colab-specific installation with conda-based Unsloth\n",
    "- ‚úÖ Converted magic commands to subprocess calls\n",
    "- ‚úÖ Removed Google Drive dependencies\n",
    "- ‚úÖ Updated paths from `/workspace/` to `/workspace/`\n",
    "- ‚úÖ Added `device_map=\"auto\"` for multi-GPU support\n",
    "- ‚úÖ Optimized batch sizes for NVIDIA GPUs\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Brev Documentation](https://docs.nvidia.com/brev)\n",
    "- [Original Notebook](https://github.com/unslothai/notebooks/blob/main/nb/HuggingFace Course-Qwen2_5_7B_VL_GRPO.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"align-center\"><a href=\"https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt\"><img src=\"https://github.com/unslothai/notebooks/raw/main/assets/hf%20course.png\" width=\"165\"></a>\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "In this [Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter12/6?fw=pt) and Unsloth notebook, you will learn to transform Qwen2 5 7B VL GRPO into a Reasoning model using GRPO.\n",
    "\n",
    "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
    "\n",
    "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://github.com/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
    "\n",
    "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check for Brev\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Configure PyTorch cache directories to avoid permission errors\n",
    "# MUST be set before any torch imports\n",
    "# Prefer /ephemeral for Brev instances (larger scratch space)\n",
    "\n",
    "# Test if /ephemeral exists and is actually writable (not just readable)\n",
    "use_ephemeral = False\n",
    "if os.path.exists(\"/ephemeral\"):\n",
    "    try:\n",
    "        test_file = \"/ephemeral/.write_test\"\n",
    "        with open(test_file, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file)\n",
    "        use_ephemeral = True\n",
    "    except (PermissionError, OSError):\n",
    "        pass\n",
    "\n",
    "if use_ephemeral:\n",
    "    cache_base = \"/ephemeral/torch_cache\"\n",
    "    triton_cache = \"/ephemeral/triton_cache\"\n",
    "    tmpdir = \"/ephemeral/tmp\"\n",
    "    print(\"Using /ephemeral for cache (Brev scratch space)\")\n",
    "else:\n",
    "    cache_base = os.path.expanduser(\"~/.cache/torch/inductor\")\n",
    "    triton_cache = os.path.expanduser(\"~/.cache/triton\")\n",
    "    tmpdir = os.path.expanduser(\"~/.cache/tmp\")\n",
    "    print(\"Using home directory for cache\")\n",
    "\n",
    "# Set ALL PyTorch/Triton cache and temp directories\n",
    "os.environ[\"TORCHINDUCTOR_CACHE_DIR\"] = cache_base\n",
    "os.environ[\"TORCH_COMPILE_DIR\"] = cache_base\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = triton_cache\n",
    "os.environ[\"XDG_CACHE_HOME\"] = os.path.expanduser(\"~/.cache\")\n",
    "os.environ[\"TMPDIR\"] = tmpdir  # Override system /tmp\n",
    "os.environ[\"TEMP\"] = tmpdir\n",
    "os.environ[\"TMP\"] = tmpdir\n",
    "\n",
    "# Create cache directories with proper permissions (777 to ensure writability)\n",
    "for cache_dir in [cache_base, triton_cache, tmpdir, os.environ[\"XDG_CACHE_HOME\"]]:\n",
    "    os.makedirs(cache_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "# Clean up any old compiled caches that point to /tmp\n",
    "old_cache = os.path.join(os.getcwd(), \"unsloth_compiled_cache\")\n",
    "if os.path.exists(old_cache):\n",
    "    print(f\"‚ö†Ô∏è  Removing old compiled cache: {old_cache}\")\n",
    "    shutil.rmtree(old_cache, ignore_errors=True)\n",
    "\n",
    "print(f\"‚úÖ PyTorch cache: {cache_base}\")\n",
    "\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"\\n‚úÖ Unsloth already available\")\n",
    "    print(f\"   Location: {FastLanguageModel.__module__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Unsloth not found - will install\")\n",
    "\n",
    "# Install unsloth using uv (the package manager for this environment)\n",
    "import subprocess\n",
    "\n",
    "print(f\"\\nInstalling packages into: {sys.executable}\")\n",
    "print(\"Using uv package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use uv to install packages into the current environment\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([\"uv\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå 'uv' command not found. Trying alternative method...\")\n",
    "    # Fallback: install pip into venv first, then use it\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--upgrade\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"unsloth\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.56.2\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"trl==0.22.2\"])\n",
    "    print(\"\\n‚úÖ Installation complete\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    print(\"‚úÖ Unsloth is now available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Installation failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please restart kernel and try again\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIy3QkjW1O4R"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B3HIT0t6nc0"
   },
   "source": [
    "We're also introducing how you can do `GSPO` inside of Unsloth as well!\n",
    "\n",
    "The goal of this notebook is to make a vision language model solve maths problems via reinforcement learning given an image input like below:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lupantech/MathVista/main/assets/our_new_3_datasets.png\" alt=\"Alt text\" height=\"256\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2703d74decdc4125807c9875c4461291",
      "67fabe475a0445ada64afdf00bff5fec",
      "0388d09875fc46a9a3ae12f49e07d6d8",
      "379b895080f34076a573de37794308e0",
      "a2f8f877b069422bb07c21075107fd61",
      "83a6ab2b1b814bf9a4992153802216eb",
      "8cc853708b534132b56231ccbd5fc978",
      "48627fefdebc461da1cd2bf67674aa86",
      "1dd095ca8ea34c63b2798d59dc4175c1",
      "84258e6eec114aa284d31816bf7a57dc",
      "9f7b7ca8b0924dbbacabab4f54fb128f",
      "43aa401abecf40079a05cd59ffcb2fb6",
      "51f90e77a5994032978429b1fc462009",
      "0fa6d1eed9c64231a0287b41553d93f0",
      "761de31df42f43c68b27c4ad57d05077",
      "3c923a46bf9446c18f9e75111f904d23",
      "b3f88f993dd648c19d5a80b8e3c74594",
      "885649093cfd4609b7ec3feb6f021090",
      "7222b46dcfa44885abbd7941aecdbc90",
      "44f05f482ff94046bdab3fe7a5a76bb3",
      "67bf36a3eb41494db99aed87b2dee4d5",
      "21b6ab8314394072b77b29e4771455fd",
      "701999df749148888e54b08547ed605f",
      "df8f9b1270dd427bae2b366ec9736e7d",
      "d2fa2570de9847bf867ccdc95ee8984b",
      "687563920b384cb286aca938b1d8c9f0",
      "8ab510e5e8a9433db392b9ead22ad411",
      "1b18de90644347d48eba777970968e4d",
      "53ab79fb9c394745a302b7916a8c7daa",
      "e309a897a3fb48bc91cbd0018d071846",
      "94c6ea60d2d14e2193972ff82a7ddeb8",
      "b4507d832b98432ab6516e0dca85e6d5",
      "d0efecf54a02470897b1c42fc830cbb0",
      "04818585b6c04b2d91626d9bfc8314a8",
      "d5c9658f835f4555b1528d8fa7f1be7e",
      "e5071b775f2c40a592f76ee6d0a042d4",
      "daba4d705af24aa08b10caf962bff8d3",
      "ec64ab8ac30b44af9d9cb40ac0ebcd83",
      "c6bc05a6214e4ef18ea77c18d19d5ef7",
      "46a59210a67745f49fbd560187a4f1e2",
      "86c5420fa39544828bfd10e8b9319e70",
      "7aa5313a6651401c8954136598fc2c4d",
      "7e23248adc60410b82a5abd6c9d2aac3",
      "aaa6c5a22e4f4b52947b4e6f5e406086",
      "a93aec849ace49b0947798d8778d5e31",
      "adbe322d06844ea99a2541527e0a8ca6",
      "242ae1d3ee9144a48523f649d30a2583",
      "51f12240f07c462bbf2dcaef05931712",
      "04f4906910ce42509ea6dc679396cb19",
      "3e975b20065d4997a7e93f1be99671db",
      "a5a8e2f302f541a098f78a1e4d01a1d9",
      "51935287a9ba40bb8f6ed48187b17e52",
      "f8355e7175704560b988c2614beeb0c8",
      "864c118fcf4146f1a820afdb97bf2096",
      "0b96088fa2724e8bb7af39faccf22de6",
      "6a548e3c33894a6b868ebbf59820a769",
      "065f74cc18654496aab441e06dc67a9a",
      "6438bd475d114d519d030b9ec36bf83c",
      "87284e24286d46108b89b35fb986cfc4",
      "a8595c028f0842b4ac1973660d0a8be0",
      "c0c489fb89e041ed803e707d10a30e55",
      "95e3cfa58d73439c993c088fb4a5ff87",
      "0a7903e4c41644988101b1d8c39ddb5c",
      "ba805b815d78471e8c3b1761966ad1ee",
      "dd7c282a6f104b56a713ad09739dd6d1",
      "3fa745de4e5144c496e8a3bc9293fb07",
      "43501f5b190443189ba9d98fe7b0f456",
      "de96e1299de243f39598e93305fc79a4",
      "bee84591293c45ed85e75d50a7eb62ef",
      "49941f7f614a43c2ba6ca9aef20ade1d",
      "8da48c0ea1fd4a47b6b276efde3bbf1f",
      "6c839449c08e4a07a77f9992c7416a6d",
      "b188fb353ec34f74a46beff26d7b4196",
      "7a3cd92c01b04856b63360f6ca163f86",
      "060dd7f94a8b466581caae0e955698dc",
      "13fb9b2143d846958024ee38bb876e51",
      "f1c33ef64f4e4ff39467ea9bc0220214",
      "1b765881158641c19f67000aae5bf33c",
      "8565636463ee4a2094778643eaa7f90a",
      "3c57b3b7b03d49ecb974c611f5d0f01e",
      "150e5adc061640f3a63b0a6ea81c1b69",
      "6a671accf1004ca4af27d1cb68b6d018",
      "ec9c9973d3824a36b347b22fdf4b4364",
      "407602e3318e440da37d619a84549f7d",
      "f0aa67f8d50d4b438cef79fd89500aaf",
      "c71a5b5d04a14f46bdc5389f3e6f39f6",
      "c872525f0cfd4f5abb6cb9238f7664e9",
      "8d8474de238b4dfe9cef6400a7271378",
      "ea9d2e8b62cb4590973626a0b5b9aaaf",
      "05c5f34d5de44ffe88a5a982e2f0cca1",
      "0c91585ffb5148d8951b3c80d7b732f6",
      "a9404719dabe4d9fa2c905bf7eae0d1b",
      "ea6170aff2a94488b5dd050dd0367334",
      "e5f5084db1624dbe9d6e914c0e14c591",
      "09890fc82d7c4d09bf1239ababcfcb28",
      "175a99c810f74fec998f87cfe8d67b57",
      "edeed5b32f0e419f9065e4579e35217e",
      "17687243fac64fc9bfd201e51384b04e",
      "2b82deb25462405f89a616b31f6604f9",
      "d035b4b1be8642ce876246dd0ea88233",
      "bde15c2147e04c32ba1bdb1b5c65f778",
      "394094401483447db2bef389b8daa19c",
      "a22d6b3e4f3f45a49a111a743e80374e",
      "29c67e51ad404d8da4ae83ae200e7764",
      "a3aaf7d4cb8544abae6ef9248f807d09",
      "85b5d66fa517430c926e4fba85cb2944",
      "9b7ca2cf2c254ff6b68e7e6d65a4653d",
      "5a156644a13d4a6a87153bc40f108c2f",
      "cd105945edb046aabb8bae64fad9f437",
      "9a29ea6715da4b69bbdde09b23a8f506",
      "8664b2a66e0b4f5e980514f49d4f3e89",
      "ca6b6ba031d24b6ea1b629d35a5421bd",
      "aef974c62c234aa7a18d4ce9fbcfc820",
      "1cb8ae2e889541f29f96a6497315de87",
      "dfbcd4e3293243b98dbcf01bf153935a",
      "485db68717ed49db82e6ee5da561a748",
      "3ff7add4261647e89ef020b29ba6158c",
      "656c9fbeccf640b38e60118e9104150a",
      "f2032aab34e04da8923e66e77bd9b345",
      "391a81ba6c284ca4b5d40648fd8188e3",
      "fe04f792e60d41c5b52259952b446888",
      "5c99b12e86c546d38c1bc3f418268d90",
      "8a5f8198574546d7908d1db327ea4dcc",
      "966b0b26838049d8b34fc0bfcf34f3cb",
      "4338c3cc113f4edbb26d943aaa859bfe",
      "cd9dd4024cea4f17becbb9d12c803da9",
      "41da762d7cb14040b3257c4011bf3048",
      "5c040e8155444520be814e993304be3e",
      "e0cfd18a76d34b7ea96842cc6b755c38",
      "65b09a51c9c84b0b9653cf679dee415d",
      "ddb1c021b5ce4a298a933659f2d62229",
      "9e232960a5924209a8f9ab5b70b29a94",
      "0d87def3e961444d932f965b8fc30eb6",
      "7997e396b29d419faea0ecbd75b80032",
      "c12c45438d2345ee864c03036919b3b8",
      "17103d4e68d8474f9bf5cf3aa7682675",
      "9123642a329c4972ba462dee8695955b",
      "a0e3215e954940a5861b7dc7907dc250",
      "aeb72c8232484fc3ab6b0fcc2813cfd6",
      "29da3d6943c74ea2aa27dfd59feac37c",
      "a29c0832c2dd4babb2fe1368fcb21e67",
      "d56931e1726b413da3c9d7a0464509f9",
      "c4fb36572f83429d82453b9ccaad8768",
      "5b085abc5a3a44fca743f657c1be87e7",
      "efe39eda4fd74705b54c6231f8ad9c75",
      "b88e4db3bbea4ba9bc73386c52665a44",
      "34110e5223c2495797190774472d5904",
      "3952473551e344a19b6ed18e7b773a70",
      "208efabcb3634119974c9c2c57263a53",
      "e9ceefe63a634cdc97d65d320fc01a6a",
      "73a86e54ca55462596806d3c2f006fb1",
      "78059fe7da184a68b9b03e7713b2c267",
      "fc1cfffa3e3740a59192395e5b61f0be",
      "599278e8720c4e02bfbfbad229390539",
      "e2fd861b24694f0fa3e4759d4ce42234",
      "ef9d088aa5ee49858ee79a24c80fbf2e",
      "ebe29caf83134bfa9a8c274a64c319d8",
      "86c44c342f79479d9e0a24cbfb9f6aae",
      "bbf495cd15294b97b4bb88172b1e813a",
      "ff5beb2db52e47e5a6e38a3e54d3ebc2",
      "7dea82152f7a417fbf3a1e2b99ea2eab",
      "0988d0037d4c4616895dc7bc6fe75b24",
      "dfba015ac24b4012947103c535500bf4",
      "7fa3f92465ae4724b087689b5fa17940",
      "f413f95dbc4c43a3a24e785d565b6f74"
     ]
    },
    "id": "DkIvEkIIkEyB",
    "outputId": "8c5eef41-84d1-43f0-e85b-a5bf293d22a0"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "max_seq_length = 16384 # Must be this long for VLMs\n",
    "lora_rank = 16 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-VL-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOXrl8iLQx6S"
   },
   "source": [
    "In Unsloth, we share vLLM's weights directly, reducing VRAM usage by > 50%. vLLM also does not yet support LoRA on the vision layers, so we can only add them on the language layers. Vision GRPO still works though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmZ6zZ5AQu7I",
    "outputId": "141c8526-d589-4a6d-a2f9-c2e4fb43f7a1"
   },
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True,  # False if not finetuning language layers\n",
    "    finetune_attention_modules = True,  # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True,  # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "### Data Prep\n",
    "<a name=\"Data\"></a>\n",
    "\n",
    "`AI4Math/MathVista` is a dataset that involves using images to solve logic and math problems.\n",
    "\n",
    "For this notebook, we will only use math problems with numeric answers for simpilicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "ba8d40a250884aa5948e6cc311bf2807",
      "f5221ac09a524ffc8fcd88bfa74b1107",
      "dff069ea56c946e8b3281363981147df",
      "fe6d4b2102424182b2d9c5e9c3bfd780",
      "da71d3f10bca43818dd48ec3d1f706b5",
      "cf5601391fa14ea69e50f11bd2dd7436",
      "c2f75a6a8a2145b3843988799c356d1a",
      "cc138e8840c5418a9ea1edc0c219bb3d",
      "60b04121fbcb4bd3b3d326683ee5b15f",
      "e0e134a5e6bd4298b4b20e80d2f57699",
      "bfd470a0dc81412d9fb694284c240d14",
      "b3e5a10094014d4b85066c6e374eafd9",
      "df0aa2e364bc49179bf6f96ab61fd7ca",
      "a162acd48f8342d8a2fdc6c94f3583ef",
      "a68eaf4e6a4341e2a89ce0896b20a793",
      "fc5301acf39f4cd6b2c3935138365228",
      "124075234c334e99b86a858c531df4ef",
      "a3d6ffb6152b403aa4212b0cdafe079c",
      "52d9bb567d6f47e58df89f7b4a80fc9b",
      "000d35c3034c4f85bf7b40804bc1a3e9",
      "24e0eaec0a054996b2195188fabe7c9b",
      "80c602b916e047e495991db51d63e07a",
      "0905b7e5a6694a698cf115331451afd1",
      "1bcc17523cef47c4a4fd176632b9cdcb",
      "a6c4365474944e9aa25e7cee813a932c",
      "70802eacc63f4679a08615610e28ae6b",
      "593c0e4c56e74c8b9d15a6b40efc7bba",
      "f002683b9d3548efbbf245137b0d6a15",
      "b4dfc687fda74a019c6e9c2ce95e5a6e",
      "742c265c72e04e0e87d6baf3bd1a691c",
      "acc32792f9824216bc9b4a778d11ced5",
      "d4953a0e29654dc39946b4dc06b48463",
      "c73adb06c58a454cbedcfdea6d91bafe",
      "ec8d416e8419485186d1731c3a8e2ad1",
      "5ab0ca64ab0d462b8b6afd1969ddac82",
      "bdd5a2e1fa84465d82ce74a1f72896e8",
      "113a87f0d74543e7943dd14c8c5e6991",
      "f75043fa73bf43269d53d1e072bbd7d3",
      "e181f51653384296a6b8380129c72a1f",
      "5df32376d1564aa9bcaccde843019c78",
      "b0fa60fc535d4147a877d0b9855bc776",
      "2bef25d61e614cb98fef16968c181335",
      "80ecb8005bfe4af38219c0fcd3100172",
      "302983a9b3814321bc2a1528e0f1f208",
      "9189d024140f4d0ebbbfa761533ecd8d",
      "479107d5efaf44a9a84c900c989249ad",
      "d0d58327b8c24e2eb741b3b69bac37b1",
      "8b836c10b99f4c53a9a6abcff0ac2044",
      "045b4b0a31e24ef4aef54a527c31b373",
      "5c8cc2a2bea44544aad594c312151e45",
      "fe1bf6f5fba64b94817248d01e466fd4",
      "ba7aa38cd11e41f5a237ce91bb98e4e7",
      "05c752df2827468ab8e5372e710262da",
      "3ca22d3140e443dda9b55e8b6f0b4282",
      "9984b2a5bb214155b19d976ece4c5889",
      "cab8d6cd3a9348a999fcc3acb9effb90",
      "53abe73536c6447d9909799175371cae",
      "e14bce0dc05a46e2a0d6bde16ac7f944",
      "9dea60453d6a462785838c85512ad103",
      "77e82a9391c74b45b0133a3632402f3a",
      "24ef5e7cd6a648168c890a2af6473ba7",
      "166660ade64d40bc8bf3e7e6856b1b10",
      "09b637e537084389938c78c27cf65e19",
      "69fff61747ff49e9b6a6aac8a78e889d",
      "f4fb34ca31384c1c86870d8dcf3ea92b",
      "3bfa5954547147b9931dc62916773fcb"
     ]
    },
    "id": "7zM1VPx5KcpF",
    "outputId": "2e2914c7-cddd-4b5e-aafb-f5c56afadf1a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "dataset = load_dataset(\"AI4Math/MathVista\", split = \"testmini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0CeDQrm6BWW"
   },
   "source": [
    "We filter the dataset to keep only float or numeric answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2cac47a5a298435bae8b65a65fe8c2da",
      "f48c22c69c6d448887f74f402aa5173e",
      "ad9fcc16d77a4bc8a17a41a8f0c6e67f",
      "2ef0a936145643a18720b0a060a2feb5",
      "974d736f07ad4dd5ae6a1e964f9f47dc",
      "82fe5f2fd35545e78a78b33651a8b195",
      "962149cdc8264c899243f9028aeb053c",
      "260ffabdf4074210a9ddc2f9d492c926",
      "e6d76b2bfbda4b92ab236dd1abeeaa9a",
      "e327bcee00fd47cabb287d16b6375346",
      "d6ad5d48f08242aeaf5263a3abdb131e"
     ]
    },
    "id": "iw8EVJmp5rsC",
    "outputId": "cb95f66f-c2e0-414f-b694-e64008f7b3a6"
   },
   "outputs": [],
   "source": [
    "def is_numeric_answer(example):\n",
    "    try:\n",
    "        float(example[\"answer\"])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "dataset = dataset.filter(is_numeric_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAPtJzy_5uLh"
   },
   "source": [
    "We also resize the images to be 512 by 512 pixels to make the images managable in context length. We also convert them to RGB so they are compatible for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c79d1d9fbb2e4737a2b0d9cf617efeb1",
      "9dacdb6b7eb74d3f82e40cc08dd65a61",
      "218a428beb444433b47d4f93c925a8fc",
      "1c985a5c473747ebb0d6cf571d30e2fd",
      "f1d006e8eea6482c80bea95406d88bc8",
      "76dcd7d50a134dda8558b3a4dc280de4",
      "f84ba6c95f864d2d914994a32851ead3",
      "31026188994a4e5cab13c86a91774255",
      "683a669cd6ba4d6a8ad43fa43a8422a5",
      "316f501c6f9746249fd139ad6a20997d",
      "24ab555d6fc746848b50c932b52c3308",
      "8177644b85884056b37734d8409885ee",
      "22309782e6974e67a96033f83e526fa9",
      "c43fedc0b63d40169e2f4cb24f2b8d1f",
      "271f48044ff4454f8aac6d3022222023",
      "31924102d0714a8e843994c93bbbbc1a",
      "4f79ba3808d64946a58606b8745785dc",
      "362a626230c34d729fa676d960683e0b",
      "837272e8d71e47d8b14290ecd25ab38b",
      "6c9d527eb32a42a1967938c5e4bd6545",
      "e5c20f2b282f4d7fb18923d72104d8d5",
      "6b816b226e2a47b6a8d920e260ba422e"
     ]
    },
    "id": "tT8WNP1A5tsh",
    "outputId": "91064eb6-76be-4ab0-9631-74294351de0c"
   },
   "outputs": [],
   "source": [
    "# Resize to (512, 512)\n",
    "def resize_images(example):\n",
    "    image = example[\"decoded_image\"]\n",
    "    image = image.resize((512, 512))\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "dataset = dataset.map(resize_images)\n",
    "\n",
    "# Then convert to RGB\n",
    "def convert_to_rgb(example):\n",
    "    image = example[\"decoded_image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "dataset = dataset.map(convert_to_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2WpGKjZ7mHI"
   },
   "source": [
    "We then create the conversational template that is needed to collate the dataset for RL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "aa61da31f90c4e07b69aae36839747d6",
      "a2c3581e842d41198a80bbb05a603f8a",
      "52db2abb30b74fae9cf5f447dfb7bd1e",
      "1225820142b64413a8840f2962e541fa",
      "78c36739e82e43a68eccb070da546a6e",
      "60bbf2d9ff5b4c7fbb7e1a99b55a3505",
      "f9b135cbad484cc5bd0686c86c3b33e5",
      "5ae4fb1e6e3e404888c4cb8c9416cdc0",
      "719735ceba00476eb04cc4326c1a835e",
      "acc4a8afd15e4c2d90bf3a37556ad990",
      "2c87f3f205f640ca9bbd7b131279c8b7"
     ]
    },
    "id": "-lvgcXGjk_a6",
    "outputId": "248b39ba-bc04-4ff6-9a73-5aac6d0c379c"
   },
   "outputs": [],
   "source": [
    "# Define the delimiter variables for clarity and easy modification\n",
    "REASONING_START = \"<REASONING>\"\n",
    "REASONING_END = \"</REASONING>\"\n",
    "SOLUTION_START = \"<SOLUTION>\"\n",
    "SOLUTION_END = \"</SOLUTION>\"\n",
    "\n",
    "def make_conversation(example):\n",
    "    # Define placeholder constants if they are not defined globally\n",
    "    # The user's text prompt\n",
    "    text_content = (\n",
    "        f\"{example['question']}. Also first provide your reasoning or working out\"\\\n",
    "        f\" on how you would go about solving the question between {REASONING_START} and {REASONING_END}\"\n",
    "        f\" and then your final answer between {SOLUTION_START} and (put a single float here) {SOLUTION_END}\"\n",
    "    )\n",
    "\n",
    "    # Construct the prompt in the desired multi-modal format\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},  # Placeholder for the image\n",
    "                {\"type\": \"text\", \"text\": text_content},  # The text part of the prompt\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    # The actual image data is kept separate for the processor\n",
    "    return {\"prompt\": prompt, \"image\": example[\"decoded_image\"], \"answer\": example[\"answer\"]}\n",
    "\n",
    "train_dataset = dataset.map(make_conversation)\n",
    "\n",
    "# We're reformatting dataset like this because decoded_images are the actual images\n",
    "# The \"image\": example[\"decoded_image\"] does not properly format the dataset correctly\n",
    "\n",
    "# 1. Remove the original 'image' column\n",
    "train_dataset = train_dataset.remove_columns(\"image\")\n",
    "\n",
    "# 2. Rename 'decoded_image' to 'image'\n",
    "train_dataset = train_dataset.rename_column(\"decoded_image\", \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOz-lAoI5fLW"
   },
   "source": [
    "Now let's apply the chat template across the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "67e8a6d2a4004daf8408eb356c67af30",
      "5e633acbf2a44f54bacb505060175e36",
      "1df6a2aae5724c1c84c42f750e4f17a2",
      "6269c684ec0045b6a592cdd12984b03f",
      "e572092edf51453f971b30a221b5b759",
      "aed484aba10d458ca7d0b4e90286d4f8",
      "0cc60eb44c4a47f5879bd179feb1215d",
      "521a02db23a94fc7b653754150568a2f",
      "413bf7f7b0b24be29b3fc42ec3b898fc",
      "b759dab4603f4b76805df0ae8c06607f",
      "a2aaac40963a41bb97826611d609679a"
     ]
    },
    "id": "ZaxwJAqS5d1L",
    "outputId": "a2cb1fde-60dc-4342-9e95-373da40ec0aa"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda example: {\n",
    "        \"prompt\": tokenizer.apply_chat_template(\n",
    "            example[\"prompt\"],\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = True, # Must add assistant\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEs2HiThleic"
   },
   "source": [
    "## Reward functions\n",
    "\n",
    "We now define some basic formatting rewards functions to see if reasoning starts and ends, and also another to see if the answers were written correctly.\n",
    "\n",
    "We also try to fix the `addCriterion` issue as described in our [blog post](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "import re\n",
    "\n",
    "def formatting_reward_func(completions,**kwargs):\n",
    "    import re\n",
    "    thinking_pattern = f'{REASONING_START}(.*?){REASONING_END}'\n",
    "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
    "\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        thinking_matches = re.findall(thinking_pattern, completion, re.DOTALL)\n",
    "        answer_matches = re.findall(answer_pattern, completion, re.DOTALL)\n",
    "        if len(thinking_matches) == 1:\n",
    "            score += 1.0\n",
    "        if len(answer_matches) == 1:\n",
    "            score += 1.0\n",
    "\n",
    "        # Fix up addCriterion issues\n",
    "        # See https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks\n",
    "        # Penalize on excessive addCriterion and newlines\n",
    "        if len(completion) != 0:\n",
    "            removal = completion.replace(\"addCriterion\", \"\").replace(\"\\n\", \"\")\n",
    "            if (len(completion)-len(removal))/len(completion) >= 0.5:\n",
    "                score -= 2.0\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
    "\n",
    "    responses = [re.findall(answer_pattern, completion, re.DOTALL) for completion in completions]\n",
    "    q = prompts[0]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:{completions[0]}\")\n",
    "    return [\n",
    "        2.0 if len(r)==1 and a == r[0].replace('\\n','') else 0.0\n",
    "        for r, a in zip(responses, answer)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrst4oDS5AFO"
   },
   "source": [
    "Here is the first example prompt in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "nXjAu-nQwpuI",
    "outputId": "4cc1d1a2-9fe0-400f-dd69-0b8a82529a0e"
   },
   "outputs": [],
   "source": [
    "train_dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YpenSUIAczo"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model on the hundredth sample of the train dataset without training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "f3ffd184909845adb9185bd70eff48af",
      "bbbedffbe8b04ea798393d2e2ecbc3ea",
      "d02f224a380e4f82967d78935219de9b",
      "51233a1e2da643a18681bdde642cb1fa",
      "7ab7ff7fc31e4242b880e8adf49ea0f1",
      "d4337170dd20434ba7d59b2f1c404083",
      "edecf6df449d4b81b177e6bf03ed1ea0",
      "472b9e92fc6941c692db7bff21410cf8",
      "803a3148879042669c388a8e68a37342",
      "3cfb6377507f4bc7be934de026ecf530",
      "6bcf42b9d2a947138d3380bb88bec210",
      "33ebd3fabc5443ddaabea841ab07b43c",
      "99fd43de669b4c089bfe1e03dca4df1a",
      "e3e10ef70fca426fb9b6d7b20ff2ff37",
      "d83041f8c26744bebf47c55c310b542e",
      "c3338b49dac641ad9191a1f78b7f669e",
      "095baa79cf69491682f7bec4a0964ea9",
      "0f3db939e5f842cd82e03f28ed1c0e3f",
      "e637ff9e894549e4860b7344b13f521d",
      "7a6cc0e9f48243b7b8406743cb89be1d",
      "0f5b4b9e00de49f2a45e700ab041eccc",
      "f1aefaef2db7441eaadb2cb9acdfc338"
     ]
    },
    "id": "xGa0rueD5Mfh",
    "outputId": "f09f58e1-d1f3-44fa-ffb0-4d84f0cb63c6"
   },
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "outputs = model.fast_generate(\n",
    "    {\n",
    "        \"prompt\": train_dataset[100][\"prompt\"],\n",
    "        \"multi_modal_data\": {\"image\": train_dataset[100][\"image\"]}\n",
    "    },\n",
    "    sampling_params,\n",
    ")\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux6iqP7z5YOo"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up the `GRPO` Trainer and all configurations! Note we actually enable `GSPO` as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptqkXK2D4d6p",
    "outputId": "80268c48-f6a6-44dc-8eee-a6c43d0aefd8"
   },
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    log_completions = False,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 1024,\n",
    "    max_completion_length = 1024,\n",
    "    num_train_epochs = 0.5, # Set to 1 for a full training run\n",
    "    # max_steps = 60,\n",
    "    save_steps = 60,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir=\"/workspace/outputs\",\n",
    "\n",
    "    # Below enables GSPO:\n",
    "    importance_sampling_level = \"sequence\",\n",
    "    mask_truncated_completions = False,\n",
    "    loss_type = \"dr_grpo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Mv8UZO5hz-"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n",
    "\n",
    "During inference, you might encounter `addCriterion` or some weird gibberish outputs. Please read our [blog post](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks) on why this occurs. It seems to be an inherent thing inside of the model, and we can ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6fUaoYJEKgpb",
    "outputId": "3fcc08f1-19a8-4da3-90e4-9a552f8943f0"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    # Pass the processor to handle multimodal inputs\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        formatting_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlaUdxC_VHpz"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Colxz9TAVMsi"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL-BcuB1VLIv"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzUvkjO6ffIs"
   },
   "source": [
    "We try calling vLLM with our trained RL model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e0c4278e00cd4511987e7cb21ea7ecd9",
      "fb76cc95e5a0480f8b348e0f56042279",
      "1e1d2fa8e4484856b1263dac5438af40",
      "86757329fd7a4b8b9518f02c4b81b181",
      "66d3bd47f3ae401096d714e134f1f5fe",
      "8ff867b41af942d6a1f4fd07788533d8",
      "6a908636183247d3b6bb9388204f1d25",
      "a1a4fdea4ae749cea384a7ea4009b1cb",
      "ccfdf7f7c2344b0ea3fe7e82d9f59daf",
      "5c4baec967c24493805d73ef57fe7ebf",
      "e59c47ce1b194dfea6305b80f7f8ed5c",
      "be18abdbf4ca44e590ebdae5ed5a26ed",
      "e3ae48fda7ca4960942b734a5ad23d87",
      "b5b3f56ebf22411cb66286118b1e4e75",
      "6cad2e52c78b4246a322668b820c4d21",
      "4f16ca8bd1614db69ea14ec60b937cbc",
      "eb8201e5d64940c38e6b3b2e47e19a3f",
      "454666bd4fc149e48381f939885a3382",
      "0ec2e46bf2fa41828f225ca856149f96",
      "dbaeb7c9184f43bc8616bac4d311d966",
      "b2f75cbeeaee4f4a89a44f8e93aa825c",
      "235a8ef9ea494c90b0fdf04d3a47d5c6"
     ]
    },
    "id": "qtcz_lpbVC92",
    "outputId": "d90bfb1f-0393-455f-bb64-8a3ed5f5c5ec"
   },
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "outputs = model.fast_generate(\n",
    "    {\n",
    "        \"prompt\": train_dataset[165][\"prompt\"],\n",
    "        \"multi_modal_data\": {\"image\": train_dataset[165][\"image\"]}\n",
    "    },\n",
    "    sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_lora\"))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4LMOBl8boGX"
   },
   "source": [
    "Verify LoRA is actually trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SfdI-ERbpiw"
   },
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NUEmHFSYNTp"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07lMVV96vz39"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WMb3k_YPt8"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://github.com/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyEjW-WuYQIm"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbm6rx-dbt5Z"
   },
   "source": [
    "Special Credits to [GAD-Cell](https://github.com/GAD-cell) for helping Unsloth create this notebook and bringing VLM GRPO into Unsloth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxaYP7QBW_Ej"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "**Additional Resources:**\n",
    "\n",
    "- üìö [Unsloth Documentation](https://docs.unsloth.ai) - Complete guides and examples\n",
    "- üí¨ [Unsloth Discord](https://discord.gg/unsloth) - Community support\n",
    "- üìñ [More Notebooks](https://github.com/unslothai/notebooks) - Full collection on GitHub\n",
    "- üöÄ [Brev Documentation](https://docs.nvidia.com/brev) - Deploy and scale on NVIDIA GPUs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
